{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from __future__ import division\n",
    "from random import shuffle\n",
    "import xmltodict\n",
    "import json\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# view all columns of pandas df\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# nltk.download('wordnet')      # download wordnet if it's not already downloaded\n",
    "\n",
    "with open (\"dreambank-public.xml\") as f:\n",
    "    doc = xmltodict.parse(f.read())\n",
    "\n",
    "def convert(data):\n",
    "    if isinstance(data, basestring):\n",
    "        return str(data)\n",
    "    elif isinstance(data, collections.Mapping):\n",
    "        return dict(map(convert, data.iteritems()))\n",
    "    elif isinstance(data, collections.Iterable):\n",
    "        return type(data)(map(convert, data))\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "def left(s, amount):\n",
    "    return s[:amount]\n",
    "\n",
    "def right(s, amount):\n",
    "    return s[-amount:]\n",
    "\n",
    "def mid(s, offset, amount):\n",
    "    return s[offset:offset+amount]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Analysis\n",
    "\n",
    "First we want to print the data to see which fields we are given and how the data looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alta: a detailed dreamer (422 dreams)\n",
      "  ID: alta\n",
      "  type: series\n",
      "  sex: F\n",
      "  age: A\n",
      "  time: 1985-1997\n",
      "  sample dream: \n",
      "    number: 1\n",
      "    date: 1957\n",
      "    report: The one at the Meads's house, where it's bigger inside than out; there's a European village just inside, with a cobblestone street and a Pied-Piper sort of man with curly hair, he can do things like j...\n",
      "\n",
      "\n",
      "Angie: age 18 & 20 (48 dreams)\n",
      "  ID: angie\n",
      "  type: series\n",
      "  sex: F\n",
      "  age: Y\n",
      "  time: 1996\n",
      "  sample dream: \n",
      "    number: 1-01\n",
      "    date: 1996-04-03\n",
      "    report: My memory of this dream is vague. I think the setting is on a college campus. I'm in a cafe and two elderly ladies walk in and start talking to me about a university that a guy I am dating got into fo...\n",
      "\n",
      "\n",
      "Arlie: a middle-aged woman (212 dreams)\n",
      "  ID: arlie\n",
      "  type: series\n",
      "  sex: F\n",
      "  age: A\n",
      "  time: 1992-1998\n",
      "  sample dream: \n",
      "    number: 1\n",
      "    date: 10/14/92\n",
      "    report: I am in an office in the town next to the town I grew up in. Everyone is taking a rest. I have to go to the bathroom, but there is no toilet so I use an empty can. I dump the can in the toilet (which ...\n",
      "\n",
      "\n",
      "Barb Sanders (3116 dreams)\n",
      "  ID: b\n",
      "  type: series\n",
      "  sex: F\n",
      "  age: YA\n",
      "  time: 1960-1997\n",
      "  sample dream: \n",
      "    number: 0000\n",
      "    date: 05/03/60\n",
      "    report: I had the neatest dream about Blake, me, Reta and Bill E....\n",
      "\n",
      "\n",
      "Barb Sanders #2 (1138 dreams)\n",
      "  ID: b2\n",
      "  type: series\n",
      "  sex: F\n",
      "  age: A\n",
      "  time: 1997-2001\n",
      "  sample dream: \n",
      "    number: 3116\n",
      "    date: 02/01/97\n",
      "    report: [\"I have the big gun.\"] I am on a beach. There is some group game going on where people run toward the ocean with guns. We had been trained by the army. Off they go and I am in my wheelchair and going...\n",
      "\n",
      "\n",
      "Bay Area girls: Grades 4-6 (234 dreams)\n",
      "  ID: bay_area_girls_456\n",
      "  type: set\n",
      "  sex: F\n",
      "  age: C\n",
      "  time: 1996-1997\n",
      "  sample dream: \n",
      "    number: 149-01\n",
      "    date: 4th grader, 01/18/97\n",
      "    report: I was looking at the moon and another thing that kinda looked like a cutesie-poo sun. I was with my friend Anna L., and Rita L., her mom, but it wasn't really Anna's mom - it was someone who looked li...\n",
      "\n",
      "\n",
      "Bay Area girls: Grades 7-9 (154 dreams)\n",
      "  ID: bay_area_girls_789\n",
      "  type: set\n",
      "  sex: F\n",
      "  age: T\n",
      "  time: 1996-1997\n",
      "  sample dream: \n",
      "    number: 401-01\n",
      "    date: 7th grader, 01/21/97\n",
      "    report: I was sitting with some girlfriends on a grassy hill when all of a sudden a fire broke out and everyone was running. Then the fire disappeared and we all turned into cat people and went swimming in a ...\n",
      "\n",
      "\n",
      "Blind dreamers (F) (238 dreams)\n",
      "  ID: blind-f\n",
      "  type: set\n",
      "  sex: F\n",
      "  age: A\n",
      "  time: mid-1990s\n",
      "  sample dream: \n",
      "    number: 101-01\n",
      "    report: I was at my parent's house that I grew up in and I was going out with a friend that I have here. I saw the house in fairly clear detail as if I were seeing it, and I went out with my friend to her hou...\n",
      "\n",
      "\n",
      "Blind dreamers (M) (143 dreams)\n",
      "  ID: blind-m\n",
      "  type: set\n",
      "  sex: M\n",
      "  age: A\n",
      "  time: mid-1990s\n",
      "  sample dream: \n",
      "    number: 104-01\n",
      "    date: 5/18/97\n",
      "    report: Dreamed I was in my computer room in the basement, trying to write a letter, to my friend H, trying to decide whether to put it in Braille or on tape. Then was sitting in front of the window, opening ...\n",
      "\n",
      "\n",
      "Robert Bosnak: A dream analyst (53 dreams)\n",
      "  ID: bosnak\n",
      "  type: series\n",
      "  sex: M\n",
      "  age: A\n",
      "  sample dream: \n",
      "    number: 01\n",
      "    report: At party. A woman friend sits at long table. My wife is there. She looks gray and old. My friend tells me that she hasn't told me the whole story. She has already filed papers for divorce. Then she si...\n",
      "\n",
      "\n",
      "Chris: a transvestite (100 dreams)\n",
      "  ID: chris\n",
      "  type: series\n",
      "  sex: M\n",
      "  age: A\n",
      "  time: 1968\n",
      "  sample dream: \n",
      "    number: 001\n",
      "    date: 06/??/68\n",
      "    report: I arrive home and note that I am driving a jeep, but it is the wrong jeep and so I drive back to a church and there a priest is standing next to my own jeep. (Priest a stranger). He says that he knew ...\n",
      "\n",
      "\n",
      "Chuck: a physical scientist (75 dreams)\n",
      "  ID: chuck\n",
      "  type: series\n",
      "  sex: M\n",
      "  age: A\n",
      "  time: 1991-1993\n",
      "  sample dream: \n",
      "    number: 01\n",
      "    date: 06/16/91\n",
      "    report: There are four of us at the beginning and we are among Those on Earth. A spirit, a powerful angel, comes for us and we follow her through the air to a room -- is it in the clouds? The room is long and...\n",
      "\n",
      "\n",
      "Dahlia: concerns with appearance (24 dreams)\n",
      "  ID: dahlia\n",
      "  type: series\n",
      "  sex: F\n",
      "  age: Y\n",
      "  sample dream: \n",
      "    number: 1\n",
      "    report: I was driving to Jack's and it was at night. When I pulled up, Bonnie's car was parked in front. There was an electrical cord leading from her car to the house. I think I thought she was charging her ...\n",
      "\n",
      "\n",
      "David: teenage dreams (166 dreams)\n",
      "  ID: david\n",
      "  type: series\n",
      "  sex: M\n",
      "  age: T\n",
      "  time: 1990-1999\n",
      "  sample dream: \n",
      "    number: 1\n",
      "    date: 01/02/90\n",
      "    report: I was in the Grammar School playground, and this time Angela had taken off to somewhere in the school. So I ran over to Julie and asked her if she knew where Angela was, and she said: \"I can't tell yo...\n",
      "\n",
      "\n",
      "Dorothea: 53 years of dreams (900 dreams)\n",
      "  ID: dorothea\n",
      "  type: series\n",
      "  sex: F\n",
      "  age: A\n",
      "  time: 1912-1965\n",
      "  sample dream: \n",
      "    number: 1\n",
      "    date: --/--/--\n",
      "    report: I remember mainly getting out of bed to go to my mother's room. The first time I was distracted by a meteorite seen through the window I passed, and so I returned to bed....\n",
      "\n",
      "\n",
      "Ed: dreams of his late wife (143 dreams)\n",
      "  ID: ed\n",
      "  type: series\n",
      "  sex: M\n",
      "  age: A\n",
      "  time: 1980-2002\n",
      "  sample dream: \n",
      "    number: 001\n",
      "    date: 07/??/80\n",
      "    codings: {'char': '1FWA', 'fri': [{'rec': 'D', 'init': '1FWA', 'code': '2>'}, {'rec': '1FWA', 'init': 'D', 'code': '1>'}]}\n",
      "    report: I see Mary in profile. She looks as lovely as she had before the illness, and when she was younger. At one point in the dream I also see a profile view of her when she and Maria press their cheeks tog...\n",
      "\n",
      "\n",
      "Edna: a blind woman (19 dreams)\n",
      "  ID: edna\n",
      "  type: series\n",
      "  sex: F\n",
      "  age: Y\n",
      "  time: 1948-1949\n",
      "  sample dream: \n",
      "    number: 01\n",
      "    date: 10/11/48\n",
      "    report: I was out eating and it was a very nice place. There were five of us, Mother and Father and two other people, a man and a woman. Father was on my right, around the corner from me, and to my left was t...\n",
      "\n",
      "\n",
      "Elizabeth: a woman in her 40s (1707 dreams)\n",
      "  ID: elizabeth\n",
      "  type: series\n",
      "  sex: F\n",
      "  age: A\n",
      "  time: 1999-\n",
      "  sample dream: \n",
      "    number: 0001\n",
      "    date: ????-??-??\n",
      "    report: After he left, another young man came out from behind the reception desk. I told him I was waiting for Debbie Phillips [unknown in waking life]. He said, oh yeah you have some forms to fill out. At th...\n",
      "\n",
      "\n",
      "Emma: 48 years of dreams (1521 dreams)\n",
      "  ID: emma\n",
      "  type: series\n",
      "  sex: F\n",
      "  age: A\n",
      "  time: 1949-1997\n",
      "  sample dream: \n",
      "    number: Emma-1960-001\n",
      "    codings: {'char': ['1MHA', '1IUA', '1FMA', '1MFA'], 'fri': [{'rec': 'D', 'init': '1MHA', 'code': '7>'}, {'rec': 'D', 'init': '1MHA', 'code': '7>'}, {'rec': '1MHA', 'init': '1FMA', 'code': '2>'}]}\n",
      "\n",
      "\n",
      "Emma's Husband (72 dreams)\n",
      "  ID: emmas_husband\n",
      "  type: series\n",
      "  sex: M\n",
      "  age: A\n",
      "  time: 1940-1998\n",
      "  sample dream: \n",
      "    number: 001\n",
      "    date: 1940\n",
      "    report: I go into small \"shotgun house\" a sharecropper house--Three rooms in straight row. A black woman is sexy with print dress and no underwear, and big breasts show thru strained buttons in front. I want ...\n",
      "\n",
      "\n",
      "Esther: an adolescent girl (110 dreams)\n",
      "  ID: esther\n",
      "  type: series\n",
      "  sex: F\n",
      "  age: TY\n",
      "  time: 1998\n",
      "  sample dream: \n",
      "    number: 1\n",
      "    date: [age 12]\n",
      "    report: I am a little blond girl on an escalator. I am holding hands with two younger blond boys. We are all dressed in white. Everything is glowing... there's lots of light. Almost blinding, but very peacefu...\n",
      "\n",
      "\n",
      "College women, late 1940s (681 dreams)\n",
      "  ID: hall_female\n",
      "  type: set\n",
      "  sex: F\n",
      "  age: Y\n",
      "  time: 1946-1950\n",
      "  sample dream: \n",
      "    number: 0001\n",
      "    date: Code 001, Age 24, 11/??/47\n",
      "    report: I dreamed that I was at a public affair but I don't know which affair it was although it was outdoors. There were many people around us and they were of all ages. I was at this affair with B. He is ab...\n",
      "\n",
      "\n",
      "Izzy (all) (4352 dreams)\n",
      "  ID: izzy\n",
      "  type: series\n",
      "  sex: F\n",
      "  age: TY\n",
      "  time: 2003-2016\n",
      "  sample dream: \n",
      "    number: 0001\n",
      "    date: 1997-??-?? (?)\n",
      "    report: I was out the front of the house, and I saw a creepy guy. I tried to scream but I couldn't. I ended up getting inside, and Mom put me over her shoulder and was carrying me down the hallway. The guy ca...\n",
      "\n",
      "\n",
      "Jasmine (all) (664 dreams)\n",
      "  ID: jasmine\n",
      "  type: series\n",
      "  sex: F\n",
      "  age: TY\n",
      "  time: 1999-2011\n",
      "  sample dream: \n",
      "    number: A-001\n",
      "    date: 1999 (14)\n",
      "    report: I was at school with Mom, and her and I were up at this little mixing room type of a thing, up above on the third floor of the main building. But it was really supposed to be a computer lab. Anyway, s...\n",
      "\n",
      "\n",
      "Jeff: a lucid dreamer (87 dreams)\n",
      "  ID: jeff\n",
      "  type: series\n",
      "  sex: M\n",
      "  age: T\n",
      "  time: 2000\n",
      "  sample dream: \n",
      "    number: 001\n",
      "    date: 01/30/2000\n",
      "    report: In my dream, the first thing I remember was this very nerdy guy that goes to my school (in my opinion he was the biggest nerd at my school). So, then I saw a girl talking to him, and all of a sudden s...\n",
      "\n",
      "\n",
      "Joan: a lesbian (42 dreams)\n",
      "  ID: joan\n",
      "  type: series\n",
      "  sex: F\n",
      "  age: Y\n",
      "  time: mid-1980s\n",
      "  sample dream: \n",
      "    number: 1\n",
      "    report: I was at a campground where a seminar was being held. I was talking with Eliza. A group of people were gathering around the stairs in front of a building to have their picture taken. Eliza said, \"Hurr...\n",
      "\n",
      "\n",
      "Kenneth (2022 dreams)\n",
      "  ID: kenneth\n",
      "  type: series\n",
      "  sex: M\n",
      "  age: Y\n",
      "  time: 1996-1998\n",
      "  sample dream: \n",
      "    number: 001\n",
      "    date: 01/04/1996\n",
      "    report: <I>Disease Room</I> <BR><BR> I am with my friend Kevin in a small room with a long rectangular table. There is a television in an upper corner of the room. It is on. My classmate Ned Stallone is a sec...\n",
      "\n",
      "\n",
      "Madeline 1: High School (98 dreams)\n",
      "  ID: madeline1-hs\n",
      "  type: series\n",
      "  sex: F\n",
      "  age: T\n",
      "  time: 1997-2000\n",
      "  sample dream: \n",
      "    number: 0030\n",
      "    date: HS:??/??/97\n",
      "    report: I was swimming in a river.  For some reason I remember my father was with me.  We started swimming or drifting a ways, but came to an area where the river was full of corpses floating around.  I was d...\n",
      "\n",
      "\n",
      "Madeline 2: College Dorms (186 dreams)\n",
      "  ID: madeline2-dorms\n",
      "  type: series\n",
      "  sex: F\n",
      "  age: Y\n",
      "  time: 2000-2001\n",
      "  sample dream: \n",
      "    number: 0128\n",
      "    date: Dorms:09/25/00\n",
      "    report: I saw my (deceased) maternal Grandpa Gerald again.  It didn't seem as though he fully welcomed my hug.  The strange thing was that he told me several Bible references.  Romans 1.1, 1.4, possibly 1.7 o...\n",
      "\n",
      "\n",
      "Madeline 3: Off-Campus (348 dreams)\n",
      "  ID: madeline3-offcampus\n",
      "  type: series\n",
      "  sex: F\n",
      "  age: Y\n",
      "  time: 2001-2003\n",
      "  sample dream: \n",
      "    number: 0314\n",
      "    date: Off-Campus:08/02/01\n",
      "    report: I dreamt of driving to my maternal Grandma Jane's house.  Once there, though, my (deceased) maternal Grandpa Gerald showed up.  He had greenish red lesions on his neck and shoulder that was supposed t...\n",
      "\n",
      "\n",
      "Madeline 4: After College (294 dreams)\n",
      "  ID: madeline4-postgrad\n",
      "  type: series\n",
      "  sex: F\n",
      "  age: Y\n",
      "  time: 2003-2004\n",
      "  sample dream: \n",
      "    number: 0662\n",
      "    date: Post-Grad:06/07/03\n",
      "    report: My boyfriend Jeremy came back to the apartment and said he \"was crashed\".  I tried to look over the balcony to see the damage to the truck, but I couldn't see....\n",
      "\n",
      "\n",
      "Mack: A poor recaller (38 dreams)\n",
      "  ID: mack\n",
      "  type: series\n",
      "  sex: M\n",
      "  age: Y\n",
      "  time: late 1990s\n",
      "  sample dream: \n",
      "    number: 001\n",
      "    report: There was-a white, nylon windbreaker that was hanging on a chair, almost as if on display. It was of an old style, maybe from the 1970s or 8Os and of a generic cheap quality. I noted that I liked it a...\n",
      "\n",
      "\n",
      "Mark: a young boy (23 dreams)\n",
      "  ID: mark\n",
      "  type: series\n",
      "  sex: M\n",
      "  age: C\n",
      "  time: 1997-1999\n",
      "  sample dream: \n",
      "    number: 01\n",
      "    date: October, 1997\n",
      "    report: I was at the park. I had these little cars in my hand, I was riding a bicycle. The I crashed into a car. The cars in my hand got bigger and I got in them. This little girl had some in her hand and wan...\n",
      "\n",
      "\n",
      "Melissa: a young girl (89 dreams)\n",
      "  ID: melissa\n",
      "  type: series\n",
      "  sex: F\n",
      "  age: C\n",
      "  time: 1998-2000\n",
      "  sample dream: \n",
      "    number: 1-01\n",
      "    date: 1998-03-25\n",
      "    report: I dreamed that a tiger named Shirkan got in the house. We came back and got him out. There was a radio with a microphone and I yelled Shirkan get out of my house....\n",
      "\n",
      "\n",
      "Merri: an artist (315 dreams)\n",
      "  ID: merri\n",
      "  type: series\n",
      "  sex: F\n",
      "  age: A\n",
      "  time: 1999-2000\n",
      "  sample dream: \n",
      "    number: 001\n",
      "    date: 05/13/1999\n",
      "    report: South of Canal St. in Chinatown we were using the Chinese factory after hours to frame pictures and glue them. Joe Fong cut out beautiful, eery, disturbed pictures of blue houses and glued them agains...\n",
      "\n",
      "\n",
      "Miami Home-Lab: Home (171 dreams)\n",
      "  ID: miami-home\n",
      "  type: set\n",
      "  sex: M\n",
      "  age: Y\n",
      "  time: 1963-1965\n",
      "  sample dream: \n",
      "    number: Bart-H-1\n",
      "    date: 12/19/63\n",
      "    report: I dreamt I was at a party. Everyone was having a good time except the hostess. She hated her husband and wanted to kill him. I remember helping her look for something to throw at her husband. We were ...\n",
      "\n",
      "\n",
      "Miami Home-Lab: Lab (274 dreams)\n",
      "  ID: miami-lab\n",
      "  type: set\n",
      "  sex: M\n",
      "  age: Y\n",
      "  time: 1963-1965\n",
      "  sample dream: \n",
      "    number: Bart-A-1\n",
      "    date: 01/31/64\n",
      "    report: I was going to Georgia and I remembered the tune, the song, \"I'm going back to Georgia.\" I heard the last few days on the radio. I was humming the song to myself when I was going back. I was sitting i...\n",
      "\n",
      "\n",
      "Melora (Melvin's wife) (211 dreams)\n",
      "  ID: melora\n",
      "  type: series\n",
      "  sex: F\n",
      "  age: Y\n",
      "  time: 1962\n",
      "  sample dream: \n",
      "    number: 001\n",
      "    date: 06/19/62\n",
      "    report: We were in a used car lot looking for a car, and most of the cars that we saw were little white Fiats like the one that we have. The car lot was up in the mountains and there were all sorts of winding...\n",
      "\n",
      "\n",
      "Melvin (Melora's husband) (128 dreams)\n",
      "  ID: melvin\n",
      "  type: series\n",
      "  sex: M\n",
      "  age: Y\n",
      "  time: 1962\n",
      "  sample dream: \n",
      "    number: 001\n",
      "    date: 06/18/62\n",
      "    report: I was in the basement of another man's house. I don't know who this man was. I knew him in the dream, but I can't recall his identity. There was some sort of tunnel or shaft leading into the basement ...\n",
      "\n",
      "\n",
      "Nancy: Caring & headstrong (44 dreams)\n",
      "  ID: nancy\n",
      "  type: series\n",
      "  sex: F\n",
      "  age: A\n",
      "  time: 1997\n",
      "  sample dream: \n",
      "    number: 001\n",
      "    report: I was parked outside my mom's house and my little girl, Cindy, was still in her car seat. I saw Alice, who used to be married to my boyfriend, Zack, come out from a house across the street. She had a ...\n",
      "\n",
      "\n",
      "The Natural Scientist (234 dreams)\n",
      "  ID: natural_scientist\n",
      "  type: series\n",
      "  sex: M\n",
      "  age: A\n",
      "  time: 1939\n",
      "  sample dream: \n",
      "    number: 001\n",
      "    date: 07/14/39\n",
      "    report: Smoked a cigarette, and felt rather despondent and uncomfortable because of breaking a resolution....\n",
      "\n",
      "\n",
      "Norman: a child molester (1235 dreams)\n",
      "  ID: norman\n",
      "  type: series\n",
      "  sex: M\n",
      "  age: A\n",
      "  time: 1963-1967\n",
      "  sample dream: \n",
      "    number: 001\n",
      "    date: 09/15/63\n",
      "    report: I was in a state of anxiety throughout the dream. I saw a man lying on the floor, shot. I was confined to an institution which I had never seen in real life. I left without permission. Someone tried t...\n",
      "\n",
      "\n",
      "Hall/VdC Norms: Female (491 dreams)\n",
      "  ID: norms-f\n",
      "  type: set\n",
      "  sex: F\n",
      "  age: Y\n",
      "  time: 1940s-1950s\n",
      "  sample dream: \n",
      "    number: 0001\n",
      "    codings: {'set': 'IQ', 'obj': ['AE', 'MO', 'RG', 'FO', 'FO', 'FO'], 'agg': [{'rec': '1FMA +1MKA', 'init': 'D', 'code': '3>'}, {'rec': '1MKA', 'init': 'D', 'code': '1>'}], 'fri': [{'rec': '1MKA', 'init': 'D', 'code': '7='}, {'rec': '1MKA', 'init': 'D', 'code': '1>'}], 'emot': [{'char': 'D', 'code': 'AP'}, {'char': 'D', 'code': 'AP'}, {'char': 'D', 'code': 'HA'}], 'char': ['1MKA', '1IUA', '1MKA', '1FMA'], 'act': [{'rec': 'D', 'init': '1FMA', 'code': 'V>'}, {'rec': 'D', 'init': '1MKA', 'code': 'V>'}, {'init': 'D', 'code': 'C'}, {'rec': 'D', 'init': '1IUA', 'code': 'V>'}, {'init': 'D', 'code': 'C'}, {'rec': 'D', 'init': '1FMA', 'code': 'V>'}, {'rec': '1MKA', 'init': 'D', 'code': 'P>'}, {'init': 'D', 'code': 'P'}, {'rec': '1MKA', 'init': 'D', 'code': 'V>'}, {'rec': '1IUA', 'init': 'D', 'code': 'VR'}], 'mod': ['S-', 'E+', 'I+', 'E+', 'I+', 'I+']}\n",
      "    report: I dreamed it was next summer and that I was going to be married to my boyfriend at home. Mother advised me not to, but said she would not stand in my way. He had very little money and George, after fi...\n",
      "\n",
      "\n",
      "Hall/VdC Norms: Male (500 dreams)\n",
      "  ID: norms-m\n",
      "  type: set\n",
      "  sex: M\n",
      "  age: Y\n",
      "  time: 1940s-1950s\n",
      "  sample dream: \n",
      "    number: 0001\n",
      "    codings: {'set': 'ID', 'obj': 'AV', 'emot': {'char': 'D', 'code': 'AP'}, 'char': ['2JUA', '2FUA', '1MKA'], 'succ': {'char': 'D'}, 'act': [{'rec': '2JUA', 'init': '1MKA', 'code': 'V>'}, {'rec': 'D', 'init': '1MKA', 'code': 'V>'}], 'mod': 'S+'}\n",
      "    report: I was in professor Teimes' classroom in Corp. Finance. The room was larger than usual. We had several women in the class, which is not as it is. The professor was asking the class questions on our wor...\n",
      "\n",
      "\n",
      "Pegasus: a factory worker (1093 dreams)\n",
      "  ID: pegasus\n",
      "  type: series\n",
      "  sex: M\n",
      "  age: A\n",
      "  time: 1949-1964\n",
      "  sample dream: \n",
      "    number: 0001\n",
      "    date: 07/10/49\n",
      "    report: I saw a lot of sparrows playing in the dust on the ground. I shot a couple of bb's at them as they walked and some were as large as quail. I sneaked up on them and caught a baby and an old one in my h...\n",
      "\n",
      "\n",
      "Peruvian men (384 dreams)\n",
      "  ID: peru-m\n",
      "  type: set\n",
      "  sex: M\n",
      "  age: Y\n",
      "  time: 1970\n",
      "  sample dream: \n",
      "    number: M01-1\n",
      "    report: I was with two friends (M) and we were walking and playing jokes on one another. Then we went to pick up a girl I like and got her inside the car, and since I was the last one, they were going to play...\n",
      "\n",
      "\n",
      "Peruvian women (382 dreams)\n",
      "  ID: peru-f\n",
      "  type: set\n",
      "  sex: F\n",
      "  age: Y\n",
      "  time: 1970\n",
      "  sample dream: \n",
      "    number: F01-1\n",
      "    report: I dreamed I was a the beach with my sister, sun-bathing, and next to me there was a girl whom I don't know in real life. Then we went to this girl's house and some boys came towards us and they threw ...\n",
      "\n",
      "\n",
      "Phil 1: teens (106 dreams)\n",
      "  ID: phil1\n",
      "  type: series\n",
      "  sex: M\n",
      "  age: A\n",
      "  time: 1957-1959\n",
      "  sample dream: \n",
      "    number: 1-001\n",
      "    date: 1957-08-05\n",
      "    report: Uncle Albert is getting ready to leave for a vacation and is taking me along.  I don't want to go, and I tell Uncle Albert that someone is coming by to get me at 11:00 a.m., but it makes no difference...\n",
      "\n",
      "\n",
      "Phil 2: late 20s (220 dreams)\n",
      "  ID: phil2\n",
      "  type: series\n",
      "  sex: M\n",
      "  age: A\n",
      "  time: 1971\n",
      "  sample dream: \n",
      "    number: 2-001\n",
      "    date: 1971-01-01\n",
      "    report: I was speaking to Kathy Reynault at night in front of her house (but not her real house).  Her mother was nearby, and I either realized she knew who I was or Kathy actually introduced us, and she didn...\n",
      "\n",
      "\n",
      "Phil 3: retirement (180 dreams)\n",
      "  ID: phil3\n",
      "  type: series\n",
      "  sex: M\n",
      "  age: A\n",
      "  time: 2004\n",
      "  sample dream: \n",
      "    number: 3-001\n",
      "    date: 2004-01-08\n",
      "    report: I am at some kind of resort with a few people I know-maybe three girls and at least one other guy, all of them younger than me, maybe students.  We leave the resort and are on a bridge high above a ve...\n",
      "\n",
      "\n",
      "The Physiologist (86 dreams)\n",
      "  ID: physiologist\n",
      "  type: series\n",
      "  sex: M\n",
      "  age: A\n",
      "  time: 1897-1918\n",
      "  sample dream: \n",
      "    number: 001\n",
      "    date: 10/24/1897\n",
      "    report: I dreamed that I was standing on a small shed and that a mad bull was trying to get at me. He circled round the building and made wonderful leaps to gain the roof. At each plunge he would get his fore...\n",
      "\n",
      "\n",
      "Ringo: from the 1960s (16 dreams)\n",
      "  ID: ringo\n",
      "  type: series\n",
      "  sex: M\n",
      "  age: Y\n",
      "  time: 1964\n",
      "  sample dream: \n",
      "    number: 1\n",
      "    date: 10/07/64\n",
      "    report: My little brother chased me out of the house one day with marshmallows. He was throwing them at me. I decided not to throw any back. I ran to my car and took off. I burned my tires going out he drivew...\n",
      "\n",
      "\n",
      "Samantha: in her 20s (63 dreams)\n",
      "  ID: samantha\n",
      "  type: series\n",
      "  sex: F\n",
      "  age: Y\n",
      "  time: 1992-1999\n",
      "  sample dream: \n",
      "    number: 1\n",
      "    date: 11/09/92\n",
      "    report: Feels like the same ol' chicken soup dream yet I don't know if I've ever dreamt about this soup before. Crush was playing. My dad hired them I think. They played in this church/bar parking lot. Nobody...\n",
      "\n",
      "\n",
      "Seventh grade girls (69 dreams)\n",
      "  ID: seventh_graders\n",
      "  type: set\n",
      "  sex: F\n",
      "  age: T\n",
      "  time: 1996\n",
      "  sample dream: \n",
      "    number: 001-A-14F\n",
      "    date: 03/26/96 [recent]\n",
      "    report: Last week I had a weird dream about being a butterfly and chasing a kite that also looked like a butterfly. It was super strange because I kept on waking up, then going back to sleep and the dream kep...\n",
      "\n",
      "\n",
      "Midwest teenagers (F) (111 dreams)\n",
      "  ID: midwest_teens-f\n",
      "  type: set\n",
      "  sex: F\n",
      "  age: T\n",
      "  time: 1998\n",
      "  sample dream: \n",
      "    number: 001\n",
      "    date: F, age 14\n",
      "    report: All I remember was being with my two friends and I was eating something in my dream and my teeth fell out, but they were like still there. It was like the end/bottom of my teeth fell off. I remembered...\n",
      "\n",
      "\n",
      "Midwest teenagers (M) (83 dreams)\n",
      "  ID: midwest_teens-m\n",
      "  type: set\n",
      "  sex: M\n",
      "  age: T\n",
      "  time: 1998\n",
      "  sample dream: \n",
      "    number: 001\n",
      "    date: M, age 14\n",
      "    report: I was an ant. The queen had just given me my mission. I was to collect the great bounty of food left over by the giants. When I reached the destination, a picnic table, I saw on it a gleaming piece of...\n",
      "\n",
      "\n",
      "West Coast teenage girls (89 dreams)\n",
      "  ID: west_coast_teens\n",
      "  type: set\n",
      "  sex: F\n",
      "  age: T\n",
      "  time: mid-1990s\n",
      "  sample dream: \n",
      "    number: 01\n",
      "    date: F, age 11\n",
      "    report: My dream was about, that I was walking home one day and this boy comes out of nowhere and tells me to be careful but I didn't listen to him. <BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; But then all of a ...\n",
      "\n",
      "\n",
      "Toby: A friendly party animal (33 dreams)\n",
      "  ID: toby\n",
      "  type: series\n",
      "  sex: M\n",
      "  age: Y\n",
      "  time: 2003-2006\n",
      "  sample dream: \n",
      "    number: 001\n",
      "    date: 12/02/03\n",
      "    report: I'm in a strange restaurant on some tall building and it's raining outside. I'm with a lot of people who I know, but now I don't think I did know them. I have to use the bathroom, so I leave the table...\n",
      "\n",
      "\n",
      "Tom: An outgoing man (27 dreams)\n",
      "  ID: tom\n",
      "  type: series\n",
      "  sex: M\n",
      "  age: Y\n",
      "  time: 1990s\n",
      "  sample dream: \n",
      "    number: 01\n",
      "    report: I am in an unfamiliar classroom and I seem to be taking care of or supervising a boy about the age of five. He has blond hair and seems very average in appearance. He is playing quietly by himself and...\n",
      "\n",
      "\n",
      "UCSC women, 1996 (81 dreams)\n",
      "  ID: ucsc_women\n",
      "  type: set\n",
      "  sex: F\n",
      "  age: Y\n",
      "  time: 1996\n",
      "  sample dream: \n",
      "    number: 1\n",
      "    date: 18/female\n",
      "    report: I had this dream at the beginning of the school year, which is my first year here at UC Santa Cruz. In my dream I'm leaving a party or gathering of some type that was being held at the Stevenson dinin...\n",
      "\n",
      "\n",
      "Vickie: a 10-year-old girl (35 dreams)\n",
      "  ID: vickie\n",
      "  type: series\n",
      "  sex: F\n",
      "  age: C\n",
      "  time: 1995\n",
      "  sample dream: \n",
      "    number: 1\n",
      "    date: 04/16/95\n",
      "    report: My mom and I were in the grocery store. I went over to the free cookie area. And this guy gave me a cookie. I had seen the cookies, and they were pretend grasshoppers. I saw a little spider go by (on ...\n",
      "\n",
      "\n",
      "Vietnam Vet: 1970-2008 war dreams (98 dreams)\n",
      "  ID: vietnam_vet\n",
      "  type: series\n",
      "  sex: M\n",
      "  age: A\n",
      "  time: 1970-2008\n",
      "  sample dream: \n",
      "    number: 0001\n",
      "    date: 06/28/1970\n",
      "    report: Nightmare in Cambodia.  In the dream we are being overrun by sappers who have got past the Night Defensive Perimeter trips and claymores and now crawl forward. I wake up and see a boot tread close to ...\n",
      "\n",
      "\n",
      "Vietnam Vet: 2015 dreams (32 dreams)\n",
      "  ID: vietnam_vet2\n",
      "  type: series\n",
      "  sex: M\n",
      "  age: A\n",
      "  time: 2015\n",
      "  sample dream: \n",
      "    number: 0001\n",
      "    date: 05/30/2015\n",
      "    report: I'm in the Army, I have been to war, and now am in a large military warehouse. I'm climbing up a huge pile of boxes that are packed with thousands of small black toy trains. An MP comes by and harasse...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dreamer in doc['dreambank']['collection']:\n",
    "    print dreamer['name'] + ' (' + str(len(dreamer['dream'][:])) + ' dreams)' \n",
    "    print '  ID: ' + dreamer['id']\n",
    "    print '  type: ' + dreamer['type']\n",
    "    print '  sex: ' + dreamer['sex']\n",
    "    print '  age: ' + dreamer['age']\n",
    "    \n",
    "    try:\n",
    "        print '  time: ' + dreamer['time']\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print '  sample dream: ' \n",
    "    \n",
    "    odict = dreamer['dream'][0]\n",
    "    for key, value in odict.items():\n",
    "        if convert(key) == 'report':\n",
    "            print '    report: ' + left(convert(value), 200) + '...'\n",
    "        else:\n",
    "            print '    ' + convert(key) + ': ' + str(convert(value))\n",
    "        \n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Dream collections from individuals---\n",
      "\n",
      "{alta} Alta: a detailed dreamer (422 dreams) [F]\n",
      "w266 ID: 1\n",
      "\n",
      "\n",
      "{angie} Angie: age 18 & 20 (48 dreams) [F]\n",
      "w266 ID: 2\n",
      "\n",
      "\n",
      "{arlie} Arlie: a middle-aged woman (212 dreams) [F]\n",
      "w266 ID: 3\n",
      "\n",
      "\n",
      "{b} Barb Sanders (3116 dreams) [F]\n",
      "w266 ID: 4\n",
      "{b2} Barb Sanders #2 (1138 dreams) [F]\n",
      "w266 ID: 4\n",
      "\n",
      "\n",
      "{bosnak} Robert Bosnak: A dream analyst (53 dreams) [M]\n",
      "w266 ID: 5\n",
      "\n",
      "\n",
      "{chris} Chris: a transvestite (100 dreams) [M]\n",
      "w266 ID: 6\n",
      "\n",
      "\n",
      "{chuck} Chuck: a physical scientist (75 dreams) [M]\n",
      "w266 ID: 7\n",
      "\n",
      "\n",
      "{dahlia} Dahlia: concerns with appearance (24 dreams) [F]\n",
      "w266 ID: 8\n",
      "\n",
      "\n",
      "{david} David: teenage dreams (166 dreams) [M]\n",
      "w266 ID: 9\n",
      "\n",
      "\n",
      "{dorothea} Dorothea: 53 years of dreams (900 dreams) [F]\n",
      "w266 ID: 10\n",
      "\n",
      "\n",
      "{ed} Ed: dreams of his late wife (143 dreams) [M]\n",
      "w266 ID: 11\n",
      "\n",
      "\n",
      "{edna} Edna: a blind woman (19 dreams) [F]\n",
      "w266 ID: 12\n",
      "\n",
      "\n",
      "{elizabeth} Elizabeth: a woman in her 40s (1707 dreams) [F]\n",
      "w266 ID: 13\n",
      "\n",
      "\n",
      "{emma} Emma: 48 years of dreams (1521 dreams) [F]\n",
      "w266 ID: 14\n",
      "\n",
      "\n",
      "{emmas_husband} Emma's Husband (72 dreams) [M]\n",
      "w266 ID: 15\n",
      "\n",
      "\n",
      "{esther} Esther: an adolescent girl (110 dreams) [F]\n",
      "w266 ID: 16\n",
      "\n",
      "\n",
      "{izzy} Izzy (all) (4352 dreams) [F]\n",
      "w266 ID: 17\n",
      "\n",
      "\n",
      "{jasmine} Jasmine (all) (664 dreams) [F]\n",
      "w266 ID: 18\n",
      "\n",
      "\n",
      "{jeff} Jeff: a lucid dreamer (87 dreams) [M]\n",
      "w266 ID: 19\n",
      "\n",
      "\n",
      "{joan} Joan: a lesbian (42 dreams) [F]\n",
      "w266 ID: 20\n",
      "\n",
      "\n",
      "{kenneth} Kenneth (2022 dreams) [M]\n",
      "w266 ID: 21\n",
      "\n",
      "\n",
      "{madeline1-hs} Madeline 1: High School (98 dreams) [F]\n",
      "w266 ID: 22\n",
      "{madeline2-dorms} Madeline 2: College Dorms (186 dreams) [F]\n",
      "w266 ID: 22\n",
      "{madeline3-offcampus} Madeline 3: Off-Campus (348 dreams) [F]\n",
      "w266 ID: 22\n",
      "{madeline4-postgrad} Madeline 4: After College (294 dreams) [F]\n",
      "w266 ID: 22\n",
      "\n",
      "\n",
      "{mack} Mack: A poor recaller (38 dreams) [M]\n",
      "w266 ID: 23\n",
      "\n",
      "\n",
      "{mark} Mark: a young boy (23 dreams) [M]\n",
      "w266 ID: 24\n",
      "\n",
      "\n",
      "{melissa} Melissa: a young girl (89 dreams) [F]\n",
      "w266 ID: 25\n",
      "\n",
      "\n",
      "{merri} Merri: an artist (315 dreams) [F]\n",
      "w266 ID: 26\n",
      "\n",
      "\n",
      "{melora} Melora (Melvin's wife) (211 dreams) [F]\n",
      "w266 ID: 27\n",
      "\n",
      "\n",
      "{melvin} Melvin (Melora's husband) (128 dreams) [M]\n",
      "w266 ID: 28\n",
      "\n",
      "\n",
      "{nancy} Nancy: Caring & headstrong (44 dreams) [F]\n",
      "w266 ID: 29\n",
      "\n",
      "\n",
      "{natural_scientist} The Natural Scientist (234 dreams) [M]\n",
      "w266 ID: 30\n",
      "\n",
      "\n",
      "{norman} Norman: a child molester (1235 dreams) [M]\n",
      "w266 ID: 31\n",
      "\n",
      "\n",
      "{pegasus} Pegasus: a factory worker (1093 dreams) [M]\n",
      "w266 ID: 32\n",
      "\n",
      "\n",
      "{phil1} Phil 1: teens (106 dreams) [M]\n",
      "w266 ID: 33\n",
      "{phil2} Phil 2: late 20s (220 dreams) [M]\n",
      "w266 ID: 33\n",
      "{phil3} Phil 3: retirement (180 dreams) [M]\n",
      "w266 ID: 33\n",
      "\n",
      "\n",
      "{physiologist} The Physiologist (86 dreams) [M]\n",
      "w266 ID: 34\n",
      "\n",
      "\n",
      "{ringo} Ringo: from the 1960s (16 dreams) [M]\n",
      "w266 ID: 35\n",
      "\n",
      "\n",
      "{samantha} Samantha: in her 20s (63 dreams) [F]\n",
      "w266 ID: 36\n",
      "\n",
      "\n",
      "{toby} Toby: A friendly party animal (33 dreams) [M]\n",
      "w266 ID: 37\n",
      "\n",
      "\n",
      "{tom} Tom: An outgoing man (27 dreams) [M]\n",
      "w266 ID: 38\n",
      "\n",
      "\n",
      "{vickie} Vickie: a 10-year-old girl (35 dreams) [F]\n",
      "w266 ID: 39\n",
      "\n",
      "\n",
      "{vietnam_vet} Vietnam Vet: 1970-2008 war dreams (98 dreams) [M]\n",
      "w266 ID: 40\n",
      "{vietnam_vet2} Vietnam Vet: 2015 dreams (32 dreams) [M]\n",
      "w266 ID: 40\n",
      "\n",
      "\n",
      "Total Number of individuals to test vs. 'others': 40\n"
     ]
    }
   ],
   "source": [
    "print '---Dream collections from individuals---' + '\\n'\n",
    "MultIDs = ['b', 'madeline1-hs', 'madeline2-dorms', 'madeline3-offcampus', 'phil1', 'phil2', 'vietnam_vet']\n",
    "NumberOfSeries = 1\n",
    "\n",
    "for dreamer in doc['dreambank']['collection']:\n",
    "    if dreamer['type'] == 'series':\n",
    "        print '{' + dreamer['id']  + '} ' + dreamer['name'] + ' (' + str(len(dreamer['dream'][:])) + ' dreams)' + ' [' + dreamer['sex'] + ']'    \n",
    "        dreamer['w266ID'] = NumberOfSeries\n",
    "        \n",
    "        # Assign a dreamer ID that groups the same dreamers together \n",
    "        # and skips the dream collections of multiple dreamers\n",
    "        print 'w266 ID: ' + str(dreamer['w266ID'])\n",
    "        if dreamer['id'] not in MultIDs:\n",
    "            print '\\n'\n",
    "            NumberOfSeries += 1\n",
    "    else:\n",
    "        dreamer['w266ID'] = 0\n",
    "        \n",
    "print \"Total Number of individuals to test vs. 'others': \" + str(NumberOfSeries - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Dreams: 26000\n"
     ]
    }
   ],
   "source": [
    "DreamNum = 0\n",
    "\n",
    "for dreamer in doc['dreambank']['collection']:\n",
    "    for odict in dreamer['dream']:\n",
    "        for key, value in odict.items():            \n",
    "            if convert(key) == 'report':\n",
    "                DreamNum += 1\n",
    "\n",
    "print \"Total Dreams: \" + str(DreamNum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA - Reduce To Noun-Only And Lemmatize\n",
    "\n",
    "For our topic modeling, we will want to lemmatize the corpus and reduce to nouns-only. However, before we get to topic modeling, it will be helpful to test out the lemmatization and noun-only reduction techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alta: a detailed dreamer (422 dreams) [F]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'The', u'one', u'at', u'the', u'Meads', u\"'s\", u'house', u',', u'where', u'it', u\"'s\", u'bigger', u'inside', u'than', u'out', u';', u'there', u\"'s\", u'a', u'European', u'village', u'just', u'inside', u',', u'with', u'a', u'cobblestone', u'street', u'and', u'a', u'Pied-Piper', u'sort', u'of', u'man', u'with', u'curly', u'hair', u',', u'he', u'can', u'do', u'things', u'like', u'juggle', u'-', u'I', u'go', u'up', u'the', u'back', u'stairs', u'[', u'there', u'are', u\"n't\", u'any', u'in', u'the', u'real', u'house', u']', u'and', u'then', u'down', u'the', u'other', u'side', u'[', u'since', u'there', u\"'s\", u'a', u'second', u'set', u',', u'immediately', u']', u'then', u'down', u'a', u'short', u'empty', u'hallway', u'that', u'turns', u'a', u'corner', u',', u'where', u'I', u'find', u'a', u'tiny', u'room', u'...', u'a', u'young', u'woman', u'with', u'shoulder-length', u'blonde', u'hair', u'in', u'a', u'pageboy', u'is', u'there', u',', u'cooking', u'at', u'a', u'stove', u'that', u'almost', u'fills', u'the', u'room', u'...', u'she', u\"'s\", u'nice', u'to', u'me', u'.', u'Now', u'outside', u',', u'I', u\"'m\", u'waiting', u'for', u'my', u'aunt', u'to', u'pick', u'me', u'up', u'-', u'she', u'arrives', u'in', u'a', u'little', u'round', u'convertible', u'and', u'we', u'go', u'for', u'a', u'drive', u',', u'not', u'very', u'far', u'-', u'we', u'cross', u'a', u'little', u'bridge', u'over', u'a', u'creek', u',', u'then', u'double', u'back', u'and', u'she', u'drops', u'me', u'off', u'at', u'the', u'house', u'again', u'.', u'Inside', u'(', u'?', u')', u'I', u'sit', u'with', u'a', u'couple', u'of', u'people', u',', u'playing', u'with', u'a', u'string', u'of', u'blue', u'balloons', u'.']\n",
      "noun only: [u'one', u'Meads', u'house', u'village', u'cobblestone', u'street', u'sort', u'man', u'hair', u'things', u'juggle', u'[', u'house', u']', u'side', u'[', u'set', u'hallway', u'corner', u'room', u'woman', u'blonde', u'hair', u'pageboy', u'stove', u'room', u'aunt', u'round', u'drive', u'bridge', u'creek', u'house', u'Inside', u'couple', u'people', u'string', u'balloons']\n",
      "lemmatized: [u'one', u'Meads', u'house', u'village', u'cobblestone', u'street', u'sort', u'man', u'hair', u'thing', u'juggle', u'[', u'house', u']', u'side', u'[', u'set', u'hallway', u'corner', u'room', u'woman', u'blonde', u'hair', u'pageboy', u'stove', u'room', u'aunt', u'round', u'drive', u'bridge', u'creek', u'house', u'Inside', u'couple', u'people', u'string', u'balloon']\n",
      "Counter({u'house': 3, u'hair': 2, u'[': 2, u'room': 2, u'bridge': 1, u'set': 1, u'creek': 1, u'people': 1, u'stove': 1, u'one': 1, u'street': 1, u'village': 1, u'corner': 1, u'blonde': 1, u'string': 1, u'pageboy': 1, u'balloon': 1, u'juggle': 1, u'couple': 1, u'sort': 1, u'hallway': 1, u'woman': 1, u'Meads': 1, u'Inside': 1, u'cobblestone': 1, u']': 1, u'man': 1, u'drive': 1, u'round': 1, u'thing': 1, u'aunt': 1, u'side': 1})\n",
      "\n",
      "\n",
      "Angie: age 18 & 20 (48 dreams) [F]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'My', u'memory', u'of', u'this', u'dream', u'is', u'vague', u'.', u'I', u'think', u'the', u'setting', u'is', u'on', u'a', u'college', u'campus', u'.', u'I', u\"'m\", u'in', u'a', u'cafe', u'and', u'two', u'elderly', u'ladies', u'walk', u'in', u'and', u'start', u'talking', u'to', u'me', u'about', u'a', u'university', u'that', u'a', u'guy', u'I', u'am', u'dating', u'got', u'into', u'for', u'law', u'school', u'.', u'They', u'were', u'saying', u'that', u'I', u'was', u'accepted', u'.', u'I', u'thought', u'that', u'this', u'information', u'was', u'weird', u'because', u'I', u'did', u\"n't\", u'even', u'apply', u'to', u'this', u'school', u'.', u'I', u'got', u'the', u'feeling', u'that', u'while', u'I', u'was', u'talking', u'to', u'these', u'ladies', u',', u'that', u'they', u'were', u'interviewing', u'me', u'as', u'art', u'of', u'the', u'orientation', u'to', u'go', u'there', u'.', u'I', u'was', u'also', u'pregnant', u'in', u'the', u'dream', u'and', u'he', u'cafe', u'that', u'I', u'was', u'in', u'was', u'a', u'hospital', u'cafe', u'.', u'The', u'guy', u'I', u'am', u'dating', u'is', u'in', u'the', u'dream', u'and', u'we', u'were', u'talking', u',', u'but', u'I', u\"'m\", u'not', u'sure', u'about', u'what', u'.']\n",
      "noun only: [u'memory', u'dream', u'setting', u'college', u'campus', u'ladies', u'university', u'guy', u'law', u'school', u'information', u'school', u'feeling', u'ladies', u'art', u'orientation', u'dream', u'hospital', u'cafe', u'guy', u'dream']\n",
      "lemmatized: [u'memory', u'dream', u'setting', u'college', u'campus', u'lady', u'university', u'guy', u'law', u'school', u'information', u'school', u'feeling', u'lady', u'art', u'orientation', u'dream', u'hospital', u'cafe', u'guy', u'dream']\n",
      "Counter({u'dream': 3, u'school': 2, u'guy': 2, u'lady': 2, u'hospital': 1, u'information': 1, u'feeling': 1, u'art': 1, u'orientation': 1, u'university': 1, u'cafe': 1, u'setting': 1, u'college': 1, u'memory': 1, u'law': 1, u'campus': 1})\n",
      "\n",
      "\n",
      "Arlie: a middle-aged woman (212 dreams) [F]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'am', u'in', u'an', u'office', u'in', u'the', u'town', u'next', u'to', u'the', u'town', u'I', u'grew', u'up', u'in', u'.', u'Everyone', u'is', u'taking', u'a', u'rest', u'.', u'I', u'have', u'to', u'go', u'to', u'the', u'bathroom', u',', u'but', u'there', u'is', u'no', u'toilet', u'so', u'I', u'use', u'an', u'empty', u'can', u'.', u'I', u'dump', u'the', u'can', u'in', u'the', u'toilet', u'(', u'which', u'is', u'now', u'there', u')', u'.']\n",
      "noun only: [u'office', u'town', u'town', u'Everyone', u'rest', u'bathroom', u'toilet', u'toilet']\n",
      "lemmatized: [u'office', u'town', u'town', u'Everyone', u'rest', u'bathroom', u'toilet', u'toilet']\n",
      "Counter({u'town': 2, u'toilet': 2, u'bathroom': 1, u'Everyone': 1, u'office': 1, u'rest': 1})\n",
      "\n",
      "\n",
      "Barb Sanders (3116 dreams) [F]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'had', u'the', u'neatest', u'dream', u'about', u'Blake', u',', u'me', u',', u'Reta', u'and', u'Bill', u'E', u'.']\n",
      "noun only: [u'dream', u'Blake', u'Reta', u'Bill', u'E']\n",
      "lemmatized: [u'dream', u'Blake', u'Reta', u'Bill', u'E']\n",
      "Counter({u'E': 1, u'Blake': 1, u'Bill': 1, u'dream': 1, u'Reta': 1})\n",
      "\n",
      "\n",
      "Barb Sanders #2 (1138 dreams) [F]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'[', u'``', u'I', u'have', u'the', u'big', u'gun', u'.', u\"''\", u']', u'I', u'am', u'on', u'a', u'beach', u'.', u'There', u'is', u'some', u'group', u'game', u'going', u'on', u'where', u'people', u'run', u'toward', u'the', u'ocean', u'with', u'guns', u'.', u'We', u'had', u'been', u'trained', u'by', u'the', u'army', u'.', u'Off', u'they', u'go', u'and', u'I', u'am', u'in', u'my', u'wheelchair', u'and', u'going', u'as', u'fast', u'as', u'I', u'can', u',', u'but', u'in', u'the', u'sand', u',', u'it', u'is', u\"n't\", u'fast', u'enough', u'.', u'The', u'group', u'arrives', u'at', u'the', u'shore', u'and', u'fire', u'their', u'guns', u'.', u'They', u'all', u'run', u'out', u'of', u'bullets', u'just', u'about', u'when', u'I', u'get', u'there', u'.', u'I', u'take', u'careful', u'aim', u'at', u'the', u'big', u'boulders', u'out', u'in', u'the', u'surf', u'and', u'fire', u'one', u'shot', u'from', u'a', u'huge', u'pistol', u'.', u'I', u'blow', u'the', u'boulders', u'to', u'smithereens', u'.', u'It', u\"'s\", u'a', u'large', u'explosion', u'.', u'Now', u'I', u'go', u'back', u'to', u'the', u'sand', u'to', u'rest', u'and', u'play', u'with', u'Charla', u'.', u'We', u'are', u'hanging', u'out', u'together', u'when', u'some', u'man', u'picks', u'up', u'our', u'backpack', u',', u'with', u'Charla', u\"'s\", u'boots', u'in', u'it', u'and', u'walks', u'off', u'.', u'I', u'call', u'after', u'him', u'.', u'He', u'drops', u'it', u'.', u'I', u'see', u'the', u'boots', u'laying', u'in', u'the', u'sand', u'.', u'I', u'now', u'want', u'to', u'return', u'the', u'gun', u'to', u'the', u'army', u'but', u'ca', u\"n't\", u'find', u'any', u'army', u'people', u'around', u'.', u'I', u'wander', u'around', u'and', u'see', u'a', u'rifle', u'in', u'the', u'sand', u'.', u'I', u'return', u'to', u'where', u'Charla', u'is', u'.', u'We', u'sit', u'and', u'the', u'wind', u'picks', u'up', u'until', u'the', u'sand', u'storm', u'is', u'very', u'powerful', u'.', u'I', u'hold', u'a', u'pink', u'cloth', u',', u'perhaps', u'a', u'scarf', u'up', u'behind', u'Charla', u'to', u'protect', u'us', u'.', u'Now', u'I', u'notice', u'street', u'lights', u'going', u'by', u'and', u'see', u'that', u'I', u'am', u'now', u'riding', u'in', u'the', u'back', u'of', u'a', u'pickup', u'truck', u'.', u'My', u'own', u'truck', u'.', u'I', u'figure', u'Ellie', u'my', u'daughter', u'got', u'tired', u'of', u'hanging', u'out', u'on', u'the', u'beach', u'and', u'decided', u'to', u'drive', u'us', u'home', u'.', u'I', u'try', u'to', u'look', u'into', u'the', u'cab', u'but', u'the', u'glass', u'is', u'difficult', u'to', u'see', u'through', u'.', u'I', u'see', u'the', u'shadows', u'of', u'four', u'people', u'.', u'Two', u'men', u'and', u'two', u'women', u'.', u'Some', u'punk', u'teens', u'had', u'stolen', u'my', u'truck', u'.', u'I', u'ask', u'one', u'woman', u'to', u'stop', u'the', u'truck', u'and', u'she', u'laughs', u'and', u'says', u',', u'``', u'Hey', u',', u'I', u\"'m\", u'only', u'the', u'girlfriend', u'.', u\"''\", u'I', u'sneer', u'and', u'say', u'``', u'Oh', u',', u'That', u'kind', u'of', u'a', u'woman', u'huh', u'?', u\"''\", u'I', u'turn', u'to', u'the', u'ones', u'driving', u'and', u'tell', u'them', u'to', u'stop', u'the', u'truck', u'.', u'They', u'laugh', u'.', u'So', u'I', u'pull', u'out', u'the', u'big', u'gun', u'I', u'still', u'had', u'and', u'smash', u'open', u'the', u'glass', u'threatening', u'to', u'shoot', u'.', u'They', u'pull', u'over', u'and', u'get', u'out', u'.', u'Charla', u'and', u'I', u'get', u'into', u'the', u'truck', u'to', u'drive', u'away', u'.']\n",
      "noun only: [u'gun', u']', u'beach', u'group', u'game', u'people', u'ocean', u'guns', u'army', u'wheelchair', u'sand', u'group', u'shore', u'guns', u'bullets', u'aim', u'boulders', u'surf', u'fire', u'shot', u'pistol', u'boulders', u'smithereens', u'explosion', u'sand', u'Charla', u'man', u'backpack', u'Charla', u'boots', u'boots', u'sand', u'gun', u'army', u'people', u'rifle', u'sand', u'Charla', u'wind', u'sand', u'storm', u'pink', u'cloth', u'scarf', u'Charla', u'lights', u'back', u'truck', u'truck', u'Ellie', u'daughter', u'beach', u'home', u'cab', u'glass', u'shadows', u'people', u'men', u'women', u'punk', u'teens', u'truck', u'woman', u'truck', u'Hey', u'girlfriend', u'kind', u'woman', u'huh', u'ones', u'truck', u'gun', u'glass', u'Charla', u'truck']\n",
      "lemmatized: [u'gun', u']', u'beach', u'group', u'game', u'people', u'ocean', u'gun', u'army', u'wheelchair', u'sand', u'group', u'shore', u'gun', u'bullet', u'aim', u'boulder', u'surf', u'fire', u'shot', u'pistol', u'boulder', u'smithereens', u'explosion', u'sand', u'Charla', u'man', u'backpack', u'Charla', u'boot', u'boot', u'sand', u'gun', u'army', u'people', u'rifle', u'sand', u'Charla', u'wind', u'sand', u'storm', u'pink', u'cloth', u'scarf', u'Charla', u'light', u'back', u'truck', u'truck', u'Ellie', u'daughter', u'beach', u'home', u'cab', u'glass', u'shadow', u'people', u'men', u'woman', u'punk', u'teen', u'truck', u'woman', u'truck', u'Hey', u'girlfriend', u'kind', u'woman', u'huh', u'one', u'truck', u'gun', u'glass', u'Charla', u'truck']\n",
      "Counter({u'truck': 6, u'gun': 5, u'sand': 5, u'Charla': 5, u'people': 3, u'woman': 3, u'group': 2, u'army': 2, u'boot': 2, u'beach': 2, u'boulder': 2, u'glass': 2, u'surf': 1, u'shot': 1, u'back': 1, u'one': 1, u'smithereens': 1, u'Ellie': 1, u'girlfriend': 1, u'home': 1, u'pink': 1, u'backpack': 1, u'shore': 1, u'teen': 1, u'storm': 1, u'pistol': 1, u'explosion': 1, u'fire': 1, u'men': 1, u'huh': 1, u'Hey': 1, u'cloth': 1, u'game': 1, u'rifle': 1, u'shadow': 1, u']': 1, u'man': 1, u'kind': 1, u'daughter': 1, u'bullet': 1, u'light': 1, u'wheelchair': 1, u'punk': 1, u'ocean': 1, u'aim': 1, u'scarf': 1, u'cab': 1, u'wind': 1})\n",
      "\n",
      "\n",
      "Bay Area girls: Grades 4-6 (234 dreams) [F]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'was', u'looking', u'at', u'the', u'moon', u'and', u'another', u'thing', u'that', u'kinda', u'looked', u'like', u'a', u'cutesie-poo', u'sun', u'.', u'I', u'was', u'with', u'my', u'friend', u'Anna', u'L.', u',', u'and', u'Rita', u'L.', u',', u'her', u'mom', u',', u'but', u'it', u'was', u\"n't\", u'really', u'Anna', u\"'s\", u'mom', u'-', u'it', u'was', u'someone', u'who', u'looked', u'like', u'her', u'mom', u'.', u'And', u'then', u'Rita', u'got', u'transported', u'to', u'the', u'moon', u'and', u'told', u'the', u'people', u'on', u'the', u'moon', u'to', u'take', u'a', u'man', u'who', u'looked', u'like', u'Al', u'L.', u'(', u'Anna', u\"'s\", u'father', u')', u'and', u'transport', u'him', u'to', u'the', u'moon', u'.', u'So', u'they', u'went', u'down', u'to', u'earth', u'(', u'the', u'people', u'on', u'the', u'moon', u')', u'to', u'get', u'the', u'person', u'who', u'looked', u'like', u'Al', u',', u'but', u'Al', u'-', u'or', u'rather', u'the', u'person', u'who', u'looked', u'like', u'him', u'-', u'did', u'not', u'want', u'to', u'go', u'.', u'The', u'people', u'who', u'came', u'from', u'the', u'moon', u'told', u'the', u'person', u'who', u'looked', u'like', u'Al', u'that', u'the', u'person', u'who', u'looked', u'like', u'Rita', u'said', u'to', u'bring', u'him', u'to', u'the', u'moon', u'.', u'But', u'then', u'the', u'person', u'who', u'resembled', u'Rita', u'said', u'that', u'she', u'never', u'said', u'that', u'.', u'But', u'the', u'people', u'from', u'the', u'moon', u'said', u'they', u'had', u'proof', u'because', u'they', u'had', u'one', u'of', u'Rita', u\"'s\", u'200', u'earrings', u'.', u'And', u'that', u'was', u'the', u'end', u'.', u'<', u'UL', u'>', u'<', u'LI', u'>', u'Setting', u':', u'Part', u'of', u'it', u'was', u'in', u'a', u'strange', u'room', u',', u'part', u'of', u'it', u'was', u'on', u'the', u'moon.', u'<', u'/LI', u'>', u'<', u'LI', u'>', u'Characters', u':', u'One', u'was', u'my', u'friend', u'Anna', u'-', u'she', u'is', u'my', u'best', u'friend', u',', u'and', u'two', u'people', u'who', u'looked', u'like', u'Anna', u\"'s\", u'parents', u',', u'Al', u'&', u'Rita', u'.', u'Anna', u'had', u'blonde', u'hair', u'and', u'blue', u'eyes', u',', u'her', u'hair', u'was', u'medium', u'long', u'.', u'Rita', u'had', u'short', u'brown', u'hair', u'-', u'she', u'was', u'wearing', u'a', u'blue', u'shirt', u'with', u'orange', u'pants', u'.', u'Al', u'was', u'wearing', u'jean', u'shorts', u'and', u'a', u'red', u'flannel', u'shirt.', u'<', u'/LI', u'>', u'<', u'LI', u'>', u'Feelings', u'and', u'Thoughts', u':', u'Worried', u'.', u'Maybe', u'anxious', u'.', u'Was', u\"n't\", u'that', u'a', u'weird', u'dream', u'?', u'!', u'It', u'was', u'very', u'weird.', u'<', u'/LI', u'>', u'<', u'/UL', u'>']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun only: [u'moon', u'thing', u'sun', u'friend', u'Anna', u'L.', u'Rita', u'L.', u'mom', u'Anna', u'mom', u'someone', u'mom', u'Rita', u'moon', u'people', u'moon', u'man', u'Al', u'L.', u'Anna', u'father', u'moon', u'earth', u'people', u'moon', u'person', u'Al', u'Al', u'person', u'people', u'moon', u'person', u'Al', u'person', u'Rita', u'moon', u'person', u'Rita', u'people', u'moon', u'proof', u'Rita', u'earrings', u'end', u'UL', u'>', u'<', u'LI', u'>', u'Setting', u'Part', u'room', u'part', u'moon.', u'<', u'/LI', u'>', u'<', u'LI', u'>', u'Characters', u'friend', u'Anna', u'friend', u'people', u'Anna', u'parents', u'Al', u'Rita', u'Anna', u'hair', u'eyes', u'hair', u'medium', u'Rita', u'hair', u'shirt', u'pants', u'Al', u'shorts', u'flannel', u'shirt.', u'<', u'/LI', u'>', u'<', u'LI', u'>', u'Feelings', u'Thoughts', u'Worried', u'dream', u'<', u'/LI', u'>', u'<', u'/UL', u'>']\n",
      "lemmatized: [u'moon', u'thing', u'sun', u'friend', u'Anna', u'L.', u'Rita', u'L.', u'mom', u'Anna', u'mom', u'someone', u'mom', u'Rita', u'moon', u'people', u'moon', u'man', u'Al', u'L.', u'Anna', u'father', u'moon', u'earth', u'people', u'moon', u'person', u'Al', u'Al', u'person', u'people', u'moon', u'person', u'Al', u'person', u'Rita', u'moon', u'person', u'Rita', u'people', u'moon', u'proof', u'Rita', u'earring', u'end', u'UL', u'>', u'<', u'LI', u'>', u'Setting', u'Part', u'room', u'part', u'moon.', u'<', u'/LI', u'>', u'<', u'LI', u'>', u'Characters', u'friend', u'Anna', u'friend', u'people', u'Anna', u'parent', u'Al', u'Rita', u'Anna', u'hair', u'eye', u'hair', u'medium', u'Rita', u'hair', u'shirt', u'pant', u'Al', u'short', u'flannel', u'shirt.', u'<', u'/LI', u'>', u'<', u'LI', u'>', u'Feelings', u'Thoughts', u'Worried', u'dream', u'<', u'/LI', u'>', u'<', u'/UL', u'>']\n",
      "Counter({u'moon': 8, u'>': 8, u'Rita': 7, u'<': 7, u'Anna': 6, u'Al': 6, u'people': 5, u'person': 5, u'/LI': 3, u'hair': 3, u'LI': 3, u'friend': 3, u'L.': 3, u'mom': 3, u'earring': 1, u'Setting': 1, u'earth': 1, u'medium': 1, u'end': 1, u'shirt': 1, u'sun': 1, u'pant': 1, u'father': 1, u'shirt.': 1, u'Part': 1, u'Characters': 1, u'UL': 1, u'someone': 1, u'parent': 1, u'/UL': 1, u'Worried': 1, u'part': 1, u'eye': 1, u'flannel': 1, u'man': 1, u'short': 1, u'room': 1, u'moon.': 1, u'Feelings': 1, u'thing': 1, u'Thoughts': 1, u'dream': 1, u'proof': 1})\n",
      "\n",
      "\n",
      "Bay Area girls: Grades 7-9 (154 dreams) [F]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'was', u'sitting', u'with', u'some', u'girlfriends', u'on', u'a', u'grassy', u'hill', u'when', u'all', u'of', u'a', u'sudden', u'a', u'fire', u'broke', u'out', u'and', u'everyone', u'was', u'running', u'.', u'Then', u'the', u'fire', u'disappeared', u'and', u'we', u'all', u'turned', u'into', u'cat', u'people', u'and', u'went', u'swimming', u'in', u'a', u'pool', u'of', u'milk', u'.', u'<', u'UL', u'>', u'<', u'LI', u'>', u'Setting', u':', u'A', u'hill', u'with', u'a', u'stage', u'in', u'the', u'bottom.', u'<', u'/LI', u'>', u'<', u'LI', u'>', u'Characters', u':', u'Friends', u':', u'Chloe', u',', u'Robin', u'F.', u',', u'Leah', u',', u'Robin', u'L.', u',', u'Holly', u',', u'etc', u'.', u'Everyone', u'in', u'the', u'grade.', u'<', u'/LI', u'>', u'<', u'LI', u'>', u'Feelings', u'and', u'Thoughts', u':', u'Happy', u',', u'scared', u'.', u'I', u'was', u'scared', u'when', u'the', u'fire', u'broke', u'out', u',', u'then', u'happy', u'when', u'we', u'went', u'swimming.', u'<', u'/LI', u'>', u'<', u'/UL', u'>']\n",
      "noun only: [u'girlfriends', u'grassy', u'hill', u'fire', u'everyone', u'fire', u'people', u'pool', u'milk', u'UL', u'>', u'<', u'LI', u'>', u'Setting', u'hill', u'stage', u'bottom.', u'<', u'/LI', u'>', u'<', u'LI', u'>', u'Characters', u'Friends', u'Chloe', u'Robin', u'F.', u'Leah', u'Robin', u'L.', u'Holly', u'Everyone', u'grade.', u'<', u'/LI', u'>', u'<', u'LI', u'>', u'Feelings', u'Thoughts', u'fire', u'<', u'/LI', u'>', u'<', u'/UL', u'>']\n",
      "lemmatized: [u'girlfriend', u'grassy', u'hill', u'fire', u'everyone', u'fire', u'people', u'pool', u'milk', u'UL', u'>', u'<', u'LI', u'>', u'Setting', u'hill', u'stage', u'bottom.', u'<', u'/LI', u'>', u'<', u'LI', u'>', u'Characters', u'Friends', u'Chloe', u'Robin', u'F.', u'Leah', u'Robin', u'L.', u'Holly', u'Everyone', u'grade.', u'<', u'/LI', u'>', u'<', u'LI', u'>', u'Feelings', u'Thoughts', u'fire', u'<', u'/LI', u'>', u'<', u'/UL', u'>']\n",
      "Counter({u'>': 8, u'<': 7, u'/LI': 3, u'LI': 3, u'fire': 3, u'hill': 2, u'Robin': 2, u'everyone': 1, u'grassy': 1, u'people': 1, u'bottom.': 1, u'Setting': 1, u'girlfriend': 1, u'Holly': 1, u'milk': 1, u'/UL': 1, u'L.': 1, u'Leah': 1, u'Characters': 1, u'Friends': 1, u'Thoughts': 1, u'grade.': 1, u'pool': 1, u'stage': 1, u'Everyone': 1, u'Chloe': 1, u'Feelings': 1, u'UL': 1, u'F.': 1})\n",
      "\n",
      "\n",
      "Blind dreamers (F) (238 dreams) [F]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'was', u'at', u'my', u'parent', u\"'s\", u'house', u'that', u'I', u'grew', u'up', u'in', u'and', u'I', u'was', u'going', u'out', u'with', u'a', u'friend', u'that', u'I', u'have', u'here', u'.', u'I', u'saw', u'the', u'house', u'in', u'fairly', u'clear', u'detail', u'as', u'if', u'I', u'were', u'seeing', u'it', u',', u'and', u'I', u'went', u'out', u'with', u'my', u'friend', u'to', u'her', u'house', u'which', u'was', u'another', u'large', u'place', u'with', u'a', u'lot', u'of', u'stairs', u'.', u'Again', u'there', u'were', u'a', u'lot', u'of', u'visual', u'details', u'.', u'I', u'could', u'see', u'a', u'lot', u'of', u'colors', u'in', u'the', u'rooms', u',', u'the', u'colors', u'of', u'the', u'carpeting', u'and', u'the', u'furniture', u'and', u'all', u'the', u'different', u'shapes', u'and', u'different', u'lighting', u'as', u'we', u'moved', u'from', u'room', u'to', u'room', u',', u'and', u'she', u'and', u'I', u'were', u'talking', u',', u'I', u'do', u\"n't\", u'remember', u'what', u'about', u',', u'and', u'I', u'got', u'to', u'the', u'top', u'of', u'a', u'long', u'flight', u'of', u'stairs', u'inside', u'the', u'house', u'and', u'I', u'was', u'looking', u'around', u'and', u'I', u'said', u'to', u'her', u'something', u'like', u'I', u'forgot', u'to', u'close', u',', u'to', u'lock', u'the', u'front', u'door', u'of', u'my', u'house', u'and', u'that', u'I', u'had', u'better', u'go', u'.', u'And', u'I', u'had', u'forgotten', u'to', u'lock', u'the', u'door', u'of', u'my', u'house', u',', u'but', u'mostly', u'I', u'just', u'wanted', u'to', u'go', u'home', u',', u'so', u'we', u'went', u'back', u'down', u'this', u'really', u'long', u'flight', u'of', u'stairs', u',', u'and', u'I', u'do', u\"n't\", u'remember', u'if', u'I', u'had', u'my', u'brace', u'on', u',', u'that', u'I', u'wear', u',', u'or', u'not', u',', u'just', u'for', u'some', u'reason', u'I', u'was', u'focusing', u'on', u'the', u'stairs', u'a', u'lot', u'.', u'She', u'does', u'in', u'fact', u'have', u'a', u'long', u'flight', u'of', u'stairs', u'leading', u'into', u'her', u'house', u',', u'but', u'they', u'are', u'outside', u'not', u'inside', u'.']\n",
      "noun only: [u'parent', u'house', u'friend', u'house', u'detail', u'friend', u'house', u'place', u'lot', u'stairs', u'lot', u'details', u'lot', u'colors', u'rooms', u'colors', u'carpeting', u'furniture', u'shapes', u'lighting', u'room', u'room', u'top', u'flight', u'stairs', u'house', u'something', u'door', u'house', u'door', u'house', u'home', u'flight', u'stairs', u'brace', u'reason', u'stairs', u'lot', u'fact', u'flight', u'stairs', u'house']\n",
      "lemmatized: [u'parent', u'house', u'friend', u'house', u'detail', u'friend', u'house', u'place', u'lot', u'stair', u'lot', u'detail', u'lot', u'color', u'room', u'color', u'carpeting', u'furniture', u'shape', u'lighting', u'room', u'room', u'top', u'flight', u'stair', u'house', u'something', u'door', u'house', u'door', u'house', u'home', u'flight', u'stair', u'brace', u'reason', u'stair', u'lot', u'fact', u'flight', u'stair', u'house']\n",
      "Counter({u'house': 7, u'stair': 5, u'lot': 4, u'flight': 3, u'room': 3, u'door': 2, u'color': 2, u'detail': 2, u'friend': 2, u'parent': 1, u'top': 1, u'fact': 1, u'reason': 1, u'shape': 1, u'place': 1, u'something': 1, u'carpeting': 1, u'home': 1, u'lighting': 1, u'furniture': 1, u'brace': 1})\n",
      "\n",
      "\n",
      "Blind dreamers (M) (143 dreams) [M]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'Dreamed', u'I', u'was', u'in', u'my', u'computer', u'room', u'in', u'the', u'basement', u',', u'trying', u'to', u'write', u'a', u'letter', u',', u'to', u'my', u'friend', u'H', u',', u'trying', u'to', u'decide', u'whether', u'to', u'put', u'it', u'in', u'Braille', u'or', u'on', u'tape', u'.', u'Then', u'was', u'sitting', u'in', u'front', u'of', u'the', u'window', u',', u'opening', u'the', u'curtains', u'to', u'feel', u'how', u'warm', u'the', u'sun', u'was', u'then', u'turned', u'back', u'to', u'the', u'computer', u'and', u'could', u'hear', u'opera', u'music', u'from', u'a', u'radio', u'station', u'coming', u'through', u'the', u'computer', u'speaker', u'.']\n",
      "noun only: [u'Dreamed', u'computer', u'room', u'basement', u'letter', u'friend', u'H', u'Braille', u'tape', u'front', u'window', u'curtains', u'sun', u'computer', u'music', u'radio', u'station', u'computer', u'speaker']\n",
      "lemmatized: [u'Dreamed', u'computer', u'room', u'basement', u'letter', u'friend', u'H', u'Braille', u'tape', u'front', u'window', u'curtain', u'sun', u'computer', u'music', u'radio', u'station', u'computer', u'speaker']\n",
      "Counter({u'computer': 3, u'station': 1, u'room': 1, u'music': 1, u'Braille': 1, u'Dreamed': 1, u'H': 1, u'speaker': 1, u'radio': 1, u'window': 1, u'tape': 1, u'letter': 1, u'basement': 1, u'front': 1, u'curtain': 1, u'sun': 1, u'friend': 1})\n",
      "\n",
      "\n",
      "Robert Bosnak: A dream analyst (53 dreams) [M]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'At', u'party', u'.', u'A', u'woman', u'friend', u'sits', u'at', u'long', u'table', u'.', u'My', u'wife', u'is', u'there', u'.', u'She', u'looks', u'gray', u'and', u'old', u'.', u'My', u'friend', u'tells', u'me', u'that', u'she', u'has', u\"n't\", u'told', u'me', u'the', u'whole', u'story', u'.', u'She', u'has', u'already', u'filed', u'papers', u'for', u'divorce', u'.', u'Then', u'she', u'sits', u'at', u'other', u'table', u'.', u'I', u'get', u'up', u'alone', u'and', u'go', u'home', u'and', u'watch', u'a', u'silly', u'erotic', u'flick', u'.', u'The', u'kids', u'are', u'home', u'and', u'will', u'watch', u'a', u'film', u'in', u'living', u'room', u'that', u'a', u'young', u'girl', u'has', u'brought', u'.', u'I', u'am', u'bored', u'.']\n",
      "noun only: [u'party', u'woman', u'friend', u'sits', u'table', u'wife', u'friend', u'story', u'papers', u'divorce', u'table', u'home', u'flick', u'kids', u'home', u'film', u'living', u'room', u'girl']\n",
      "lemmatized: [u'party', u'woman', u'friend', u'sits', u'table', u'wife', u'friend', u'story', u'paper', u'divorce', u'table', u'home', u'flick', u'kid', u'home', u'film', u'living', u'room', u'girl']\n",
      "Counter({u'home': 2, u'table': 2, u'friend': 2, u'flick': 1, u'living': 1, u'story': 1, u'woman': 1, u'room': 1, u'wife': 1, u'divorce': 1, u'paper': 1, u'film': 1, u'party': 1, u'girl': 1, u'sits': 1, u'kid': 1})\n",
      "\n",
      "\n",
      "Chris: a transvestite (100 dreams) [M]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'arrive', u'home', u'and', u'note', u'that', u'I', u'am', u'driving', u'a', u'jeep', u',', u'but', u'it', u'is', u'the', u'wrong', u'jeep', u'and', u'so', u'I', u'drive', u'back', u'to', u'a', u'church', u'and', u'there', u'a', u'priest', u'is', u'standing', u'next', u'to', u'my', u'own', u'jeep', u'.', u'(', u'Priest', u'a', u'stranger', u')', u'.', u'He', u'says', u'that', u'he', u'knew', u'I', u'had', u'the', u'wrong', u'one', u',', u'but', u'that', u'the', u'owner', u'of', u'my', u'jeep', u'had', u'already', u'departed', u'.', u'He', u'said', u'that', u'I', u'was', u'to', u'bring', u'both', u'jeeps', u'with', u'me', u'.', u'I', u'look', u'for', u'warm', u'clothing', u'since', u'the', u'jeep', u'is', u'open', u'and', u'it', u'is', u'now', u'nighttime', u'.', u'I', u'look', u'in', u'a', u'garage', u'and', u'also', u'see', u'the', u'priest', u'there', u'fastening', u'rear', u'and', u'front', u'bumpers', u'of', u'the', u'two', u'jeeps', u'together', u',', u'and', u'I', u'wonder', u'how', u'this', u'is', u'going', u'to', u'work', u'.']\n",
      "noun only: [u'home', u'note', u'jeep', u'jeep', u'church', u'priest', u'jeep', u'stranger', u'owner', u'jeep', u'jeeps', u'clothing', u'jeep', u'garage', u'rear', u'bumpers', u'jeeps']\n",
      "lemmatized: [u'home', u'note', u'jeep', u'jeep', u'church', u'priest', u'jeep', u'stranger', u'owner', u'jeep', u'jeep', u'clothing', u'jeep', u'garage', u'rear', u'bumper', u'jeep']\n",
      "Counter({u'jeep': 7, u'stranger': 1, u'bumper': 1, u'note': 1, u'garage': 1, u'priest': 1, u'church': 1, u'owner': 1, u'home': 1, u'clothing': 1, u'rear': 1})\n",
      "\n",
      "\n",
      "Chuck: a physical scientist (75 dreams) [M]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'There', u'are', u'four', u'of', u'us', u'at', u'the', u'beginning', u'and', u'we', u'are', u'among', u'Those', u'on', u'Earth', u'.', u'A', u'spirit', u',', u'a', u'powerful', u'angel', u',', u'comes', u'for', u'us', u'and', u'we', u'follow', u'her', u'through', u'the', u'air', u'to', u'a', u'room', u'--', u'is', u'it', u'in', u'the', u'clouds', u'?', u'The', u'room', u'is', u'long', u'and', u'narrow', u',', u'and', u'we', u'sit', u'at', u'chairs', u'on', u'the', u'same', u'side', u'of', u'a', u'long', u',', u'narrow', u'table', u',', u'looking', u'blankly', u'at', u'the', u'Earth', u'somewhere', u'below', u'us', u'.', u'My', u'chair', u'is', u'the', u'left-most', u'one', u',', u'and', u'I', u'have', u'a', u'vague', u'feeling', u'of', u'loss', u',', u'a', u'vague', u'sense', u'that', u'the', u'right-most', u'seat', u'of', u'the', u'four', u'is', u'empty', u'.', u'The', u'angel', u'announces', u',', u'``', u'One', u'of', u'us', u'did', u'not', u'make', u'it', u',', u'and', u'we', u'know', u'what', u'that', u'means', u'.', u\"''\", u'I', u'do', u'not', u'completely', u'understand', u'the', u'remark', u'.', u'Something', u'about', u'the', u'angel', u'reminds', u'me', u'of', u'my', u'mother', u'.', u'The', u'angel', u'then', u'flies', u'down', u'to', u'sing', u'for', u'Those', u'on', u'Earth', u'.', u'The', u'scene', u'is', u'a', u'rock', u'concert', u',', u'where', u'the', u'angel', u'is', u'on', u'a', u'stage', u'with', u'microphones', u'.', u'Her', u'audience', u'on', u'Earth', u'is', u'very', u'large', u',', u'and', u'we', u'can', u'see', u'all', u'of', u'this', u'from', u'our', u'seats', u'in', u'the', u'long', u',', u'narrow', u'room', u'.', u'It', u'is', u'night', u'time', u'and', u'the', u'angel', u'sings', u'of', u'the', u'stars', u'.', u'I', u'look', u'at', u'the', u'sky', u'and', u'the', u'stars', u'appear', u'beautifully', u'to', u'me', u'.', u'I', u'want', u'to', u'see', u'the', u'stars', u'more', u'closely', u',', u'so', u'I', u'get', u'up', u'and', u'leave', u'the', u'long', u',', u'narrow', u'room', u',', u'a', u'move', u'I', u'am', u'not', u'sure', u'I', u'should', u'be', u'making', u'.', u'Curiosity', u'is', u'stronger', u'than', u'doubt', u'and', u'I', u'do', u'go', u'back', u'to', u'the', u'Earth', u'.', u'When', u'I', u'arrive', u'I', u'find', u'I', u'am', u'a', u'butterfly', u'skipping', u'across', u'a', u'large', u'meadow', u'near', u'a', u'mountain', u'.', u'One', u'of', u'Those', u'on', u'Earth', u'sees', u'me', u'and', u'touches', u'me', u'.', u'It', u'is', u'very', u'painful', u':', u'my', u'body', u',', u'my', u'wings', u',', u'all', u'feel', u'as', u'if', u'on', u'fire', u'!', u'Even', u'the', u'very', u'sky', u'turns', u'red', u'as', u'I', u'am', u'touched', u',', u'and', u'I', u'yell', u'out', u'a', u'very', u'loud', u',', u'``', u'Ow', u'!', u\"''\", u'The', u'person', u'touching', u'me', u'can', u'not', u'hear', u'the', u'yell', u',', u'and', u'touches', u'me', u'again', u',', u'apparently', u'trying', u'to', u'catch', u'me', u'as', u'I', u'fly', u'away', u'in', u'hurt', u'.', u'The', u'second', u'touch', u'is', u'just', u'as', u'painful', u'.', u'Altogether', u'I', u'am', u'touched', u'two', u'or', u'three', u'times', u'before', u'escaping', u'.', u'Now', u'I', u'am', u'hiding', u'in', u'the', u'mountains', u',', u'halfway', u'between', u'Earth', u'and', u'the', u'long', u',', u'narrow', u'room', u'in', u'the', u'sky', u'.', u'I', u'can', u'not', u'go', u'back', u'to', u'either', u',', u'and', u'I', u'am', u'now', u'alone', u'.', u'And', u'suddenly', u'I', u'have', u'a', u'great', u'insight', u':', u'the', u'other', u'from', u'my', u'original', u'group', u'on', u'Earth', u'(', u'the', u'one', u'who', u'did', u'not', u'make', u'it', u'to', u'the', u'right-most', u'seat', u'at', u'the', u'long', u',', u'narrow', u'table', u')', u'is', u'a', u'butterfly', u'just', u'like', u'me', u'!', u'The', u'butterflies', u'are', u'the', u'ones', u'who', u'leave', u'boredom', u'looking', u'for', u'beauty', u'.', u'The', u'long', u',', u'narrow', u'room', u'is', u'a', u'place', u'we', u'must', u'go', u'to', u'when', u'its', u'time', u'has', u'come', u',', u'but', u'it', u'is', u'also', u'a', u'place', u'we', u'leave', u'when', u'we', u'are', u'ready', u'.']\n",
      "noun only: [u'beginning', u'Earth', u'A', u'spirit', u'angel', u'air', u'room', u'clouds', u'room', u'chairs', u'side', u'table', u'blankly', u'Earth', u'chair', u'one', u'feeling', u'loss', u'sense', u'seat', u'angel', u'announces', u'remark', u'angel', u'mother', u'angel', u'Earth', u'scene', u'rock', u'concert', u'angel', u'stage', u'microphones', u'audience', u'Earth', u'seats', u'room', u'night', u'time', u'sings', u'stars', u'sky', u'stars', u'stars', u'room', u'move', u'Curiosity', u'doubt', u'Earth', u'butterfly', u'meadow', u'mountain', u'Earth', u'sees', u'body', u'wings', u'fire', u'turns', u'person', u'yell', u'touches', u'hurt', u'touch', u'times', u'mountains', u'halfway', u'Earth', u'room', u'sky', u'insight', u'group', u'Earth', u'one', u'seat', u'table', u'butterfly', u'butterflies', u'ones', u'boredom', u'beauty', u'room', u'place', u'time', u'place']\n",
      "lemmatized: [u'beginning', u'Earth', u'A', u'spirit', u'angel', u'air', u'room', u'cloud', u'room', u'chair', u'side', u'table', u'blankly', u'Earth', u'chair', u'one', u'feeling', u'loss', u'sense', u'seat', u'angel', u'announces', u'remark', u'angel', u'mother', u'angel', u'Earth', u'scene', u'rock', u'concert', u'angel', u'stage', u'microphone', u'audience', u'Earth', u'seat', u'room', u'night', u'time', u'sings', u'star', u'sky', u'star', u'star', u'room', u'move', u'Curiosity', u'doubt', u'Earth', u'butterfly', u'meadow', u'mountain', u'Earth', u'see', u'body', u'wing', u'fire', u'turn', u'person', u'yell', u'touch', u'hurt', u'touch', u'time', u'mountain', u'halfway', u'Earth', u'room', u'sky', u'insight', u'group', u'Earth', u'one', u'seat', u'table', u'butterfly', u'butterfly', u'one', u'boredom', u'beauty', u'room', u'place', u'time', u'place']\n",
      "Counter({u'Earth': 8, u'room': 6, u'angel': 5, u'butterfly': 3, u'seat': 3, u'time': 3, u'star': 3, u'one': 3, u'touch': 2, u'table': 2, u'chair': 2, u'sky': 2, u'mountain': 2, u'place': 2, u'concert': 1, u'move': 1, u'scene': 1, u'see': 1, u'sense': 1, u'cloud': 1, u'meadow': 1, u'sings': 1, u'microphone': 1, u'group': 1, u'person': 1, u'announces': 1, u'night': 1, u'doubt': 1, u'Curiosity': 1, u'wing': 1, u'A': 1, u'insight': 1, u'beauty': 1, u'fire': 1, u'rock': 1, u'body': 1, u'hurt': 1, u'halfway': 1, u'boredom': 1, u'beginning': 1, u'spirit': 1, u'stage': 1, u'loss': 1, u'remark': 1, u'air': 1, u'blankly': 1, u'turn': 1, u'audience': 1, u'mother': 1, u'yell': 1, u'feeling': 1, u'side': 1})\n",
      "\n",
      "\n",
      "Dahlia: concerns with appearance (24 dreams) [F]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'was', u'driving', u'to', u'Jack', u\"'s\", u'and', u'it', u'was', u'at', u'night', u'.', u'When', u'I', u'pulled', u'up', u',', u'Bonnie', u\"'s\", u'car', u'was', u'parked', u'in', u'front', u'.', u'There', u'was', u'an', u'electrical', u'cord', u'leading', u'from', u'her', u'car', u'to', u'the', u'house', u'.', u'I', u'think', u'I', u'thought', u'she', u'was', u'charging', u'her', u'battery', u'.', u'The', u'house', u'I', u'was', u'at', u'was', u'Jack', u\"'s\", u',', u'but', u'it', u'was', u'laid', u'out', u'differently', u'.', u'When', u'I', u'walked', u'in', u',', u'there', u'were', u'these', u'two', u'girls', u'in', u'the', u'front', u'room', u'talking', u'about', u'face', u'lotion', u'.', u'One', u'girl', u'was', u'really', u'fat', u'and', u'had', u'red', u'hair', u'and', u'I', u'am', u'not', u'sure', u'about', u'the', u'other', u'one', u'.', u'They', u'were', u'trying', u'to', u'decide', u'which', u'lotion', u'was', u'the', u'best', u'.', u'I', u'said', u'that', u'I', u'liked', u'Chanel', u'Base', u'Matte', u'.', u'One', u'of', u'the', u'girls', u'said', u'that', u'it', u'was', u'good', u',', u'but', u'she', u'did', u\"n't\", u'like', u'it', u'because', u'it', u'covered', u'up', u'too', u'much', u'.', u'Then', u'one', u'of', u'the', u'girls', u'was', u'my', u'friend', u'Rachel', u',', u'who', u'just', u'appeared', u'.', u'Then', u'on', u'TV', u'was', u'a', u'Chanel', u'infomercial', u'.', u'They', u'were', u'selling', u'a', u'kit', u'of', u'Chanel', u'makeup', u'.']\n",
      "noun only: [u'Jack', u'night', u'Bonnie', u'car', u'front', u'cord', u'car', u'house', u'battery', u'house', u'Jack', u'girls', u'front', u'room', u'face', u'lotion', u'girl', u'hair', u'one', u'lotion', u'Chanel', u'Base', u'Matte', u'girls', u'girls', u'friend', u'Rachel', u'TV', u'Chanel', u'infomercial', u'kit', u'Chanel', u'makeup']\n",
      "lemmatized: [u'Jack', u'night', u'Bonnie', u'car', u'front', u'cord', u'car', u'house', u'battery', u'house', u'Jack', u'girl', u'front', u'room', u'face', u'lotion', u'girl', u'hair', u'one', u'lotion', u'Chanel', u'Base', u'Matte', u'girl', u'girl', u'friend', u'Rachel', u'TV', u'Chanel', u'infomercial', u'kit', u'Chanel', u'makeup']\n",
      "Counter({u'girl': 4, u'Chanel': 3, u'house': 2, u'Jack': 2, u'front': 2, u'lotion': 2, u'car': 2, u'battery': 1, u'one': 1, u'hair': 1, u'TV': 1, u'Base': 1, u'friend': 1, u'cord': 1, u'infomercial': 1, u'makeup': 1, u'kit': 1, u'room': 1, u'Rachel': 1, u'Bonnie': 1, u'face': 1, u'Matte': 1, u'night': 1})\n",
      "\n",
      "\n",
      "David: teenage dreams (166 dreams) [M]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'was', u'in', u'the', u'Grammar', u'School', u'playground', u',', u'and', u'this', u'time', u'Angela', u'had', u'taken', u'off', u'to', u'somewhere', u'in', u'the', u'school', u'.', u'So', u'I', u'ran', u'over', u'to', u'Julie', u'and', u'asked', u'her', u'if', u'she', u'knew', u'where', u'Angela', u'was', u',', u'and', u'she', u'said', u':', u'``', u'I', u'ca', u\"n't\", u'tell', u'you', u'that', u'.', u\"''\", u'At', u'first', u'I', u'thought', u'she', u'was', u'just', u'being', u'a', u'bitch', u'about', u'it', u'.', u'But', u'then', u'I', u'asked', u'her', u'why', u'.', u'Then', u'she', u'sort', u'of', u'laughed', u'and', u'said', u':', u'``', u'I', u'have', u\"n't\", u'seen', u'her', u'all', u'day', u'.', u'Why', u'do', u\"n't\", u'you', u'talk', u'to', u'her', u'brother', u'over', u'there', u'?', u\"''\", u'So', u'then', u'I', u'walked', u'over', u'to', u'the', u'bus', u'stop', u'.', u'I', u'was', u'greeted', u'by', u'a', u'7-foot-tall', u'giant', u'of', u'a', u'dork', u'who', u'said', u':', u'``', u'Who', u'the', u'hell', u'are', u'you', u'?', u\"''\", u'I', u'said', u':', u'``', u'I', u\"'m\", u'David', u',', u'have', u'you', u'seen', u'Angela', u'?', u'He', u'said', u',', u'``', u'Yeah', u',', u'she', u\"'s\", u'in', u'the', u'school', u'building', u'.', u\"''\", u'I', u'said', u',', u'``', u'Great', u',', u\"''\", u'and', u'started', u'walking', u'to', u'the', u'school', u'.', u'He', u'stopped', u'me', u',', u'gave', u'me', u'a', u'giant', u'slab', u'of', u'masking', u'tape', u'rolled', u'up', u'to', u'look', u'like', u'a', u'joint', u'the', u'size', u'of', u'a', u'cigar', u'.', u'He', u'said', u',', u'``', u'first', u'let', u\"'s\", u'play', u'some', u'games', u'.', u\"''\", u'We', u'both', u'walked', u'out', u'into', u'the', u'street', u'to', u'a', u'cone', u',', u'then', u'stood', u'there', u'until', u'a', u'truck', u'came', u'by', u'.', u'Then', u'we', u\"'d\", u'wait', u'until', u'the', u'truck', u'was', u'a', u'little', u'bit', u'away', u',', u'then', u'we', u\"'d\", u'run', u'into', u'the', u'bushes', u'pretending', u'to', u'swear', u'in', u'Spanish', u'.', u'We', u'did', u'this', u'five', u'or', u'six', u'times', u',', u'then', u'I', u'woke', u'up', u'.']\n",
      "noun only: [u'Grammar', u'School', u'playground', u'time', u'Angela', u'school', u'Julie', u'Angela', u'bitch', u'sort', u'laughed', u'day', u'brother', u'bus', u'stop', u'giant', u'dork', u'hell', u'David', u'Angela', u'school', u'building', u'Great', u'school', u'slab', u'tape', u'size', u'cigar', u'let', u'games', u'street', u'cone', u'truck', u'truck', u'bit', u'bushes', u'times']\n",
      "lemmatized: [u'Grammar', u'School', u'playground', u'time', u'Angela', u'school', u'Julie', u'Angela', u'bitch', u'sort', u'laughed', u'day', u'brother', u'bus', u'stop', u'giant', u'dork', u'hell', u'David', u'Angela', u'school', u'building', u'Great', u'school', u'slab', u'tape', u'size', u'cigar', u'let', u'game', u'street', u'cone', u'truck', u'truck', u'bit', u'bush', u'time']\n",
      "Counter({u'Angela': 3, u'school': 3, u'truck': 2, u'time': 2, u'Great': 1, u'street': 1, u'bitch': 1, u'hell': 1, u'slab': 1, u'size': 1, u'giant': 1, u'bush': 1, u'tape': 1, u'playground': 1, u'laughed': 1, u'sort': 1, u'School': 1, u'Grammar': 1, u'bus': 1, u'cigar': 1, u'stop': 1, u'David': 1, u'game': 1, u'let': 1, u'bit': 1, u'day': 1, u'dork': 1, u'building': 1, u'Julie': 1, u'brother': 1, u'cone': 1})\n",
      "\n",
      "\n",
      "Dorothea: 53 years of dreams (900 dreams) [F]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'remember', u'mainly', u'getting', u'out', u'of', u'bed', u'to', u'go', u'to', u'my', u'mother', u\"'s\", u'room', u'.', u'The', u'first', u'time', u'I', u'was', u'distracted', u'by', u'a', u'meteorite', u'seen', u'through', u'the', u'window', u'I', u'passed', u',', u'and', u'so', u'I', u'returned', u'to', u'bed', u'.']\n",
      "noun only: [u'bed', u'mother', u'room', u'time', u'meteorite', u'window', u'bed']\n",
      "lemmatized: [u'bed', u'mother', u'room', u'time', u'meteorite', u'window', u'bed']\n",
      "Counter({u'bed': 2, u'room': 1, u'mother': 1, u'meteorite': 1, u'window': 1, u'time': 1})\n",
      "\n",
      "\n",
      "Ed: dreams of his late wife (143 dreams) [M]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'see', u'Mary', u'in', u'profile', u'.', u'She', u'looks', u'as', u'lovely', u'as', u'she', u'had', u'before', u'the', u'illness', u',', u'and', u'when', u'she', u'was', u'younger', u'.', u'At', u'one', u'point', u'in', u'the', u'dream', u'I', u'also', u'see', u'a', u'profile', u'view', u'of', u'her', u'when', u'she', u'and', u'Maria', u'press', u'their', u'cheeks', u'together', u'.', u'Mary', u'does', u\"n't\", u'look', u'at', u'me', u',', u'nor', u'am', u'I', u'aware', u'that', u'she', u'spoke', u'.', u'It', u'is', u'like', u'a', u'TV', u'scene', u'where', u'someone', u'is', u'not', u'speaking', u'yet', u'you', u'hear', u'that', u'person', u\"'s\", u'voice', u'in', u'the', u'background', u'.', u'I', u'hear', u'Mary', u'say', u',', u'``', u'I', u'want', u'you', u'to', u'be', u'happy', u',', u'Ed', u'.', u\"''\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun only: [u'profile', u'illness', u'point', u'dream', u'view', u'Maria', u'cheeks', u'Mary', u'TV', u'scene', u'someone', u'person', u'voice', u'background', u'Ed']\n",
      "lemmatized: [u'profile', u'illness', u'point', u'dream', u'view', u'Maria', u'cheek', u'Mary', u'TV', u'scene', u'someone', u'person', u'voice', u'background', u'Ed']\n",
      "Counter({u'profile': 1, u'illness': 1, u'point': 1, u'TV': 1, u'Ed': 1, u'cheek': 1, u'scene': 1, u'voice': 1, u'person': 1, u'someone': 1, u'background': 1, u'view': 1, u'dream': 1, u'Mary': 1, u'Maria': 1})\n",
      "\n",
      "\n",
      "Edna: a blind woman (19 dreams) [F]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'was', u'out', u'eating', u'and', u'it', u'was', u'a', u'very', u'nice', u'place', u'.', u'There', u'were', u'five', u'of', u'us', u',', u'Mother', u'and', u'Father', u'and', u'two', u'other', u'people', u',', u'a', u'man', u'and', u'a', u'woman', u'.', u'Father', u'was', u'on', u'my', u'right', u',', u'around', u'the', u'corner', u'from', u'me', u',', u'and', u'to', u'my', u'left', u'was', u'this', u'other', u'middle-aged', u'man', u',', u'and', u'next', u'to', u'him', u'was', u'my', u'mother', u',', u'and', u'next', u'to', u'her', u'the', u'other', u'middle-aged', u'woman', u'.', u'I', u'was', u'talking', u'to', u'this', u'man', u'on', u'my', u'left', u',', u'but', u'I', u'overheard', u'my', u'father', u'saying', u'in', u'a', u'low', u'voice', u'to', u'this', u'woman', u'next', u'to', u'him', u'that', u'he', u'was', u'very', u'embarrassed', u'about', u'his', u'daughter', u'because', u'of', u'her', u'poor', u'eating', u'manners', u'.', u'I', u'woke', u'up', u'crying', u'bitterly', u'.']\n",
      "noun only: [u'eating', u'place', u'Mother', u'Father', u'people', u'man', u'woman', u'Father', u'right', u'corner', u'left', u'man', u'mother', u'woman', u'man', u'left', u'father', u'voice', u'woman', u'daughter', u'manners']\n",
      "lemmatized: [u'eating', u'place', u'Mother', u'Father', u'people', u'man', u'woman', u'Father', u'right', u'corner', u'left', u'man', u'mother', u'woman', u'man', u'left', u'father', u'voice', u'woman', u'daughter', u'manner']\n",
      "Counter({u'woman': 3, u'man': 3, u'Father': 2, u'left': 2, u'eating': 1, u'daughter': 1, u'people': 1, u'father': 1, u'mother': 1, u'voice': 1, u'right': 1, u'place': 1, u'manner': 1, u'Mother': 1, u'corner': 1})\n",
      "\n",
      "\n",
      "Elizabeth: a woman in her 40s (1707 dreams) [F]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'After', u'he', u'left', u',', u'another', u'young', u'man', u'came', u'out', u'from', u'behind', u'the', u'reception', u'desk', u'.', u'I', u'told', u'him', u'I', u'was', u'waiting', u'for', u'Debbie', u'Phillips', u'[', u'unknown', u'in', u'waking', u'life', u']', u'.', u'He', u'said', u',', u'oh', u'yeah', u'you', u'have', u'some', u'forms', u'to', u'fill', u'out', u'.', u'At', u'that', u'point', u'I', u'felt', u'like', u'I', u'was', u'a', u'new', u'employee', u'.', u'I', u'do', u\"n't\", u'know', u'why', u'I', u'thought', u'that', u'.', u'He', u'walked', u'away', u'and', u'I', u'was', u'just', u'kind', u'of', u'wandering', u'around', u'the', u'front', u'office', u'area', u'there', u'.', u'There', u'was', u'an', u'antique', u'sewing', u'machine', u'there', u'.', u'Very', u'similar', u'to', u'mine', u'only', u'I', u'remember', u'thinking', u'how', u'large', u'it', u'was', u'and', u'how', u'I', u'guess', u'this', u'interested', u'in', u'it', u'.', u'I', u'remember', u'thinking', u'how', u'much', u'I', u'like', u'antiques', u'.', u'There', u'was', u'no', u'name', u'on', u'it', u'.', u'It', u'did', u\"n't\", u'have', u'the', u'make', u'of', u'the', u'sewing', u'machine', u'like', u'mine', u'has', u'on', u'it', u'.', u'I', u'do', u\"n't\", u'remember', u'seeing', u'the', u'pedal', u'at', u'the', u'floor', u',', u'but', u'I', u'walked', u'around', u'the', u'front', u'and', u'pulled', u'some', u'of', u'the', u'drawers', u'open', u'.', u'There', u'were', u'papers', u'and', u'stuff', u'in', u'there', u'.', u'At', u'that', u'point', u',', u'the', u'second', u'young', u'man', u'came', u'back', u'out', u'.', u'I', u'said', u',', u'sorry', u',', u'I', u\"'m\", u'just', u'really', u'interested', u'in', u'antiques', u'.', u'I', u'have', u'an', u'antique', u'sewing', u'machine', u',', u'but', u'this', u'one', u'is', u'larger', u'and', u'I', u'was', u'somewhat', u'intrigued', u'by', u'it', u'.', u'He', u'said', u',', u'oh', u',', u'that', u\"'s\", u'okay', u'.']\n",
      "noun only: [u'man', u'reception', u'desk', u'Debbie', u'Phillips', u'[', u'life', u']', u'yeah', u'forms', u'point', u'employee', u'kind', u'office', u'area', u'sewing', u'machine', u'antiques', u'name', u'make', u'machine', u'mine', u'pedal', u'floor', u'front', u'drawers', u'papers', u'stuff', u'point', u'man', u'sorry', u'antiques', u'sewing', u'machine', u'okay']\n",
      "lemmatized: [u'man', u'reception', u'desk', u'Debbie', u'Phillips', u'[', u'life', u']', u'yeah', u'form', u'point', u'employee', u'kind', u'office', u'area', u'sewing', u'machine', u'antique', u'name', u'make', u'machine', u'mine', u'pedal', u'floor', u'front', u'drawer', u'paper', u'stuff', u'point', u'man', u'sorry', u'antique', u'sewing', u'machine', u'okay']\n",
      "Counter({u'machine': 3, u'antique': 2, u'point': 2, u'sewing': 2, u'man': 2, u'office': 1, u'mine': 1, u'Phillips': 1, u'paper': 1, u'Debbie': 1, u'area': 1, u'make': 1, u'employee': 1, u'sorry': 1, u'life': 1, u'okay': 1, u'form': 1, u'yeah': 1, u'desk': 1, u'front': 1, u'pedal': 1, u'[': 1, u']': 1, u'kind': 1, u'name': 1, u'drawer': 1, u'floor': 1, u'stuff': 1, u'reception': 1})\n",
      "\n",
      "\n",
      "Emma: 48 years of dreams (1521 dreams) [F]\n",
      "  noun-only sample dream: \n",
      "\n",
      "\n",
      "Emma's Husband (72 dreams) [M]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'go', u'into', u'small', u'``', u'shotgun', u'house', u\"''\", u'a', u'sharecropper', u'house', u'--', u'Three', u'rooms', u'in', u'straight', u'row', u'.', u'A', u'black', u'woman', u'is', u'sexy', u'with', u'print', u'dress', u'and', u'no', u'underwear', u',', u'and', u'big', u'breasts', u'show', u'thru', u'strained', u'buttons', u'in', u'front', u'.', u'I', u'want', u'to', u'touch', u'them', u',', u'and', u'suddenly', u'I', u'see', u'her', u'baby', u'who', u'is', u'sucking', u'his', u'thumb', u'and', u'first', u'he', u'gets', u'a', u'little', u'erection', u'and', u'then', u'little', u'erections', u'come', u'out', u'all', u'over', u'his', u'body', u'.', u'Amazing', u'.', u'As', u'I', u'look', u'at', u'the', u'dozens', u'of', u'little', u'penises', u',', u'her', u'husband', u'comes', u'in', u'with', u'a', u'shotgun', u'from', u'hunting', u'.', u'I', u'say', u'hello', u'and', u'leave', u'.']\n",
      "noun only: [u'house', u'house', u'rooms', u'row', u'woman', u'print', u'dress', u'breasts', u'buttons', u'front', u'baby', u'thumb', u'erection', u'erections', u'body', u'Amazing', u'dozens', u'penises', u'husband', u'shotgun', u'hello']\n",
      "lemmatized: [u'house', u'house', u'room', u'row', u'woman', u'print', u'dress', u'breast', u'button', u'front', u'baby', u'thumb', u'erection', u'erection', u'body', u'Amazing', u'dozen', u'penis', u'husband', u'shotgun', u'hello']\n",
      "Counter({u'house': 2, u'erection': 2, u'baby': 1, u'body': 1, u'woman': 1, u'room': 1, u'button': 1, u'penis': 1, u'thumb': 1, u'front': 1, u'Amazing': 1, u'breast': 1, u'shotgun': 1, u'print': 1, u'dozen': 1, u'dress': 1, u'hello': 1, u'husband': 1, u'row': 1})\n",
      "\n",
      "\n",
      "Esther: an adolescent girl (110 dreams) [F]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'am', u'a', u'little', u'blond', u'girl', u'on', u'an', u'escalator', u'.', u'I', u'am', u'holding', u'hands', u'with', u'two', u'younger', u'blond', u'boys', u'.', u'We', u'are', u'all', u'dressed', u'in', u'white', u'.', u'Everything', u'is', u'glowing', u'...', u'there', u\"'s\", u'lots', u'of', u'light', u'.', u'Almost', u'blinding', u',', u'but', u'very', u'peaceful', u'.', u'The', u'escalator', u'is', u'going', u'up', u'towards', u'this', u'huge', u'mall', u'where', u'everything', u'was', u'free', u'.']\n",
      "noun only: [u'blond', u'girl', u'escalator', u'hands', u'blond', u'boys', u'Everything', u'lots', u'light', u'Almost', u'blinding', u'escalator', u'towards', u'mall', u'everything']\n",
      "lemmatized: [u'blond', u'girl', u'escalator', u'hand', u'blond', u'boy', u'Everything', u'lot', u'light', u'Almost', u'blinding', u'escalator', u'towards', u'mall', u'everything']\n",
      "Counter({u'escalator': 2, u'blond': 2, u'boy': 1, u'towards': 1, u'everything': 1, u'Almost': 1, u'light': 1, u'hand': 1, u'Everything': 1, u'mall': 1, u'lot': 1, u'girl': 1, u'blinding': 1})\n",
      "\n",
      "\n",
      "College women, late 1940s (681 dreams) [F]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'dreamed', u'that', u'I', u'was', u'at', u'a', u'public', u'affair', u'but', u'I', u'do', u\"n't\", u'know', u'which', u'affair', u'it', u'was', u'although', u'it', u'was', u'outdoors', u'.', u'There', u'were', u'many', u'people', u'around', u'us', u'and', u'they', u'were', u'of', u'all', u'ages', u'.', u'I', u'was', u'at', u'this', u'affair', u'with', u'B', u'.', u'He', u'is', u'about', u'twenty-six', u'years', u'old', u'and', u'he', u'is', u'the', u'boy-friend', u'of', u'one', u'of', u'the', u'girls', u'that', u'lives', u'in', u'the', u'dormitory', u'that', u'I', u'do', u'.', u'Whenever', u'I', u'felt', u'the', u'urge', u'to', u'get', u'away', u'from', u'my', u'escort', u'or', u'from', u'the', u'people', u'at', u'the', u'affair', u',', u'I', u'would', u'start', u'to', u'fly', u'.', u'(', u'like', u'superman', u')', u'.', u'While', u'up', u'in', u'the', u'air', u'I', u'felt', u'very', u'uneasy', u'and', u'worried', u'about', u'how', u'I', u'would', u'get', u'back', u'down', u'without', u'hurting', u'myself', u'.', u'I', u'left', u'my', u'escort', u'about', u'three', u'times', u'in', u'this', u'way', u'.', u'I', u'do', u'not', u'remember', u'why', u'I', u'felt', u'that', u'I', u'had', u'to', u'get', u'away', u'.', u'<', u'BR', u'>', u'<', u'BR', u'>', u'<', u'TABLE', u'border=', u\"''\", u'1', u\"''\", u'cellspacing=', u\"''\", u'0', u\"''\", u'cellpadding=', u\"''\", u'3', u\"''\", u'>', u'<', u'TR', u'valign=', u\"''\", u'top', u\"''\", u'>', u'<', u'TD', u'>', u'<', u'I', u'>', u'Interpretation', u'<', u'/I', u'>', u'<', u'BR', u'>', u'I', u'do', u'not', u'know', u'why', u'I', u'would', u'dream', u'of', u'B.', u'I', u'do', u'not', u'know', u'him', u'very', u'well', u'and', u'I', u'do', u'not', u'feel', u'very', u'friendly', u'toward', u'him', u'when', u'I', u'do', u'see', u'him', u'.', u'I', u'believe', u'that', u'I', u'associated', u'him', u'with', u'my', u'studies', u'and', u'felt', u'that', u'I', u'had', u'to', u'get', u'away', u'for', u'a', u'short', u'while', u'.', u'When', u'I', u'had', u'this', u'dream', u',', u'I', u'had', u\"n't\", u'been', u'home', u'for', u'about', u'eight', u'weeks', u'and', u'was', u'looking', u'forward', u'to', u'going', u'home', u'.', u'I', u'felt', u'that', u'I', u'wanted', u'a', u'short', u'vacation', u'from', u'my', u'studies', u'and', u'this', u'dream', u'was', u'an', u'escape', u'mechanism', u'in', u'the', u'form', u'of', u'a', u'fantasy', u'to', u'get', u'away', u'from', u'my', u'classes', u'for', u'a', u'short', u'while.', u'<', u'/TD', u'>', u'<', u'TD', u'>', u'<', u'I', u'>', u'Answers', u'to', u'questions', u'<', u'/I', u'>', u'<', u'BR', u'>', u'2', u'.', u'Frustrated', u'.', u'I', u'felt', u'that', u'I', u'had', u'to', u'get', u'away.', u'<', u'BR', u'>', u'3', u'.', u'actual', u'participant', u'<', u'BR', u'>', u'4', u'.', u'unpleasant', u'<', u'BR', u'>', u'5', u'.', u'Vague', u',', u'but', u'it', u'was', u'outdoors.', u'<', u'BR', u'>', u'6', u'.', u'No.', u'<', u'BR', u'>', u'7', u'.', u'No.', u'<', u'/TD', u'>', u'<', u'/TR', u'>', u'<', u'/TABLE', u'>']\n",
      "noun only: [u'affair', u'affair', u'outdoors', u'people', u'ages', u'affair', u'B', u'years', u'boy-friend', u'girls', u'dormitory', u'urge', u'escort', u'people', u'affair', u'superman', u'air', u'escort', u'times', u'way', u'BR', u'>', u'<', u'BR', u'>', u'<', u'TABLE', u'border=', u'cellspacing=', u'cellpadding=', u'>', u'<', u'TR', u'valign=', u'top', u'<', u'TD', u'>', u'<', u'Interpretation', u'<', u'/I', u'>', u'<', u'BR', u'>', u'B.', u'studies', u'while', u'dream', u'weeks', u'home', u'vacation', u'studies', u'dream', u'escape', u'mechanism', u'form', u'fantasy', u'classes', u'while.', u'<', u'/TD', u'>', u'<', u'TD', u'>', u'<', u'Answers', u'questions', u'<', u'/I', u'>', u'<', u'BR', u'<', u'BR', u'participant', u'<', u'BR', u'BR', u'>', u'Vague', u'<', u'BR', u'No.', u'BR', u'>', u'No.', u'/TD', u'>', u'<', u'/TR', u'>', u'<', u'/TABLE', u'>']\n",
      "lemmatized: [u'affair', u'affair', u'outdoors', u'people', u'age', u'affair', u'B', u'year', u'boy-friend', u'girl', u'dormitory', u'urge', u'escort', u'people', u'affair', u'superman', u'air', u'escort', u'time', u'way', u'BR', u'>', u'<', u'BR', u'>', u'<', u'TABLE', u'border=', u'cellspacing=', u'cellpadding=', u'>', u'<', u'TR', u'valign=', u'top', u'<', u'TD', u'>', u'<', u'Interpretation', u'<', u'/I', u'>', u'<', u'BR', u'>', u'B.', u'study', u'while', u'dream', u'week', u'home', u'vacation', u'study', u'dream', u'escape', u'mechanism', u'form', u'fantasy', u'class', u'while.', u'<', u'/TD', u'>', u'<', u'TD', u'>', u'<', u'Answers', u'question', u'<', u'/I', u'>', u'<', u'BR', u'<', u'BR', u'participant', u'<', u'BR', u'BR', u'>', u'Vague', u'<', u'BR', u'No.', u'BR', u'>', u'No.', u'/TD', u'>', u'<', u'/TR', u'>', u'<', u'/TABLE', u'>']\n",
      "Counter({u'<': 17, u'>': 14, u'BR': 9, u'affair': 4, u'/I': 2, u'people': 2, u'TD': 2, u'escort': 2, u'/TD': 2, u'No.': 2, u'study': 2, u'dream': 2, u'question': 1, u'escape': 1, u'while.': 1, u'fantasy': 1, u'Answers': 1, u'year': 1, u'home': 1, u'girl': 1, u'superman': 1, u'participant': 1, u'cellpadding=': 1, u'top': 1, u'TR': 1, u'vacation': 1, u'mechanism': 1, u'dormitory': 1, u'way': 1, u'/TABLE': 1, u'TABLE': 1, u'week': 1, u'urge': 1, u'B': 1, u'form': 1, u'time': 1, u'cellspacing=': 1, u'Vague': 1, u'boy-friend': 1, u'/TR': 1, u'class': 1, u'Interpretation': 1, u'valign=': 1, u'age': 1, u'air': 1, u'while': 1, u'border=': 1, u'outdoors': 1, u'B.': 1})\n",
      "\n",
      "\n",
      "Izzy (all) (4352 dreams) [F]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'was', u'out', u'the', u'front', u'of', u'the', u'house', u',', u'and', u'I', u'saw', u'a', u'creepy', u'guy', u'.', u'I', u'tried', u'to', u'scream', u'but', u'I', u'could', u\"n't\", u'.', u'I', u'ended', u'up', u'getting', u'inside', u',', u'and', u'Mom', u'put', u'me', u'over', u'her', u'shoulder', u'and', u'was', u'carrying', u'me', u'down', u'the', u'hallway', u'.', u'The', u'guy', u'came', u'inside', u'and', u'he', u'had', u'a', u'knife', u'.', u'I', u'was', u'holding', u'one', u'of', u'those', u'little', u'dog', u'toys', u'with', u'the', u'plastic', u'head', u'and', u'the', u'bean-y', u'body', u'.', u'He', u'was', u'right', u'behind', u'us', u',', u'and', u'I', u'could', u\"n't\", u'do', u'anything', u'.', u'Then', u'Mom', u'put', u'me', u'in', u'a', u'cupboard', u'.']\n",
      "noun only: [u'front', u'house', u'creepy', u'guy', u'Mom', u'shoulder', u'hallway', u'guy', u'knife', u'toys', u'head', u'body', u'anything', u'Mom', u'cupboard']\n",
      "lemmatized: [u'front', u'house', u'creepy', u'guy', u'Mom', u'shoulder', u'hallway', u'guy', u'knife', u'toy', u'head', u'body', u'anything', u'Mom', u'cupboard']\n",
      "Counter({u'Mom': 2, u'guy': 2, u'shoulder': 1, u'hallway': 1, u'head': 1, u'toy': 1, u'cupboard': 1, u'anything': 1, u'house': 1, u'body': 1, u'creepy': 1, u'front': 1, u'knife': 1})\n",
      "\n",
      "\n",
      "Jasmine (all) (664 dreams) [F]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'was', u'at', u'school', u'with', u'Mom', u',', u'and', u'her', u'and', u'I', u'were', u'up', u'at', u'this', u'little', u'mixing', u'room', u'type', u'of', u'a', u'thing', u',', u'up', u'above', u'on', u'the', u'third', u'floor', u'of', u'the', u'main', u'building', u'.', u'But', u'it', u'was', u'really', u'supposed', u'to', u'be', u'a', u'computer', u'lab', u'.', u'Anyway', u',', u'sorry', u'about', u'that', u'saw', u'in', u'the', u'background', u'.', u'Anyway', u',', u'Mom', u'and', u'I', u'were', u'in', u'this', u'room', u',', u'and', u'she', u'bent', u'something', u'like', u'a', u'computer', u'that', u'makes', u'it', u'really', u'stand', u'out', u',', u'and', u'she', u'has', u'to', u'get', u'help', u'from', u'Uncle', u'Larry', u'.', u'I', u'think', u'this', u'is', u'how', u'it', u'went', u'.', u'It', u'was', u'so', u'complicated', u',', u'I', u'ca', u\"n't\", u'even', u'remember', u'it', u'.', u'But', u'anyway', u',', u'she', u'makes', u'the', u'computer', u'stand', u'out', u',', u'and', u'Uncle', u'Larry', u'comes', u'up', u'and', u'is', u'criticizing', u'me', u'because', u'I', u'do', u\"n't\", u'ever', u'work', u'with', u'him', u'on', u'anything', u',', u'and', u'is', u\"n't\", u'going', u'to', u'help', u'us', u'now', u'.', u'Matter', u'of', u'fact', u',', u'he', u'threatens', u'to', u'activate', u'the', u'alarm', u'system', u'.', u'And', u'then', u'I', u'think', u'somebody', u'was', u'riding', u'around', u'outside', u'in', u'a', u'little', u'golf', u'cart', u'thing', u'like', u'the', u'administrators', u'do', u',', u'and', u'for', u'some', u'reason', u'or', u'other', u'we', u'had', u'to', u'get', u'out', u'of', u'there', u'quick', u'because', u'he', u'did', u'something', u'else', u'with', u'the', u'computer', u'that', u'was', u'going', u'to', u'start', u'this', u'virus', u'thing', u'going', u'around', u'the', u'school', u'.', u'And', u'so', u'we', u'got', u'on', u'this', u'golf', u'cart', u'and', u'we', u'just', u'drive', u',', u'and', u'I', u'did', u\"n't\", u'recognize', u',', u'and', u'we', u'went', u'downstairs', u'via', u'the', u'elevator', u'into', u'the', u'library', u'.', u'Then', u'we', u'were', u'talking', u'with', u'Mrs.', u'Talmadge', u'and', u'she', u'said', u'that', u'I', u'had', u'this', u'problem', u'with', u'rocking', u'back', u'and', u'forth', u',', u'which', u'I', u'do', u\"n't\", u',', u'but', u'that', u'the', u'problem', u'was', u'common', u'with', u'blind', u'people', u',', u'but', u'I', u'do', u\"n't\", u'have', u'it', u'.', u'And', u'Mrs.', u'Talmadge', u'said', u'that', u'I', u'had', u'a', u'problem', u'with', u'rocking', u'back', u'and', u'forth', u'and', u'that', u'we', u'needed', u'to', u'work', u'on', u'that', u'.']\n",
      "noun only: [u'school', u'Mom', u'room', u'type', u'thing', u'floor', u'building', u'computer', u'lab', u'Anyway', u'saw', u'background', u'Anyway', u'Mom', u'room', u'something', u'computer', u'help', u'Uncle', u'Larry', u'computer', u'Uncle', u'Larry', u'anything', u'Matter', u'fact', u'alarm', u'system', u'somebody', u'golf', u'thing', u'administrators', u'reason', u'something', u'computer', u'thing', u'school', u'golf', u'cart', u'drive', u'downstairs', u'elevator', u'library', u'Mrs.', u'Talmadge', u'problem', u'forth', u'problem', u'people', u'Mrs.', u'Talmadge', u'problem', u'forth']\n",
      "lemmatized: [u'school', u'Mom', u'room', u'type', u'thing', u'floor', u'building', u'computer', u'lab', u'Anyway', u'saw', u'background', u'Anyway', u'Mom', u'room', u'something', u'computer', u'help', u'Uncle', u'Larry', u'computer', u'Uncle', u'Larry', u'anything', u'Matter', u'fact', u'alarm', u'system', u'somebody', u'golf', u'thing', u'administrator', u'reason', u'something', u'computer', u'thing', u'school', u'golf', u'cart', u'drive', u'downstairs', u'elevator', u'library', u'Mrs.', u'Talmadge', u'problem', u'forth', u'problem', u'people', u'Mrs.', u'Talmadge', u'problem', u'forth']\n",
      "Counter({u'computer': 4, u'thing': 3, u'problem': 3, u'golf': 2, u'something': 2, u'Larry': 2, u'Uncle': 2, u'Mom': 2, u'school': 2, u'room': 2, u'forth': 2, u'Mrs.': 2, u'Anyway': 2, u'Talmadge': 2, u'help': 1, u'people': 1, u'library': 1, u'drive': 1, u'saw': 1, u'floor': 1, u'system': 1, u'elevator': 1, u'type': 1, u'administrator': 1, u'somebody': 1, u'lab': 1, u'Matter': 1, u'reason': 1, u'cart': 1, u'background': 1, u'downstairs': 1, u'building': 1, u'anything': 1, u'alarm': 1, u'fact': 1})\n",
      "\n",
      "\n",
      "Jeff: a lucid dreamer (87 dreams) [M]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'In', u'my', u'dream', u',', u'the', u'first', u'thing', u'I', u'remember', u'was', u'this', u'very', u'nerdy', u'guy', u'that', u'goes', u'to', u'my', u'school', u'(', u'in', u'my', u'opinion', u'he', u'was', u'the', u'biggest', u'nerd', u'at', u'my', u'school', u')', u'.', u'So', u',', u'then', u'I', u'saw', u'a', u'girl', u'talking', u'to', u'him', u',', u'and', u'all', u'of', u'a', u'sudden', u'she', u'kissed', u'him', u'.', u'Just', u'that', u'event', u'was', u'so', u'dramatic', u'to', u'me', u',', u'I', u'started', u'crying', u'selflessly', u'at', u'what', u'had', u'just', u'happened-', u'a', u'completely', u'beautiful', u'girl', u'kissed', u'this', u'complete', u'geek', u',', u'and', u'I', u'was', u'so', u'touched', u'by', u'that', u'moment', u',', u'that', u'I', u'think', u'I', u'fell', u'in', u'love', u'with', u'her', u'on', u'the', u'spot', u'and', u'realized', u'she', u'was', u'the', u'most', u'beautiful', u'girl', u'I', u\"'d\", u'ever', u'seen', u'.', u'Later', u'I', u'caught', u'up', u'with', u'her', u'at', u'a', u'cafe', u'of', u'some', u'sort', u',', u'and', u'sat', u'and', u'talked', u'to', u'her', u',', u'but', u'I', u'did', u\"n't\", u'want', u'to', u'tell', u'her', u'how', u'much', u'I', u'liked', u'her', u'because', u'she', u'might', u'have', u'thought', u'I', u'was', u'trying', u'to', u'take', u'advantage', u'of', u'her', u',', u'and', u'with', u'such', u'a', u'nice', u'girl', u',', u'I', u'would', u\"n't\", u'want', u'to', u'do', u'THAT', u'.']\n",
      "noun only: [u'dream', u'thing', u'guy', u'school', u'opinion', u'nerd', u'school', u'girl', u'Just', u'event', u'girl', u'geek', u'moment', u'love', u'spot', u'girl', u'cafe', u'sort', u'advantage', u'her', u'girl']\n",
      "lemmatized: [u'dream', u'thing', u'guy', u'school', u'opinion', u'nerd', u'school', u'girl', u'Just', u'event', u'girl', u'geek', u'moment', u'love', u'spot', u'girl', u'cafe', u'sort', u'advantage', u'her', u'girl']\n",
      "Counter({u'girl': 4, u'school': 2, u'sort': 1, u'love': 1, u'Just': 1, u'advantage': 1, u'spot': 1, u'event': 1, u'geek': 1, u'thing': 1, u'moment': 1, u'her': 1, u'opinion': 1, u'guy': 1, u'cafe': 1, u'dream': 1, u'nerd': 1})\n",
      "\n",
      "\n",
      "Joan: a lesbian (42 dreams) [F]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'was', u'at', u'a', u'campground', u'where', u'a', u'seminar', u'was', u'being', u'held', u'.', u'I', u'was', u'talking', u'with', u'Eliza', u'.', u'A', u'group', u'of', u'people', u'were', u'gathering', u'around', u'the', u'stairs', u'in', u'front', u'of', u'a', u'building', u'to', u'have', u'their', u'picture', u'taken', u'.', u'Eliza', u'said', u',', u'``', u'Hurry', u',', u'do', u\"n't\", u'you', u'want', u'to', u'be', u'in', u'the', u'picture', u'?', u\"''\", u'I', u'ran', u'towards', u'the', u'group', u'but', u'it', u'was', u'like', u'I', u'was', u'running', u'in', u'slow', u'motion', u'.', u'When', u'I', u'reached', u'the', u'group', u'everyone', u'was', u'leaving', u'.']\n",
      "noun only: [u'campground', u'seminar', u'Eliza', u'A', u'group', u'people', u'stairs', u'front', u'building', u'picture', u'Eliza', u'Hurry', u'picture', u'group', u'motion', u'group', u'everyone']\n",
      "lemmatized: [u'campground', u'seminar', u'Eliza', u'A', u'group', u'people', u'stair', u'front', u'building', u'picture', u'Eliza', u'Hurry', u'picture', u'group', u'motion', u'group', u'everyone']\n",
      "Counter({u'group': 3, u'picture': 2, u'Eliza': 2, u'A': 1, u'building': 1, u'everyone': 1, u'stair': 1, u'people': 1, u'motion': 1, u'front': 1, u'campground': 1, u'Hurry': 1, u'seminar': 1})\n",
      "\n",
      "\n",
      "Kenneth (2022 dreams) [M]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'<', u'I', u'>', u'Disease', u'Room', u'<', u'/I', u'>', u'<', u'BR', u'>', u'<', u'BR', u'>', u'I', u'am', u'with', u'my', u'friend', u'Kevin', u'in', u'a', u'small', u'room', u'with', u'a', u'long', u'rectangular', u'table', u'.', u'There', u'is', u'a', u'television', u'in', u'an', u'upper', u'corner', u'of', u'the', u'room', u'.', u'It', u'is', u'on', u'.', u'My', u'classmate', u'Ned', u'Stallone', u'is', u'a', u'security', u'guard', u'.', u'He', u'walks', u'by', u',', u'stops', u',', u'and', u'enters', u'.', u'He', u'has', u'a', u'mask', u'over', u'his', u'mouth', u'.', u'I', u'think', u'there', u'is', u'a', u'disease', u'somewhere', u'.', u'It', u'may', u'be', u'in', u'our', u'room', u',', u'or', u'somewhere', u'else', u'.', u'There', u'is', u'a', u'boy', u'sitting', u'at', u'a', u'table', u'with', u'us', u'.', u'He', u\"'s\", u'eating', u'a', u'huge', u'bowl', u'of', u'fruit', u'and', u'ice', u'cream', u',', u'which', u'came', u'in', u'rectangular', u'chunks', u'.', u'Ned', u'tells', u'us', u'to', u'meet', u'at', u'8:30', u'in', u'Major', u'Hall', u'Dormitory', u'.']\n",
      "noun only: [u'<', u'Disease', u'Room', u'<', u'/I', u'>', u'<', u'BR', u'>', u'<', u'BR', u'>', u'friend', u'Kevin', u'room', u'rectangular', u'table', u'television', u'corner', u'room', u'classmate', u'Ned', u'Stallone', u'security', u'guard', u'stops', u'enters', u'mask', u'mouth', u'disease', u'room', u'sitting', u'table', u'bowl', u'fruit', u'ice', u'cream', u'chunks', u'Ned', u'Major', u'Hall', u'Dormitory']\n",
      "lemmatized: [u'<', u'Disease', u'Room', u'<', u'/I', u'>', u'<', u'BR', u'>', u'<', u'BR', u'>', u'friend', u'Kevin', u'room', u'rectangular', u'table', u'television', u'corner', u'room', u'classmate', u'Ned', u'Stallone', u'security', u'guard', u'stop', u'enters', u'mask', u'mouth', u'disease', u'room', u'sitting', u'table', u'bowl', u'fruit', u'ice', u'cream', u'chunk', u'Ned', u'Major', u'Hall', u'Dormitory']\n",
      "Counter({u'<': 4, u'>': 3, u'room': 3, u'Ned': 2, u'BR': 2, u'table': 2, u'Major': 1, u'Stallone': 1, u'/I': 1, u'Hall': 1, u'Dormitory': 1, u'guard': 1, u'corner': 1, u'cream': 1, u'enters': 1, u'bowl': 1, u'ice': 1, u'chunk': 1, u'friend': 1, u'Room': 1, u'sitting': 1, u'stop': 1, u'Disease': 1, u'fruit': 1, u'mouth': 1, u'television': 1, u'mask': 1, u'disease': 1, u'rectangular': 1, u'classmate': 1, u'security': 1, u'Kevin': 1})\n",
      "\n",
      "\n",
      "Madeline 1: High School (98 dreams) [F]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'was', u'swimming', u'in', u'a', u'river', u'.', u'For', u'some', u'reason', u'I', u'remember', u'my', u'father', u'was', u'with', u'me', u'.', u'We', u'started', u'swimming', u'or', u'drifting', u'a', u'ways', u',', u'but', u'came', u'to', u'an', u'area', u'where', u'the', u'river', u'was', u'full', u'of', u'corpses', u'floating', u'around', u'.', u'I', u'was', u'disturbed', u'and', u'tried', u'to', u'stay', u'away', u'from', u'the', u'dead', u'bodies', u',', u'but', u'the', u'waves', u'splashing', u'made', u'would', u'only', u'bring', u'them', u'closer', u',', u'so', u'it', u'was', u'a', u'struggle', u'to', u'get', u'away', u'from', u'the', u\"'contaminated\", u'area', u\"'\", u'.', u'I', u'was', u'afraid', u'to', u'be', u'near', u'the', u'bodies', u'.']\n",
      "noun only: [u'river', u'reason', u'father', u'ways', u'area', u'river', u'corpses', u'bodies', u'waves', u'struggle', u'area', u'bodies']\n",
      "lemmatized: [u'river', u'reason', u'father', u'way', u'area', u'river', u'corps', u'body', u'wave', u'struggle', u'area', u'body']\n",
      "Counter({u'body': 2, u'area': 2, u'river': 2, u'corps': 1, u'father': 1, u'wave': 1, u'reason': 1, u'struggle': 1, u'way': 1})\n",
      "\n",
      "\n",
      "Madeline 2: College Dorms (186 dreams) [F]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'saw', u'my', u'(', u'deceased', u')', u'maternal', u'Grandpa', u'Gerald', u'again', u'.', u'It', u'did', u\"n't\", u'seem', u'as', u'though', u'he', u'fully', u'welcomed', u'my', u'hug', u'.', u'The', u'strange', u'thing', u'was', u'that', u'he', u'told', u'me', u'several', u'Bible', u'references', u'.', u'Romans', u'1.1', u',', u'1.4', u',', u'possibly', u'1.7', u'or', u'``', u'sevens', u\"''\", u'.', u'I', u'remembered', u'thinking', u'this', u'was', u'important', u',', u'so', u'I', u'wrote', u'them', u'down', u'in', u'my', u'dream', u'so', u'I', u'could', u'look', u'them', u'up', u'later', u'and', u'find', u'out', u'what', u'they', u'were', u'.']\n",
      "noun only: [u'Grandpa', u'Gerald', u'hug', u'thing', u'references', u'Romans', u'sevens', u'dream']\n",
      "lemmatized: [u'Grandpa', u'Gerald', u'hug', u'thing', u'reference', u'Romans', u'seven', u'dream']\n",
      "Counter({u'seven': 1, u'hug': 1, u'reference': 1, u'Gerald': 1, u'Romans': 1, u'thing': 1, u'Grandpa': 1, u'dream': 1})\n",
      "\n",
      "\n",
      "Madeline 3: Off-Campus (348 dreams) [F]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'dreamt', u'of', u'driving', u'to', u'my', u'maternal', u'Grandma', u'Jane', u\"'s\", u'house', u'.', u'Once', u'there', u',', u'though', u',', u'my', u'(', u'deceased', u')', u'maternal', u'Grandpa', u'Gerald', u'showed', u'up', u'.', u'He', u'had', u'greenish', u'red', u'lesions', u'on', u'his', u'neck', u'and', u'shoulder', u'that', u'was', u'supposed', u'to', u'be', u'glaucoma', u';', u'I', u'think', u'it', u'was', u'actually', u'cancer', u'.', u'In', u'any', u'case', u',', u'it', u'was', u'good', u'to', u'see', u'him', u'again', u',', u'although', u'I', u'did', u\"n't\", u'get', u'to', u'give', u'him', u'one', u'of', u'those', u'long', u'hugs', u'like', u'I', u'used', u'to', u'always', u'do', u'in', u'dreams', u'.', u'I', u'remember', u'wondering', u'.', u'It', u'was', u'a', u'collective', u'dream', u',', u'having', u'him', u'there', u'.', u'Grandma', u'Jane', u'dreamt', u'it', u',', u'but', u'the', u'rest', u'of', u'us', u'could', u'see', u'him', u'too', u'.']\n",
      "noun only: [u'Grandma', u'Jane', u'house', u'Grandpa', u'Gerald', u'lesions', u'neck', u'shoulder', u'cancer', u'case', u'hugs', u'dreams', u'dream', u'Grandma', u'Jane', u'dreamt', u'rest']\n",
      "lemmatized: [u'Grandma', u'Jane', u'house', u'Grandpa', u'Gerald', u'lesion', u'neck', u'shoulder', u'cancer', u'case', u'hug', u'dream', u'dream', u'Grandma', u'Jane', u'dreamt', u'rest']\n",
      "Counter({u'Grandma': 2, u'Jane': 2, u'dream': 2, u'shoulder': 1, u'case': 1, u'hug': 1, u'lesion': 1, u'neck': 1, u'cancer': 1, u'house': 1, u'rest': 1, u'Grandpa': 1, u'Gerald': 1, u'dreamt': 1})\n",
      "\n",
      "\n",
      "Madeline 4: After College (294 dreams) [F]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'My', u'boyfriend', u'Jeremy', u'came', u'back', u'to', u'the', u'apartment', u'and', u'said', u'he', u'``', u'was', u'crashed', u\"''\", u'.', u'I', u'tried', u'to', u'look', u'over', u'the', u'balcony', u'to', u'see', u'the', u'damage', u'to', u'the', u'truck', u',', u'but', u'I', u'could', u\"n't\", u'see', u'.']\n",
      "noun only: [u'boyfriend', u'Jeremy', u'apartment', u'balcony', u'damage', u'truck']\n",
      "lemmatized: [u'boyfriend', u'Jeremy', u'apartment', u'balcony', u'damage', u'truck']\n",
      "Counter({u'apartment': 1, u'Jeremy': 1, u'damage': 1, u'truck': 1, u'boyfriend': 1, u'balcony': 1})\n",
      "\n",
      "\n",
      "Mack: A poor recaller (38 dreams) [M]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'There', u'was-a', u'white', u',', u'nylon', u'windbreaker', u'that', u'was', u'hanging', u'on', u'a', u'chair', u',', u'almost', u'as', u'if', u'on', u'display', u'.', u'It', u'was', u'of', u'an', u'old', u'style', u',', u'maybe', u'from', u'the', u'1970s', u'or', u'8Os', u'and', u'of', u'a', u'generic', u'cheap', u'quality', u'.', u'I', u'noted', u'that', u'I', u'liked', u'it', u'a', u'lot', u',', u'but', u'attributed', u'it', u'as', u'being', u'a', u'little', u'too', u'feminine', u'on', u'account', u'of', u'the', u'color', u'.', u'I', u'thought', u'it', u'would', u'look', u'attractive', u'on', u'one', u'of', u'my', u'two', u'female', u'housemates', u',', u'and', u'decided', u'to', u'suggest', u'it', u'to', u'them', u'.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun only: [u'windbreaker', u'chair', u'display', u'style', u'quality', u'lot', u'account', u'color', u'housemates']\n",
      "lemmatized: [u'windbreaker', u'chair', u'display', u'style', u'quality', u'lot', u'account', u'color', u'housemate']\n",
      "Counter({u'style': 1, u'color': 1, u'account': 1, u'housemate': 1, u'lot': 1, u'windbreaker': 1, u'chair': 1, u'quality': 1, u'display': 1})\n",
      "\n",
      "\n",
      "Mark: a young boy (23 dreams) [M]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'was', u'at', u'the', u'park', u'.', u'I', u'had', u'these', u'little', u'cars', u'in', u'my', u'hand', u',', u'I', u'was', u'riding', u'a', u'bicycle', u'.', u'The', u'I', u'crashed', u'into', u'a', u'car', u'.', u'The', u'cars', u'in', u'my', u'hand', u'got', u'bigger', u'and', u'I', u'got', u'in', u'them', u'.', u'This', u'little', u'girl', u'had', u'some', u'in', u'her', u'hand', u'and', u'wanted', u'to', u'drive', u'in', u'them', u'.', u'I', u'went', u'into', u'a', u'fort', u'and', u'this', u'guy', u'did', u\"n't\", u'want', u'us', u'to', u'go', u'in', u'it', u'.', u'I', u'was', u'in', u'the', u'house', u'a', u'lot', u'.', u'We', u'were', u'at', u'our', u'house', u'.', u'John', u'was', u'telling', u'a', u'very', u',', u'very', u',', u'scary', u'story', u'.', u'And', u'the', u'car', u'came', u'to', u'our', u'house', u'.', u'Then', u'I', u'thought', u'I', u'was', u'in', u'my', u'house', u'in', u'bed', u'and', u'I', u'woke', u'up', u'in', u'my', u'school', u'.', u'And', u'then', u',', u'my', u'teacher', u'said', u'we', u'can', u'have', u'drinks', u'.', u'I', u'got', u'a', u'drink', u'.', u'And', u'then', u'a', u'little', u'boy', u'came', u'up', u'to', u'me', u'and', u'he', u'said', u',', u'What', u\"'s\", u'the', u'matter', u'and', u'I', u'said', u'my', u'mom', u'is', u'missing', u'.', u'And', u'then', u'he', u'said', u',', u'want', u'to', u'look', u'at', u'JC', u'Penney', u'?', u'We', u\"'re\", u'at', u'JC', u'Penney', u',', u'.and', u'I', u'woke', u'up', u'.']\n",
      "noun only: [u'park', u'cars', u'hand', u'bicycle', u'car', u'cars', u'hand', u'girl', u'hand', u'fort', u'guy', u'house', u'lot', u'house', u'John', u'story', u'car', u'house', u'house', u'bed', u'school', u'teacher', u'drinks', u'drink', u'boy', u'matter', u'mom', u'JC', u'Penney', u'JC', u'Penney', u'.and']\n",
      "lemmatized: [u'park', u'car', u'hand', u'bicycle', u'car', u'car', u'hand', u'girl', u'hand', u'fort', u'guy', u'house', u'lot', u'house', u'John', u'story', u'car', u'house', u'house', u'bed', u'school', u'teacher', u'drink', u'drink', u'boy', u'matter', u'mom', u'JC', u'Penney', u'JC', u'Penney', u'.and']\n",
      "Counter({u'car': 4, u'house': 4, u'hand': 3, u'Penney': 2, u'JC': 2, u'drink': 2, u'boy': 1, u'story': 1, u'bicycle': 1, u'mom': 1, u'park': 1, u'school': 1, u'bed': 1, u'matter': 1, u'teacher': 1, u'lot': 1, u'girl': 1, u'John': 1, u'.and': 1, u'guy': 1, u'fort': 1})\n",
      "\n",
      "\n",
      "Melissa: a young girl (89 dreams) [F]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'dreamed', u'that', u'a', u'tiger', u'named', u'Shirkan', u'got', u'in', u'the', u'house', u'.', u'We', u'came', u'back', u'and', u'got', u'him', u'out', u'.', u'There', u'was', u'a', u'radio', u'with', u'a', u'microphone', u'and', u'I', u'yelled', u'Shirkan', u'get', u'out', u'of', u'my', u'house', u'.']\n",
      "noun only: [u'tiger', u'Shirkan', u'house', u'radio', u'microphone', u'Shirkan', u'house']\n",
      "lemmatized: [u'tiger', u'Shirkan', u'house', u'radio', u'microphone', u'Shirkan', u'house']\n",
      "Counter({u'house': 2, u'Shirkan': 2, u'tiger': 1, u'microphone': 1, u'radio': 1})\n",
      "\n",
      "\n",
      "Merri: an artist (315 dreams) [F]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'South', u'of', u'Canal', u'St.', u'in', u'Chinatown', u'we', u'were', u'using', u'the', u'Chinese', u'factory', u'after', u'hours', u'to', u'frame', u'pictures', u'and', u'glue', u'them', u'.', u'Joe', u'Fong', u'cut', u'out', u'beautiful', u',', u'eery', u',', u'disturbed', u'pictures', u'of', u'blue', u'houses', u'and', u'glued', u'them', u'against', u'black', u'and', u'white', u'skies', u'.', u'He', u'left', u'glue', u'bubbles', u'everywhere', u'.', u'I', u'went', u'down', u'to', u'the', u'corner', u'to', u'wait', u'for', u'my', u'sister', u'.', u'I', u'waited', u'and', u'waited', u'.', u'A', u'guy', u'came', u'by', u'and', u'said', u',', u'``', u'What', u'are', u'you', u'doing', u'?', u\"''\", u'And', u'I', u'said', u',', u'``', u'I', u\"'m\", u'not', u'sure', u'.', u'If', u'I', u'wait', u'here', u'my', u'sister', u'will', u'walk', u'by', u'.', u\"''\", u'He', u'said', u',', u'``', u'Then', u'I', u\"'ll\", u'wait', u'for', u'you', u'.', u\"''\", u'I', u'climbed', u'up', u'the', u'ladder', u'to', u'look', u'and', u'see', u'a', u'few', u'of', u'the', u'cards', u'we', u\"'d\", u'made', u'but', u'really', u'they', u'were', u'junk', u'.', u'William', u'accidentally', u'threw', u'a', u'roll', u'of', u'paper', u'towels', u'out', u'the', u'window', u'onto', u'a', u'busy', u'Chinatown', u'street', u'.', u'Then', u'Carlos', u'threw', u'a', u'stapler', u'out', u'the', u'window', u'.', u'Then', u'a', u'stranger', u'threw', u'a', u'wrench', u'.', u'We', u'agreed', u'it', u'was', u'time', u'to', u'pour', u'wet', u'cement', u'.', u'One', u'inch', u'thick', u'and', u'boards', u'on', u'sides', u'.', u'The', u'Chinese', u'children', u'were', u'running', u'a', u'scam', u'.', u'I', u'could', u'see', u'the', u'sunlight', u'in', u'little', u'girls', u\"'\", u'hair', u'.', u'Golden', u'sunshine', u'sparkles', u'.', u'I', u'looked', u'over', u'my', u'shoulder', u'for', u'Dora', u'.', u'It', u'was', u'an', u'old', u'show', u'from', u'a', u'TV', u'that', u'was', u'playing', u'.', u'I', u'walked', u'up', u'a', u'steep', u'staircase', u'and', u'was', u'trying', u'to', u'find', u'where', u'we', u'parked', u'the', u'car', u'in', u'Chinatown', u'.', u'I', u'remembered', u'kicking', u'the', u'door', u'3', u'times', u'.', u'On', u'top', u'of', u'the', u'building', u'was', u's', u'steep', u'roof', u'covered', u'with', u'tiny', u'Chinese', u'lanterns', u'.', u'I', u'started', u'stomping', u'lanterns', u'and', u'a', u'girl', u'said', u',', u'``', u'What', u\"'s\", u'wrong', u'?', u\"''\", u'And', u'I', u'said', u',', u'``', u'My', u'sister', u'used', u'to', u'love', u'these', u'.', u\"''\", u'She', u'said', u',', u'``', u'Have', u'you', u'ever', u'had', u'a', u'telepathic', u'experience', u'?', u\"''\", u'And', u'I', u'said', u',', u'``', u'Yes', u'.', u\"''\", u'An', u'artist', u'in', u'prison', u'was', u'standing', u'up', u'on', u'a', u'desk', u'.', u'His', u'cell', u'was', u'8-sided', u'with', u'windows', u'and', u'painted', u'yellow', u'.', u'He', u'was', u'sitting', u'tracing', u'his', u'feet', u'and', u'trying', u'to', u'paint', u'a', u'comma', u',', u'huge', u',', u'in', u'one', u'window', u'because', u',', u'he', u'said', u',', u'``', u'What', u\"'s\", u'behind', u'it', u'goes', u'on', u'and', u'on', u',', u'like', u'what', u\"'s\", u'behind', u'corners', u'.', u'Endless', u'words', u'.', u'Endless', u'space', u'.', u'He', u'was', u'happy', u'drawing', u'.']\n",
      "noun only: [u'South', u'Canal', u'St.', u'Chinatown', u'factory', u'hours', u'pictures', u'Joe', u'Fong', u'pictures', u'houses', u'skies', u'bubbles', u'corner', u'sister', u'guy', u'sister', u'ladder', u'cards', u'junk', u'William', u'roll', u'paper', u'towels', u'window', u'Chinatown', u'street', u'Carlos', u'stapler', u'window', u'stranger', u'wrench', u'time', u'cement', u'inch', u'thick', u'boards', u'sides', u'children', u'scam', u'sunlight', u'girls', u'hair', u'Golden', u'sunshine', u'sparkles', u'shoulder', u'Dora', u'show', u'TV', u'staircase', u'car', u'Chinatown', u'door', u'times', u'top', u'building', u'roof', u'lanterns', u'lanterns', u'girl', u'sister', u'experience', u'artist', u'prison', u'desk', u'cell', u'windows', u'yellow', u'feet', u'comma', u'window', u'corners', u'words', u'space', u'drawing']\n",
      "lemmatized: [u'South', u'Canal', u'St.', u'Chinatown', u'factory', u'hour', u'picture', u'Joe', u'Fong', u'picture', u'house', u'sky', u'bubble', u'corner', u'sister', u'guy', u'sister', u'ladder', u'card', u'junk', u'William', u'roll', u'paper', u'towel', u'window', u'Chinatown', u'street', u'Carlos', u'stapler', u'window', u'stranger', u'wrench', u'time', u'cement', u'inch', u'thick', u'board', u'side', u'child', u'scam', u'sunlight', u'girl', u'hair', u'Golden', u'sunshine', u'sparkle', u'shoulder', u'Dora', u'show', u'TV', u'staircase', u'car', u'Chinatown', u'door', u'time', u'top', u'building', u'roof', u'lantern', u'lantern', u'girl', u'sister', u'experience', u'artist', u'prison', u'desk', u'cell', u'window', u'yellow', u'foot', u'comma', u'window', u'corner', u'word', u'space', u'drawing']\n",
      "Counter({u'window': 4, u'Chinatown': 3, u'sister': 3, u'corner': 2, u'girl': 2, u'picture': 2, u'time': 2, u'lantern': 2, u'Canal': 1, u'scam': 1, u'show': 1, u'South': 1, u'house': 1, u'St.': 1, u'desk': 1, u'yellow': 1, u'hair': 1, u'paper': 1, u'comma': 1, u'experience': 1, u'Carlos': 1, u'thick': 1, u'cell': 1, u'space': 1, u'TV': 1, u'top': 1, u'sky': 1, u'factory': 1, u'sunlight': 1, u'William': 1, u'sparkle': 1, u'Dora': 1, u'Joe': 1, u'prison': 1, u'inch': 1, u'bubble': 1, u'roll': 1, u'board': 1, u'junk': 1, u'door': 1, u'building': 1, u'staircase': 1, u'word': 1, u'ladder': 1, u'sunshine': 1, u'cement': 1, u'child': 1, u'foot': 1, u'stapler': 1, u'card': 1, u'shoulder': 1, u'Golden': 1, u'towel': 1, u'stranger': 1, u'hour': 1, u'artist': 1, u'Fong': 1, u'car': 1, u'roof': 1, u'drawing': 1, u'street': 1, u'wrench': 1, u'guy': 1, u'side': 1})\n",
      "\n",
      "\n",
      "Miami Home-Lab: Home (171 dreams) [M]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'dreamt', u'I', u'was', u'at', u'a', u'party', u'.', u'Everyone', u'was', u'having', u'a', u'good', u'time', u'except', u'the', u'hostess', u'.', u'She', u'hated', u'her', u'husband', u'and', u'wanted', u'to', u'kill', u'him', u'.', u'I', u'remember', u'helping', u'her', u'look', u'for', u'something', u'to', u'throw', u'at', u'her', u'husband', u'.', u'We', u'were', u'in', u'the', u'kitchen', u',', u'and', u'all', u'of', u'the', u'kitchen', u'drawers', u'were', u'filled', u'with', u'tweezers', u'and', u'pipe', u'wrenches', u'.']\n",
      "noun only: [u'party', u'Everyone', u'time', u'hostess', u'husband', u'look', u'something', u'husband', u'kitchen', u'kitchen', u'drawers', u'tweezers', u'wrenches']\n",
      "lemmatized: [u'party', u'Everyone', u'time', u'hostess', u'husband', u'look', u'something', u'husband', u'kitchen', u'kitchen', u'drawer', u'tweezer', u'wrench']\n",
      "Counter({u'husband': 2, u'kitchen': 2, u'Everyone': 1, u'look': 1, u'hostess': 1, u'tweezer': 1, u'wrench': 1, u'drawer': 1, u'something': 1, u'time': 1, u'party': 1})\n",
      "\n",
      "\n",
      "Miami Home-Lab: Lab (274 dreams) [M]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'was', u'going', u'to', u'Georgia', u'and', u'I', u'remembered', u'the', u'tune', u',', u'the', u'song', u',', u'``', u'I', u\"'m\", u'going', u'back', u'to', u'Georgia', u'.', u\"''\", u'I', u'heard', u'the', u'last', u'few', u'days', u'on', u'the', u'radio', u'.', u'I', u'was', u'humming', u'the', u'song', u'to', u'myself', u'when', u'I', u'was', u'going', u'back', u'.', u'I', u'was', u'sitting', u'in', u'a', u'train', u'and', u'I', u'was', u'looking', u'out', u'the', u'window', u'humming', u'this', u'tune', u'to', u'myself', u'.']\n",
      "noun only: [u'Georgia', u'tune', u'song', u'Georgia', u'days', u'radio', u'song', u'train', u'window', u'tune']\n",
      "lemmatized: [u'Georgia', u'tune', u'song', u'Georgia', u'day', u'radio', u'song', u'train', u'window', u'tune']\n",
      "Counter({u'Georgia': 2, u'song': 2, u'tune': 2, u'window': 1, u'train': 1, u'radio': 1, u'day': 1})\n",
      "\n",
      "\n",
      "Melora (Melvin's wife) (211 dreams) [F]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'We', u'were', u'in', u'a', u'used', u'car', u'lot', u'looking', u'for', u'a', u'car', u',', u'and', u'most', u'of', u'the', u'cars', u'that', u'we', u'saw', u'were', u'little', u'white', u'Fiats', u'like', u'the', u'one', u'that', u'we', u'have', u'.', u'The', u'car', u'lot', u'was', u'up', u'in', u'the', u'mountains', u'and', u'there', u'were', u'all', u'sorts', u'of', u'winding', u'roads', u'around', u'it', u'.', u'The', u'car', u'dealer', u'was', u'showing', u'my', u'husband', u'one', u'car', u'that', u'was', u'made', u'out', u'of', u'some', u'sort', u'of', u'wood', u'and', u'it', u'was', u'covered', u'with', u'this', u'whitely', u'paint', u'and', u'it', u'was', u'a', u'very', u'funny', u'car', u'.', u'It', u'really', u'did', u\"n't\", u'look', u'much', u'like', u'a', u'car', u',', u'but', u'it', u'was', u'so', u'rotten', u'that', u'it', u'was', u'starting', u'to', u'fall', u'apart', u'right', u'in', u'the', u'middle', u'of', u'the', u'demonstration', u'.', u'I', u'knew', u'that', u'he', u'was', u'trying', u'to', u'sell', u'us', u'this', u'car', u'that', u'it', u'really', u'was', u\"n't\", u'any', u'good', u'.']\n",
      "noun only: [u'car', u'lot', u'car', u'cars', u'Fiats', u'one', u'car', u'lot', u'mountains', u'sorts', u'roads', u'car', u'dealer', u'husband', u'car', u'sort', u'wood', u'paint', u'car', u'car', u'middle', u'demonstration', u'car']\n",
      "lemmatized: [u'car', u'lot', u'car', u'car', u'Fiats', u'one', u'car', u'lot', u'mountain', u'sort', u'road', u'car', u'dealer', u'husband', u'car', u'sort', u'wood', u'paint', u'car', u'car', u'middle', u'demonstration', u'car']\n",
      "Counter({u'car': 9, u'sort': 2, u'lot': 2, u'mountain': 1, u'middle': 1, u'one': 1, u'paint': 1, u'wood': 1, u'husband': 1, u'Fiats': 1, u'demonstration': 1, u'dealer': 1, u'road': 1})\n",
      "\n",
      "\n",
      "Melvin (Melora's husband) (128 dreams) [M]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'was', u'in', u'the', u'basement', u'of', u'another', u'man', u\"'s\", u'house', u'.', u'I', u'do', u\"n't\", u'know', u'who', u'this', u'man', u'was', u'.', u'I', u'knew', u'him', u'in', u'the', u'dream', u',', u'but', u'I', u'ca', u\"n't\", u'recall', u'his', u'identity', u'.', u'There', u'was', u'some', u'sort', u'of', u'tunnel', u'or', u'shaft', u'leading', u'into', u'the', u'basement', u'of', u'the', u'man', u\"'s\", u'house', u',', u'and', u'there', u'had', u'been', u'reports', u'or', u'something', u'like', u'that', u'of', u'some', u'sort', u'of', u'creature', u'coming', u'out', u'of', u'this', u'tunnel', u'.', u'I', u'was', u'arguing', u'with', u'the', u'man', u'that', u'he', u'should', u'put', u'up', u'some', u'sort', u'of', u'steel', u'doors', u'or', u'steel', u'building', u'over', u'the', u'tunnel', u'so', u'nothing', u'could', u'get', u'out', u'of', u'it', u'that', u'was', u\"n't\", u'under', u'our', u'control', u'.', u'But', u'he', u'did', u\"n't\", u'seem', u'to', u'think', u'that', u'this', u'was', u'necessary', u'or', u'something', u'to', u'that', u'effect', u'.', u'Then', u'I', u'left', u'.', u'I', u'did', u\"n't\", u'want', u'to', u'be', u'around', u'in', u'the', u'evening', u'when', u'this', u'creature', u'or', u'whatever', u'it', u'was', u',', u'came', u'out', u'.', u'But', u'somehow', u'there', u'was', u'some', u'sort', u'of', u'business', u'meeting', u'that', u'was', u'called', u'for', u'that', u'evening', u'right', u'down', u'in', u'that', u'basement', u',', u'and', u'everyone', u'had', u'to', u'come', u'.', u'I', u'came', u'and', u'one', u'or', u'two', u'other', u'men', u'came', u'that', u'were', u'in', u'this', u'company', u',', u'and', u'I', u'do', u\"n't\", u'know', u'what', u'the', u'business', u'of', u'the', u'company', u'was', u'or', u'anything', u'.', u'The', u'other', u'men', u'became', u'frightened', u'and', u'left', u'as', u'the', u'evening', u'went', u'on', u'.', u'I', u'left', u'also', u'because', u'I', u'thought', u'this', u'creature', u'was', u'bound', u'to', u'put', u'in', u'an', u'appearance', u'where', u'it', u'might', u'kill', u'and', u'maim', u'or', u'something', u'like', u'that', u'.', u'So', u'I', u'left', u'and', u'the', u'person', u'who', u'owned', u'the', u'house', u'was', u'frightened', u'and', u'did', u\"n't\", u'want', u'to', u'be', u'left', u'alone', u',', u'but', u'I', u'sort', u'of', u'thought', u'as', u'I', u'was', u'leaving', u'that', u'I', u'told', u'him', u'so', u',', u'and', u'so', u'he', u'better', u'take', u'the', u'consequences', u'for', u'not', u'doing', u'what', u'I', u'told', u'him', u'.', u'I', u'went', u'out', u'and', u'went', u'to', u'a', u'parking', u'lot', u'and', u'got', u'into', u'my', u'car', u'and', u'was', u'quite', u'sure', u'to', u'lock', u'the', u'doors', u'behind', u'me', u'as', u'I', u'got', u'in', u'the', u'car', u'.', u'I', u'was', u'planning', u'to', u'drive', u'out', u'and', u'I', u'heard', u'the', u'man', u'who', u'owned', u'the', u'house', u'screaming', u'at', u'this', u'point', u'.', u'The', u'man', u'was', u'about', u'my', u'age', u'incidentally', u',', u'or', u'a', u'few', u'years', u'older', u'.', u'So', u'I', u'went', u'back', u'to', u'the', u'house', u'.', u'I', u'don', u\"'\", u'know', u'whether', u'I', u'drove', u'or', u'whether', u'I', u'got', u'out', u'of', u'the', u'car', u'again', u',', u'hoping', u'to', u'rescue', u'him', u'or', u'something', u'.', u'My', u'recall', u'is', u'quite', u'vague', u'at', u'this', u'point', u'.', u'But', u'I', u'know', u'I', u'got', u'there', u'and', u'there', u'was', u\"n't\", u'a', u'creature', u'there', u'or', u'anything', u'like', u'that', u'.', u'I', u'then', u'started', u'talking', u'to', u'the', u'man', u'and', u'interpreting', u'the', u'whole', u'thing', u'in', u'which', u'this', u'tunnel', u'leading', u'down', u'was', u'the', u'id', u'and', u'there', u'was', u'an', u'ego', u'and', u'superego', u'in', u'there', u',', u'so', u'I', u'was', u'interpreting', u'the', u'whole', u'experience', u'he', u'had', u'or', u'perhaps', u'it', u'was', u'the', u'dream', u'itself', u',', u'I', u\"'m\", u'not', u'sure', u',', u'in', u'a', u'strictly', u'psychoanalytic', u'framework', u'.']\n",
      "noun only: [u'basement', u'man', u'house', u'man', u'dream', u'identity', u'sort', u'tunnel', u'shaft', u'basement', u'man', u'house', u'reports', u'something', u'sort', u'creature', u'tunnel', u'man', u'sort', u'steel', u'doors', u'steel', u'building', u'tunnel', u'nothing', u'control', u'something', u'effect', u'evening', u'creature', u'sort', u'business', u'meeting', u'evening', u'basement', u'everyone', u'men', u'company', u'business', u'company', u'anything', u'men', u'evening', u'creature', u'appearance', u'something', u'person', u'house', u'thought', u'consequences', u'parking', u'lot', u'car', u'doors', u'car', u'man', u'house', u'point', u'man', u'age', u'years', u'house', u'car', u'something', u'recall', u'point', u'creature', u'anything', u'man', u'thing', u'tunnel', u'id', u'ego', u'superego', u'experience', u'dream', u'framework']\n",
      "lemmatized: [u'basement', u'man', u'house', u'man', u'dream', u'identity', u'sort', u'tunnel', u'shaft', u'basement', u'man', u'house', u'report', u'something', u'sort', u'creature', u'tunnel', u'man', u'sort', u'steel', u'door', u'steel', u'building', u'tunnel', u'nothing', u'control', u'something', u'effect', u'evening', u'creature', u'sort', u'business', u'meeting', u'evening', u'basement', u'everyone', u'men', u'company', u'business', u'company', u'anything', u'men', u'evening', u'creature', u'appearance', u'something', u'person', u'house', u'thought', u'consequence', u'parking', u'lot', u'car', u'door', u'car', u'man', u'house', u'point', u'man', u'age', u'year', u'house', u'car', u'something', u'recall', u'point', u'creature', u'anything', u'man', u'thing', u'tunnel', u'id', u'ego', u'superego', u'experience', u'dream', u'framework']\n",
      "Counter({u'man': 7, u'house': 5, u'something': 4, u'creature': 4, u'sort': 4, u'tunnel': 4, u'evening': 3, u'basement': 3, u'car': 3, u'point': 2, u'steel': 2, u'door': 2, u'business': 2, u'company': 2, u'men': 2, u'anything': 2, u'dream': 2, u'control': 1, u'everyone': 1, u'year': 1, u'report': 1, u'parking': 1, u'id': 1, u'thing': 1, u'superego': 1, u'lot': 1, u'meeting': 1, u'effect': 1, u'framework': 1, u'nothing': 1, u'identity': 1, u'building': 1, u'shaft': 1, u'age': 1, u'appearance': 1, u'experience': 1, u'thought': 1, u'person': 1, u'ego': 1, u'recall': 1, u'consequence': 1})\n",
      "\n",
      "\n",
      "Nancy: Caring & headstrong (44 dreams) [F]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'was', u'parked', u'outside', u'my', u'mom', u\"'s\", u'house', u'and', u'my', u'little', u'girl', u',', u'Cindy', u',', u'was', u'still', u'in', u'her', u'car', u'seat', u'.', u'I', u'saw', u'Alice', u',', u'who', u'used', u'to', u'be', u'married', u'to', u'my', u'boyfriend', u',', u'Zack', u',', u'come', u'out', u'from', u'a', u'house', u'across', u'the', u'street', u'.', u'She', u'had', u'a', u'gift', u'for', u'Cindy', u';', u'it', u'was', u'a', u'dress', u'.', u'I', u'thought', u'to', u'myself', u',', u'it', u'was', u'about', u'time', u'she', u'made', u'a', u'gesture', u',', u'being', u'that', u'her', u'daughter', u'was', u'Cindy', u\"'s\", u'half-sister', u'and', u'I', u'always', u'got', u'gifts', u'for', u'her', u'.', u'Because', u'she', u'was', u'being', u'decent', u'for', u'a', u'change', u',', u'I', u'was', u'not', u'mean', u'to', u'her', u'.', u'She', u'was', u'standing', u'in', u'the', u'doorway', u'on', u'the', u'passenger', u'side', u',', u'talking', u'to', u'Cindy', u',', u'when', u'Zack', u'drove', u'up', u'.', u'He', u'slammed', u'on', u'his', u'brakes', u'.', u'I', u'ran', u'over', u'to', u'his', u'truck', u'.', u'He', u'was', u'furious', u'.', u'What', u'is', u'she', u'doing', u',', u'kissing', u'up', u'to', u'you', u'?', u'I', u'explained', u'to', u'him', u'I', u'was', u'not', u'becoming', u'any', u'friend', u'of', u'hers', u'.', u'Then', u'he', u'just', u'started', u'kissing', u'me', u'and', u'he', u'pulled', u'me', u'into', u'the', u'truck', u'.', u'He', u'was', u'all', u'over', u'me', u'right', u'in', u'front', u'of', u'her', u',', u'which', u'was', u'unusual', u'.']\n",
      "noun only: [u'mom', u'house', u'girl', u'Cindy', u'car', u'seat', u'Alice', u'boyfriend', u'Zack', u'house', u'street', u'gift', u'Cindy', u'dress', u'time', u'gesture', u'daughter', u'Cindy', u'half-sister', u'gifts', u'change', u'doorway', u'passenger', u'side', u'Cindy', u'Zack', u'brakes', u'truck', u'friend', u'hers', u'truck', u'front']\n",
      "lemmatized: [u'mom', u'house', u'girl', u'Cindy', u'car', u'seat', u'Alice', u'boyfriend', u'Zack', u'house', u'street', u'gift', u'Cindy', u'dress', u'time', u'gesture', u'daughter', u'Cindy', u'half-sister', u'gift', u'change', u'doorway', u'passenger', u'side', u'Cindy', u'Zack', u'brake', u'truck', u'friend', u'hers', u'truck', u'front']\n",
      "Counter({u'Cindy': 4, u'house': 2, u'Zack': 2, u'gift': 2, u'truck': 2, u'passenger': 1, u'seat': 1, u'front': 1, u'street': 1, u'brake': 1, u'girl': 1, u'dress': 1, u'friend': 1, u'half-sister': 1, u'Alice': 1, u'mom': 1, u'hers': 1, u'change': 1, u'boyfriend': 1, u'daughter': 1, u'car': 1, u'doorway': 1, u'time': 1, u'side': 1, u'gesture': 1})\n",
      "\n",
      "\n",
      "The Natural Scientist (234 dreams) [M]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'Smoked', u'a', u'cigarette', u',', u'and', u'felt', u'rather', u'despondent', u'and', u'uncomfortable', u'because', u'of', u'breaking', u'a', u'resolution', u'.']\n",
      "noun only: [u'cigarette', u'resolution']\n",
      "lemmatized: [u'cigarette', u'resolution']\n",
      "Counter({u'cigarette': 1, u'resolution': 1})\n",
      "\n",
      "\n",
      "Norman: a child molester (1235 dreams) [M]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'was', u'in', u'a', u'state', u'of', u'anxiety', u'throughout', u'the', u'dream', u'.', u'I', u'saw', u'a', u'man', u'lying', u'on', u'the', u'floor', u',', u'shot', u'.', u'I', u'was', u'confined', u'to', u'an', u'institution', u'which', u'I', u'had', u'never', u'seen', u'in', u'real', u'life', u'.', u'I', u'left', u'without', u'permission', u'.', u'Someone', u'tried', u'to', u'persuade', u'a', u'man', u'with', u'a', u'model', u'T', u'car', u'not', u'to', u'go', u',', u'but', u'he', u'insisted', u'.']\n",
      "noun only: [u'state', u'anxiety', u'dream', u'man', u'floor', u'shot', u'institution', u'life', u'permission', u'Someone', u'man', u'model', u'T', u'car']\n",
      "lemmatized: [u'state', u'anxiety', u'dream', u'man', u'floor', u'shot', u'institution', u'life', u'permission', u'Someone', u'man', u'model', u'T', u'car']\n",
      "Counter({u'man': 2, u'life': 1, u'anxiety': 1, u'shot': 1, u'floor': 1, u'car': 1, u'permission': 1, u'state': 1, u'Someone': 1, u'T': 1, u'model': 1, u'dream': 1, u'institution': 1})\n",
      "\n",
      "\n",
      "Hall/VdC Norms: Female (491 dreams) [F]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'dreamed', u'it', u'was', u'next', u'summer', u'and', u'that', u'I', u'was', u'going', u'to', u'be', u'married', u'to', u'my', u'boyfriend', u'at', u'home', u'.', u'Mother', u'advised', u'me', u'not', u'to', u',', u'but', u'said', u'she', u'would', u'not', u'stand', u'in', u'my', u'way', u'.', u'He', u'had', u'very', u'little', u'money', u'and', u'George', u',', u'after', u'finally', u'convincing', u'me', u'not', u'to', u'finish', u'school', u',', u'said', u'maybe', u'mother', u'was', u'right', u'.', u'I', u'had', u'made', u'up', u'my', u'mind', u',', u'however', u',', u'and', u'would', u'not', u'hear', u'of', u'a', u'postponement', u'.', u'The', u'next', u'part', u'is', u'after', u'we', u'are', u'married', u'.', u'Although', u'I', u'did', u'not', u'see', u'the', u'ceremony', u',', u'I', u'knew', u'it', u'was', u'very', u'simple', u'.', u'Someone', u'asked', u'where', u'we', u'should', u'go', u'on', u'our', u'honeymoon', u'and', u'I', u'ca', u\"n't\", u'remember', u'where', u'I', u'said', u',', u'but', u'they', u'thought', u'it', u'was', u'the', u'place', u'in', u'Europe', u'named', u'the', u'same', u'.', u'I', u'was', u'embarrassed', u'when', u'I', u'said', u'it', u'was', u'not', u'.', u'Then', u'I', u'thought', u'maybe', u'I', u'should', u'have', u'married', u'another', u'fellow', u'I', u'know', u'who', u'would', u'have', u'taken', u'me', u'to', u'Europe', u',', u'but', u'I', u'immediately', u'dismissed', u'this', u'from', u'my', u'mind', u'.', u'Mother', u'said', u'then', u'that', u'the', u'place', u'we', u'were', u'going', u'was', u'very', u'nice', u',', u'although', u'neither', u'of', u'us', u'had', u'previously', u'been', u'there', u'.', u'This', u'made', u'me', u'feel', u'much', u'better', u'.', u'Next', u'we', u'were', u'sitting', u'in', u'a', u'large', u'dining', u'room', u'.', u'George', u'was', u'drinking', u'a', u'beer', u'and', u'I', u'had', u'a', u'coke', u'.', u'All', u'of', u'a', u'sudden', u'it', u'struck', u'me', u'that', u'this', u'was', u'my', u'wedding', u'night', u'and', u'I', u'got', u'nervous', u'and', u'sort', u'of', u'afraid', u'.', u'I', u'then', u'asked', u'George', u'to', u'order', u'me', u'a', u'double', u'shot', u'which', u'I', u'never', u'did', u'get', u'to', u'drink', u'because', u'I', u'woke', u'up', u'.']\n",
      "noun only: [u'summer', u'boyfriend', u'home', u'Mother', u'way', u'money', u'George', u'school', u'mother', u'mind', u'postponement', u'part', u'ceremony', u'Someone', u'honeymoon', u'place', u'Europe', u'fellow', u'Europe', u'mind', u'Mother', u'place', u'dining', u'room', u'George', u'beer', u'coke', u'wedding', u'night', u'sort', u'afraid', u'George', u'order', u'shot']\n",
      "lemmatized: [u'summer', u'boyfriend', u'home', u'Mother', u'way', u'money', u'George', u'school', u'mother', u'mind', u'postponement', u'part', u'ceremony', u'Someone', u'honeymoon', u'place', u'Europe', u'fellow', u'Europe', u'mind', u'Mother', u'place', u'dining', u'room', u'George', u'beer', u'coke', u'wedding', u'night', u'sort', u'afraid', u'George', u'order', u'shot']\n",
      "Counter({u'George': 3, u'mind': 2, u'Europe': 2, u'Mother': 2, u'place': 2, u'summer': 1, u'shot': 1, u'money': 1, u'honeymoon': 1, u'fellow': 1, u'home': 1, u'ceremony': 1, u'Someone': 1, u'dining': 1, u'beer': 1, u'way': 1, u'sort': 1, u'coke': 1, u'night': 1, u'postponement': 1, u'part': 1, u'wedding': 1, u'boyfriend': 1, u'school': 1, u'room': 1, u'order': 1, u'afraid': 1, u'mother': 1})\n",
      "\n",
      "\n",
      "Hall/VdC Norms: Male (500 dreams) [M]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'was', u'in', u'professor', u'Teimes', u\"'\", u'classroom', u'in', u'Corp.', u'Finance', u'.', u'The', u'room', u'was', u'larger', u'than', u'usual', u'.', u'We', u'had', u'several', u'women', u'in', u'the', u'class', u',', u'which', u'is', u'not', u'as', u'it', u'is', u'.', u'The', u'professor', u'was', u'asking', u'the', u'class', u'questions', u'on', u'our', u'work', u'.', u'Being', u'unprepared', u',', u'I', u'feared', u'the', u'time', u'when', u'I', u'would', u'be', u'asked', u'a', u'question', u'.', u'Finally', u'I', u'was', u'asked', u'a', u'question', u',', u'which', u'evidently', u'I', u'answered', u'all', u'right', u',', u'since', u'there', u'were', u'none', u'of', u'the', u'repercussions', u'I', u'had', u'feared', u'due', u'to', u'my', u'unpreparedness', u'.']\n",
      "noun only: [u'professor', u'Teimes', u'classroom', u'Corp.', u'Finance', u'room', u'women', u'class', u'professor', u'class', u'questions', u'work', u'Being', u'time', u'question', u'question', u'right', u'none', u'repercussions', u'unpreparedness']\n",
      "lemmatized: [u'professor', u'Teimes', u'classroom', u'Corp.', u'Finance', u'room', u'woman', u'class', u'professor', u'class', u'question', u'work', u'Being', u'time', u'question', u'question', u'right', u'none', u'repercussion', u'unpreparedness']\n",
      "Counter({u'question': 3, u'professor': 2, u'class': 2, u'classroom': 1, u'unpreparedness': 1, u'none': 1, u'woman': 1, u'Finance': 1, u'Being': 1, u'work': 1, u'repercussion': 1, u'right': 1, u'time': 1, u'Teimes': 1, u'Corp.': 1, u'room': 1})\n",
      "\n",
      "\n",
      "Pegasus: a factory worker (1093 dreams) [M]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'saw', u'a', u'lot', u'of', u'sparrows', u'playing', u'in', u'the', u'dust', u'on', u'the', u'ground', u'.', u'I', u'shot', u'a', u'couple', u'of', u'bb', u\"'s\", u'at', u'them', u'as', u'they', u'walked', u'and', u'some', u'were', u'as', u'large', u'as', u'quail', u'.', u'I', u'sneaked', u'up', u'on', u'them', u'and', u'caught', u'a', u'baby', u'and', u'an', u'old', u'one', u'in', u'my', u'hand', u'.']\n",
      "noun only: [u'lot', u'sparrows', u'dust', u'ground', u'couple', u'bb', u'quail', u'baby', u'hand']\n",
      "lemmatized: [u'lot', u'sparrow', u'dust', u'ground', u'couple', u'bb', u'quail', u'baby', u'hand']\n",
      "Counter({u'bb': 1, u'sparrow': 1, u'couple': 1, u'hand': 1, u'baby': 1, u'lot': 1, u'dust': 1, u'quail': 1, u'ground': 1})\n",
      "\n",
      "\n",
      "Peruvian men (384 dreams) [M]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'was', u'with', u'two', u'friends', u'(', u'M', u')', u'and', u'we', u'were', u'walking', u'and', u'playing', u'jokes', u'on', u'one', u'another', u'.', u'Then', u'we', u'went', u'to', u'pick', u'up', u'a', u'girl', u'I', u'like', u'and', u'got', u'her', u'inside', u'the', u'car', u',', u'and', u'since', u'I', u'was', u'the', u'last', u'one', u',', u'they', u'were', u'going', u'to', u'play', u'a', u'joke', u'on', u'me', u'and', u'leave', u'me', u'behind', u'.', u'Somehow', u'I', u'was', u'able', u'to', u'get', u'in', u'the', u'car', u'and', u'I', u'got', u'angry', u'at', u'the', u'two', u'of', u'them', u'.', u'The', u'dream', u'ended', u'there', u'.']\n",
      "noun only: [u'friends', u'M', u'jokes', u'girl', u'car', u'joke', u'Somehow', u'car', u'dream']\n",
      "lemmatized: [u'friend', u'M', u'joke', u'girl', u'car', u'joke', u'Somehow', u'car', u'dream']\n",
      "Counter({u'joke': 2, u'car': 2, u'M': 1, u'Somehow': 1, u'girl': 1, u'dream': 1, u'friend': 1})\n",
      "\n",
      "\n",
      "Peruvian women (382 dreams) [F]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'dreamed', u'I', u'was', u'a', u'the', u'beach', u'with', u'my', u'sister', u',', u'sun-bathing', u',', u'and', u'next', u'to', u'me', u'there', u'was', u'a', u'girl', u'whom', u'I', u'do', u\"n't\", u'know', u'in', u'real', u'life', u'.', u'Then', u'we', u'went', u'to', u'this', u'girl', u\"'s\", u'house', u'and', u'some', u'boys', u'came', u'towards', u'us', u'and', u'they', u'threw', u'water', u'on', u'us', u'.', u'And', u'due', u'to', u'this', u'I', u'ran', u'to', u'my', u'house', u'and', u'stayed', u'there', u'till', u'my', u'sister', u'arrived', u'and', u'we', u'went', u'with', u'our', u'little', u'cousins', u'to', u'the', u'pastry', u'shop', u'to', u'buy', u'some', u'cake', u'.']\n",
      "noun only: [u'beach', u'sister', u'sun-bathing', u'life', u'girl', u'house', u'boys', u'towards', u'water', u'house', u'sister', u'cousins', u'pastry', u'shop', u'cake']\n",
      "lemmatized: [u'beach', u'sister', u'sun-bathing', u'life', u'girl', u'house', u'boy', u'towards', u'water', u'house', u'sister', u'cousin', u'pastry', u'shop', u'cake']\n",
      "Counter({u'sister': 2, u'house': 2, u'shop': 1, u'boy': 1, u'towards': 1, u'life': 1, u'sun-bathing': 1, u'water': 1, u'pastry': 1, u'cake': 1, u'cousin': 1, u'girl': 1, u'beach': 1})\n",
      "\n",
      "\n",
      "Phil 1: teens (106 dreams) [M]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'Uncle', u'Albert', u'is', u'getting', u'ready', u'to', u'leave', u'for', u'a', u'vacation', u'and', u'is', u'taking', u'me', u'along', u'.', u'I', u'do', u\"n't\", u'want', u'to', u'go', u',', u'and', u'I', u'tell', u'Uncle', u'Albert', u'that', u'someone', u'is', u'coming', u'by', u'to', u'get', u'me', u'at', u'11:00', u'a.m.', u',', u'but', u'it', u'makes', u'no', u'difference', u'.', u'Someone', u'does', u'finally', u'come', u'by', u'for', u'me', u',', u'though', u':', u'Uncle', u'Joe', u'.', u'On', u'the', u'way', u'home', u'(', u'or', u'wherever', u'we', u'are', u'going', u')', u',', u'we', u'stop', u'at', u'an', u'old', u'school', u'(', u'2-storey', u')', u'where', u'summer', u'school', u'is', u'going', u'on', u'.', u'I', u'am', u'to', u'tell', u'Pat', u'Byers', u'goodbye', u'.', u'There', u'are', u'about', u'8', u'rooms', u'on', u'each', u'floor', u'.', u'I', u'go', u'to', u'the', u'door', u'and', u'look', u'in', u'each', u'one', u'.', u'Everyone', u'is', u'apparently', u'having', u'a', u'lot', u'of', u'fun', u'.', u'When', u'I', u'am', u'spotted', u',', u'I', u'get', u'in', u'trouble', u'for', u'being', u'out', u'of', u'class', u',', u'but', u'I', u'explain', u'that', u'I', u'do', u\"n't\", u'go', u'to', u'school', u'there', u'and', u'I', u'have', u'come', u'to', u'tell', u'someone', u'goodbye', u'.', u'I', u'am', u'beginning', u'to', u'think', u'that', u'I', u'wo', u\"n't\", u'find', u'her', u'when', u'I', u'see', u'the', u'back', u'of', u'her', u'head', u',', u'in', u'the', u'16th', u'room', u'upstairs', u'.', u'I', u'tell', u'her', u'goodbye-I', u'do', u\"n't\", u'remember', u'how..', u'By', u'the', u'time', u'I', u'go', u'back', u'outside', u',', u'it', u'is', u'dusk', u',', u'and', u'almost', u'dark', u'.', u'I', u'wait', u'until', u'dark', u'for', u'Uncle', u'Joe', u',', u'look', u'around', u'the', u'building', u',', u'etc.', u',', u'but', u'ca', u\"n't\", u'find', u'him', u'.', u'I', u'am', u'growing', u'worried', u'.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun only: [u'Uncle', u'Albert', u'vacation', u'Uncle', u'Albert', u'someone', u'a.m.', u'difference', u'Someone', u'Uncle', u'Joe', u'way', u'home', u'school', u'summer', u'school', u'Pat', u'Byers', u'goodbye', u'rooms', u'floor', u'door', u'look', u'Everyone', u'lot', u'fun', u'trouble', u'class', u'school', u'someone', u'goodbye', u'back', u'head', u'room', u'upstairs', u'how..', u'time', u'dark', u'dark', u'Uncle', u'Joe', u'building', u'etc.']\n",
      "lemmatized: [u'Uncle', u'Albert', u'vacation', u'Uncle', u'Albert', u'someone', u'a.m.', u'difference', u'Someone', u'Uncle', u'Joe', u'way', u'home', u'school', u'summer', u'school', u'Pat', u'Byers', u'goodbye', u'room', u'floor', u'door', u'look', u'Everyone', u'lot', u'fun', u'trouble', u'class', u'school', u'someone', u'goodbye', u'back', u'head', u'room', u'upstairs', u'how..', u'time', u'dark', u'dark', u'Uncle', u'Joe', u'building', u'etc.']\n",
      "Counter({u'Uncle': 4, u'school': 3, u'Albert': 2, u'Joe': 2, u'someone': 2, u'dark': 2, u'room': 2, u'goodbye': 2, u'summer': 1, u'how..': 1, u'Pat': 1, u'upstairs': 1, u'back': 1, u'home': 1, u'Someone': 1, u'floor': 1, u'vacation': 1, u'lot': 1, u'head': 1, u'door': 1, u'way': 1, u'Byers': 1, u'trouble': 1, u'difference': 1, u'class': 1, u'building': 1, u'Everyone': 1, u'look': 1, u'a.m.': 1, u'time': 1, u'fun': 1, u'etc.': 1})\n",
      "\n",
      "\n",
      "Phil 2: late 20s (220 dreams) [M]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'was', u'speaking', u'to', u'Kathy', u'Reynault', u'at', u'night', u'in', u'front', u'of', u'her', u'house', u'(', u'but', u'not', u'her', u'real', u'house', u')', u'.', u'Her', u'mother', u'was', u'nearby', u',', u'and', u'I', u'either', u'realized', u'she', u'knew', u'who', u'I', u'was', u'or', u'Kathy', u'actually', u'introduced', u'us', u',', u'and', u'she', u'did', u\"n't\", u'seem', u'at', u'all', u'antagonistic', u'as', u'I', u'expected-nor', u'did', u'she', u'seem', u'particularly', u'friendly', u'.']\n",
      "noun only: [u'Kathy', u'Reynault', u'night', u'front', u'house', u'house', u'mother', u'Kathy']\n",
      "lemmatized: [u'Kathy', u'Reynault', u'night', u'front', u'house', u'house', u'mother', u'Kathy']\n",
      "Counter({u'Kathy': 2, u'house': 2, u'night': 1, u'Reynault': 1, u'mother': 1, u'front': 1})\n",
      "\n",
      "\n",
      "Phil 3: retirement (180 dreams) [M]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'am', u'at', u'some', u'kind', u'of', u'resort', u'with', u'a', u'few', u'people', u'I', u'know-maybe', u'three', u'girls', u'and', u'at', u'least', u'one', u'other', u'guy', u',', u'all', u'of', u'them', u'younger', u'than', u'me', u',', u'maybe', u'students', u'.', u'We', u'leave', u'the', u'resort', u'and', u'are', u'on', u'a', u'bridge', u'high', u'above', u'a', u'very', u'large', u'river', u',', u'something', u'like', u'the', u'Largo', u'River', u'Bridge', u'in', u'my', u'hometown', u'.', u'I', u'wonder', u'how', u'deep', u'the', u'water', u'is', u'at', u'that', u'point', u'.', u'We', u'assume', u'it', u'was', u'deep', u',', u'and', u'we', u'all', u'dive', u'off', u'.', u'When', u'we', u'come', u'up', u',', u'we', u'start', u'swimming', u'back', u'to', u'the', u'resort', u'.', u'When', u'we', u'get', u'there', u',', u'it', u'is', u'time', u'to', u'go', u'home', u'.', u'After', u'I', u'begin', u'to', u'leave', u',', u'I', u'realize', u'that', u'I', u'have', u'left', u'a', u'towel', u'or', u'something', u'behind', u',', u'and', u'I', u'go', u'back', u'.', u'I', u'have', u'to', u'go', u'up', u'some', u'stairs', u',', u'and', u'when', u'I', u'come', u'down', u',', u'I', u'see', u'Doug', u'Sterme', u'.', u'He', u'introduces', u'to', u'me', u'to', u'his', u'son', u',', u'``', u'Iran', u',', u\"''\", u'and', u'to', u'his', u'wife', u',', u'whose', u'name', u'I', u'ca', u\"n't\", u'remember', u',', u'but', u'she', u'is', u'a', u'small', u'Japanese', u'woman', u'with', u'a', u'round', u'face', u'.', u'I', u'am', u'surprised', u'that', u'his', u'wife', u'is', u'Japanese', u'.']\n",
      "noun only: [u'kind', u'resort', u'people', u'girls', u'guy', u'students', u'resort', u'bridge', u'river', u'something', u'Largo', u'River', u'Bridge', u'hometown', u'water', u'point', u'resort', u'time', u'home', u'towel', u'something', u'stairs', u'Sterme', u'son', u'Iran', u'wife', u'name', u'woman', u'round', u'face', u'wife']\n",
      "lemmatized: [u'kind', u'resort', u'people', u'girl', u'guy', u'student', u'resort', u'bridge', u'river', u'something', u'Largo', u'River', u'Bridge', u'hometown', u'water', u'point', u'resort', u'time', u'home', u'towel', u'something', u'stair', u'Sterme', u'son', u'Iran', u'wife', u'name', u'woman', u'round', u'face', u'wife']\n",
      "Counter({u'resort': 3, u'something': 2, u'wife': 2, u'bridge': 1, u'people': 1, u'hometown': 1, u'son': 1, u'home': 1, u'girl': 1, u'Iran': 1, u'stair': 1, u'point': 1, u'woman': 1, u'water': 1, u'student': 1, u'Largo': 1, u'River': 1, u'Bridge': 1, u'kind': 1, u'towel': 1, u'name': 1, u'face': 1, u'Sterme': 1, u'time': 1, u'guy': 1, u'river': 1, u'round': 1})\n",
      "\n",
      "\n",
      "The Physiologist (86 dreams) [M]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'dreamed', u'that', u'I', u'was', u'standing', u'on', u'a', u'small', u'shed', u'and', u'that', u'a', u'mad', u'bull', u'was', u'trying', u'to', u'get', u'at', u'me', u'.', u'He', u'circled', u'round', u'the', u'building', u'and', u'made', u'wonderful', u'leaps', u'to', u'gain', u'the', u'roof', u'.', u'At', u'each', u'plunge', u'he', u'would', u'get', u'his', u'fore-hoofs', u'upon', u'the', u'shingles', u',', u'``', u'scrabble', u\"''\", u'furiously', u',', u'and', u'fall', u'back', u'.', u'I', u'had', u'a', u'pair', u'of', u'lawn', u'shears', u'with', u'which', u'I', u'prodded', u'his', u'head', u'.']\n",
      "noun only: [u'shed', u'bull', u'building', u'leaps', u'roof', u'plunge', u'shingles', u'fall', u'pair', u'lawn', u'shears', u'head']\n",
      "lemmatized: [u'shed', u'bull', u'building', u'leap', u'roof', u'plunge', u'shingle', u'fall', u'pair', u'lawn', u'shear', u'head']\n",
      "Counter({u'building': 1, u'shed': 1, u'plunge': 1, u'shingle': 1, u'fall': 1, u'lawn': 1, u'roof': 1, u'leap': 1, u'bull': 1, u'pair': 1, u'head': 1, u'shear': 1})\n",
      "\n",
      "\n",
      "Ringo: from the 1960s (16 dreams) [M]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'My', u'little', u'brother', u'chased', u'me', u'out', u'of', u'the', u'house', u'one', u'day', u'with', u'marshmallows', u'.', u'He', u'was', u'throwing', u'them', u'at', u'me', u'.', u'I', u'decided', u'not', u'to', u'throw', u'any', u'back', u'.', u'I', u'ran', u'to', u'my', u'car', u'and', u'took', u'off', u'.', u'I', u'burned', u'my', u'tires', u'going', u'out', u'he', u'driveway', u',', u'making', u'a', u'lot', u'of', u'noise', u'.', u'There', u'was', u'a', u'police', u'car', u'behind', u'me', u'.', u'He', u'stopped', u'me', u'and', u'gave', u'me', u'four', u'tickets', u'.', u'This', u'made', u'me', u'mad', u'because', u'had', u'had', u'them', u'written', u'up', u'before', u'he', u'got', u'out', u'of', u'the', u'car', u'.', u'The', u'left', u'front', u'door', u'was', u'open', u'and', u'the', u'cop', u'was', u'standing', u'about', u'4-10', u'inches', u'from', u'the', u'car', u'.', u'After', u'he', u'gave', u'me', u'the', u'tickets', u',', u'I', u'slapped', u'it', u'in', u'reverse', u'and', u'took', u'off', u'.', u'This', u'knocked', u'him', u'over', u',', u'but', u'I', u'was', u'still', u'mad', u'about', u'the', u'tickets', u'.', u'I', u'went', u'over', u'to', u'school', u',', u'but', u'just', u'getting', u'to', u'school', u'I', u'stopped', u'and', u'looked', u'down', u'a', u'hold', u'in', u'the', u'ground', u'8', u\"'\", u'diameter', u'and', u'12', u\"'\", u'deep', u'.', u'It', u'had', u'like', u'cheese', u'cloth', u'over', u'the', u'top', u'and', u'music', u'and', u'news', u'were', u'emitting', u'from', u'somewhere', u'in', u'the', u'hole', u'.', u'I', u'have', u'seen', u'this', u'hole', u'many', u'times', u'in', u'other', u'dreams', u'.', u'After', u'that', u'I', u'met', u'two', u'friends', u'in', u'the', u'parking', u'lot', u'and', u'was', u'showing', u'them', u'the', u'tickets', u',', u'but', u'got', u'so', u'mad', u'about', u'them', u',', u'I', u'woke', u'up', u'.']\n",
      "noun only: [u'brother', u'house', u'day', u'marshmallows', u'car', u'tires', u'lot', u'noise', u'police', u'car', u'tickets', u'car', u'door', u'cop', u'inches', u'car', u'tickets', u'reverse', u'tickets', u'school', u'school', u'hold', u'ground', u'diameter', u'deep', u'cloth', u'music', u'news', u'hole', u'hole', u'times', u'dreams', u'friends', u'parking', u'lot', u'tickets']\n",
      "lemmatized: [u'brother', u'house', u'day', u'marshmallow', u'car', u'tire', u'lot', u'noise', u'police', u'car', u'ticket', u'car', u'door', u'cop', u'inch', u'car', u'ticket', u'reverse', u'ticket', u'school', u'school', u'hold', u'ground', u'diameter', u'deep', u'cloth', u'music', u'news', u'hole', u'hole', u'time', u'dream', u'friend', u'parking', u'lot', u'ticket']\n",
      "Counter({u'ticket': 4, u'car': 4, u'hole': 2, u'lot': 2, u'school': 2, u'diameter': 1, u'marshmallow': 1, u'house': 1, u'deep': 1, u'parking': 1, u'ground': 1, u'police': 1, u'tire': 1, u'music': 1, u'inch': 1, u'friend': 1, u'noise': 1, u'door': 1, u'cloth': 1, u'news': 1, u'hold': 1, u'day': 1, u'cop': 1, u'reverse': 1, u'brother': 1, u'time': 1, u'dream': 1})\n",
      "\n",
      "\n",
      "Samantha: in her 20s (63 dreams) [F]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'Feels', u'like', u'the', u'same', u'ol', u\"'\", u'chicken', u'soup', u'dream', u'yet', u'I', u'do', u\"n't\", u'know', u'if', u'I', u\"'ve\", u'ever', u'dreamt', u'about', u'this', u'soup', u'before', u'.', u'Crush', u'was', u'playing', u'.', u'My', u'dad', u'hired', u'them', u'I', u'think', u'.', u'They', u'played', u'in', u'this', u'church/bar', u'parking', u'lot', u'.', u'Nobody', u'came', u'.', u'I', u'was', u'pissed', u'cause', u'they', u'would', u'take', u'long', u'1/2', u'hour', u'breaks', u'or', u'longer', u'between', u'each', u'song', u'.', u'I', u'tried', u'to', u'tell', u'them', u'they', u'sucked', u'but', u'would', u'then', u'just', u'apologize', u'for', u'what', u'I', u'said', u'.', u'In', u'the', u'parking', u'lot', u',', u'2', u'other', u'bands', u'were', u'practicing', u'quietly', u'.', u'One', u'band', u'wore', u'all', u'cavemen', u'outfits', u'and', u'they', u'were', u'set', u'up', u'like', u'Hollywood', u'Squares', u'.', u'I', u'wanted', u'to', u'go', u'camping', u'.', u'I', u'had', u'this', u'favorite', u'trail', u'that', u'I', u'started', u'doing', u'over', u'and', u'over', u'again', u'.', u'It', u'almost', u'became', u'like', u'a', u'personal', u'race', u'course', u'.', u'There', u'was', u'one', u'area', u'where', u'the', u'trail', u'split', u'.', u'The', u'one', u'way', u'I', u'never', u'went', u',', u'went', u'across', u'a', u'river', u'.', u'There', u'was', u'a', u'lean-to', u'.', u'The', u'lean-to', u',', u'as', u'I', u'eventually', u'found', u'out', u'one', u'day', u'was', u'a', u'loft', u'with', u'a', u'nice', u'double', u'bed', u',', u'a', u'closet', u',', u'lights', u'-', u'just', u'like', u'a', u'regular', u'bedroom', u'.', u'It', u'had', u'once', u'belonged', u'to', u'the', u'woman', u'who', u'owned', u'14', u'K.', u'Street', u'.', u'A', u'lot', u'of', u'her', u'stuff', u'was', u'still', u'in', u'there', u'-', u'clothes', u',', u'books', u',', u'curtains', u',', u'photo', u'albums', u'.', u'There', u'was', u'also', u'my', u'moms', u'storage', u'stuff', u'like', u'a', u'sewing', u'machine', u'and', u'thread', u'.', u'I', u'looked', u'throughout', u'the', u'album', u'and', u'saw', u'a', u'picture', u'of', u'the', u'lady', u'as', u'a', u'little', u'girl', u'.', u'She', u'looked', u'exactly', u'like', u'me', u'.', u'A', u'little', u'boy', u'looked', u'just', u'like', u'Eric', u'.', u'In', u'another', u'picture', u'the', u'father', u'looked', u'like', u'Roger', u'.', u'I', u'ripped', u'a', u'picture', u'out', u'.', u'When', u'I', u'got', u'back', u'from', u'hiking', u',', u'the', u'band', u'was', u'still', u'on', u'break', u'-', u'I', u\"'m\", u'pissed', u'.', u'I', u'tell', u'everyone', u'that', u'we', u'should', u'all', u'go', u'camping', u'and', u'stay', u'in', u'the', u'lean-to', u'and', u'that', u'I', u\"'ll\", u'make', u'some', u'French', u'Onion', u'chicken', u'soup', u'.', u'Ronny', u'tells', u'everyone', u'that', u'my', u'soup', u'sucks', u'.', u'Ahmad', u'asks', u'him', u'why', u'he', u'does', u\"n't\", u'make', u'it', u'then', u'and', u'Ronny', u'says', u'because', u'he', u'hates', u'it', u'but', u'if', u'Ahmad', u'gets', u'him', u'30', u'lbs', u'of', u'dog', u'meat', u'then', u'he', u\"'ll\", u'cook', u'it', u'for', u'us', u'the', u'right', u'way', u'.', u'I', u'show', u'my', u'mother', u'the', u'picture', u'and', u'she', u'is', u'surprised', u'.', u'Sam', u'and', u'Kim', u'said', u'that', u'they', u'would', u'go', u'camping', u'but', u'then', u'Kim', u'backs', u'out', u'.', u'So', u'Sam', u'asks', u'if', u'just', u'he', u'and', u'I', u'would', u'want', u'to', u'go', u'.', u'Hmmmm', u',', u'i', u'think', u',', u'that', u'would', u'be', u'fun', u'.', u'He', u'wants', u'to', u'climb', u'a', u'mountain', u'though', u',', u'and', u'not', u'go', u'on', u'my', u'special', u'trail', u'(', u'which', u'is', u'basically', u'flat', u')', u'with', u'the', u'lean-to', u'.', u'One', u'thing', u'i', u'forgot', u'...', u'.when', u'I', u'was', u'going', u'down', u'hills', u'on', u'the', u'trail', u'it', u'almost', u'had', u'the', u'affect', u'of', u'skydiving', u'.', u'I', u'felt', u'like', u'my', u'hands', u'were', u'holding', u'onto', u'two', u'bannisters', u'and', u'I', u'would', u'slide', u'down', u'the', u'slope', u'from', u'he', u'sky', u'.', u'Sometimes', u'the', u'geography', u'would', u'turn', u'into', u'a', u'map-like', u'feature', u'.', u'I', u'could', u'see', u'small', u'trees', u'and', u'the', u'outline', u'of', u'the', u'path', u'and', u'I', u'would', u'try', u'to', u'aim', u'for', u'that', u'.', u'The', u'size', u'of', u'the', u'tree', u'tops', u'were', u'about', u'the', u'size', u'of', u'quarters', u'.', u'[', u'Crush', u'=', u'college', u'band', u'with', u'good', u'friends', u';', u'Eric', u'=', u'my', u'brother', u';', u'K.', u'Street', u'=', u'street', u'my', u'parents', u'live', u'on', u';', u'Roger', u'=', u'mother', u'and', u'fathers', u'friend', u';', u'Ronny', u'=', u'friend', u';', u'Ahmad', u'=', u'love', u'interest', u'at', u'the', u'time', u';', u'Sam', u'=', u'friend', u',', u'goes', u'out', u'with', u'Kim', u';', u'Kim', u'=', u'friend', u',', u'goes', u'out', u'with', u'Sam', u']']\n",
      "noun only: [u'Feels', u'ol', u'soup', u'dream', u'soup', u'Crush', u'dad', u'church/bar', u'parking', u'lot', u'Nobody', u'cause', u'hour', u'breaks', u'song', u'parking', u'lot', u'bands', u'band', u'cavemen', u'outfits', u'Hollywood', u'Squares', u'trail', u'race', u'course', u'area', u'trail', u'split', u'way', u'river', u'day', u'bed', u'closet', u'bedroom', u'woman', u'K.', u'Street', u'lot', u'stuff', u'clothes', u'books', u'curtains', u'photo', u'albums', u'moms', u'stuff', u'sewing', u'machine', u'thread', u'album', u'picture', u'lady', u'girl', u'boy', u'Eric', u'picture', u'father', u'Roger', u'picture', u'hiking', u'band', u'everyone', u'camping', u'Onion', u'chicken', u'soup', u'Ronny', u'everyone', u'soup', u'sucks', u'Ahmad', u'Ronny', u'Ahmad', u'lbs', u'dog', u'meat', u'way', u'mother', u'picture', u'Sam', u'Kim', u'Kim', u'Sam', u'Hmmmm', u'i', u'mountain', u'trail', u'thing', u'i', u'.when', u'hills', u'trail', u'affect', u'hands', u'bannisters', u'slope', u'geography', u'feature', u'trees', u'outline', u'path', u'size', u'tree', u'tops', u'size', u'quarters', u'Crush', u'=', u'college', u'band', u'friends', u'Eric', u'=', u'brother', u'K.', u'Street', u'=', u'street', u'parents', u'Roger', u'=', u'mother', u'fathers', u'Ronny', u'=', u'friend', u'Ahmad', u'=', u'interest', u'time', u'Sam', u'=', u'friend', u'Kim', u'Kim', u'=', u'friend', u'Sam', u']']\n",
      "lemmatized: [u'Feels', u'ol', u'soup', u'dream', u'soup', u'Crush', u'dad', u'church/bar', u'parking', u'lot', u'Nobody', u'cause', u'hour', u'break', u'song', u'parking', u'lot', u'band', u'band', u'caveman', u'outfit', u'Hollywood', u'Squares', u'trail', u'race', u'course', u'area', u'trail', u'split', u'way', u'river', u'day', u'bed', u'closet', u'bedroom', u'woman', u'K.', u'Street', u'lot', u'stuff', u'clothes', u'book', u'curtain', u'photo', u'album', u'mom', u'stuff', u'sewing', u'machine', u'thread', u'album', u'picture', u'lady', u'girl', u'boy', u'Eric', u'picture', u'father', u'Roger', u'picture', u'hiking', u'band', u'everyone', u'camping', u'Onion', u'chicken', u'soup', u'Ronny', u'everyone', u'soup', u'suck', u'Ahmad', u'Ronny', u'Ahmad', u'lb', u'dog', u'meat', u'way', u'mother', u'picture', u'Sam', u'Kim', u'Kim', u'Sam', u'Hmmmm', u'i', u'mountain', u'trail', u'thing', u'i', u'.when', u'hill', u'trail', u'affect', u'hand', u'bannister', u'slope', u'geography', u'feature', u'tree', u'outline', u'path', u'size', u'tree', u'top', u'size', u'quarter', u'Crush', u'=', u'college', u'band', u'friend', u'Eric', u'=', u'brother', u'K.', u'Street', u'=', u'street', u'parent', u'Roger', u'=', u'mother', u'father', u'Ronny', u'=', u'friend', u'Ahmad', u'=', u'interest', u'time', u'Sam', u'=', u'friend', u'Kim', u'Kim', u'=', u'friend', u'Sam', u']']\n",
      "Counter({u'=': 8, u'band': 4, u'Sam': 4, u'friend': 4, u'trail': 4, u'soup': 4, u'Kim': 4, u'picture': 4, u'Ahmad': 3, u'Ronny': 3, u'lot': 3, u'father': 2, u'everyone': 2, u'parking': 2, u'album': 2, u'stuff': 2, u'Eric': 2, u'K.': 2, u'size': 2, u'way': 2, u'Roger': 2, u'tree': 2, u'Crush': 2, u'i': 2, u'Street': 2, u'mother': 2, u'slope': 1, u'photo': 1, u'course': 1, u'bedroom': 1, u'chicken': 1, u'geography': 1, u'lb': 1, u'dad': 1, u'woman': 1, u'song': 1, u'break': 1, u'mom': 1, u'Hmmmm': 1, u'lady': 1, u'day': 1, u'meat': 1, u'brother': 1, u'race': 1, u'affect': 1, u'quarter': 1, u'clothes': 1, u'bannister': 1, u'street': 1, u'college': 1, u'girl': 1, u'closet': 1, u'caveman': 1, u'Nobody': 1, u'hill': 1, u'cause': 1, u'parent': 1, u'outfit': 1, u'path': 1, u'boy': 1, u'ol': 1, u'thread': 1, u'thing': 1, u'hiking': 1, u'river': 1, u'Hollywood': 1, u'interest': 1, u'area': 1, u'top': 1, u'church/bar': 1, u'Feels': 1, u'time': 1, u'curtain': 1, u'Onion': 1, u'hour': 1, u'bed': 1, u'Squares': 1, u'mountain': 1, u'feature': 1, u'machine': 1, u'book': 1, u'split': 1, u'.when': 1, u'sewing': 1, u'hand': 1, u'suck': 1, u']': 1, u'camping': 1, u'outline': 1, u'dog': 1, u'dream': 1})\n",
      "\n",
      "\n",
      "Seventh grade girls (69 dreams) [F]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'Last', u'week', u'I', u'had', u'a', u'weird', u'dream', u'about', u'being', u'a', u'butterfly', u'and', u'chasing', u'a', u'kite', u'that', u'also', u'looked', u'like', u'a', u'butterfly', u'.', u'It', u'was', u'super', u'strange', u'because', u'I', u'kept', u'on', u'waking', u'up', u',', u'then', u'going', u'back', u'to', u'sleep', u'and', u'the', u'dream', u'kept', u'on', u'coming', u'in', u'where', u'it', u'left', u'off', u'.', u'It', u'was', u\"n't\", u'scary', u',', u'it', u'was', u'just', u'strange', u'.', u'It', u'finally', u'stopped', u'when', u'the', u'kite', u'ripped', u'in', u'half', u'and', u'started', u'to', u'rain', u'.', u'The', u'butterfly', u'flew', u'away', u'home', u'sad', u'.']\n",
      "noun only: [u'week', u'dream', u'butterfly', u'kite', u'butterfly', u'strange', u'dream', u'kite', u'half', u'butterfly', u'home', u'sad']\n",
      "lemmatized: [u'week', u'dream', u'butterfly', u'kite', u'butterfly', u'strange', u'dream', u'kite', u'half', u'butterfly', u'home', u'sad']\n",
      "Counter({u'butterfly': 3, u'kite': 2, u'dream': 2, u'week': 1, u'sad': 1, u'strange': 1, u'half': 1, u'home': 1})\n",
      "\n",
      "\n",
      "Midwest teenagers (F) (111 dreams) [F]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'All', u'I', u'remember', u'was', u'being', u'with', u'my', u'two', u'friends', u'and', u'I', u'was', u'eating', u'something', u'in', u'my', u'dream', u'and', u'my', u'teeth', u'fell', u'out', u',', u'but', u'they', u'were', u'like', u'still', u'there', u'.', u'It', u'was', u'like', u'the', u'end/bottom', u'of', u'my', u'teeth', u'fell', u'off', u'.', u'I', u'remembered', u'because', u'I', u'was', u'eating', u'in', u'the', u'car', u'with', u'my', u'two', u'friends', u'that', u'were', u'in', u'the', u'dream', u'and', u'it', u'just', u'popped', u'into', u'my', u'head', u'.']\n",
      "noun only: [u'friends', u'something', u'dream', u'teeth', u'end/bottom', u'teeth', u'car', u'friends', u'dream', u'head']\n",
      "lemmatized: [u'friend', u'something', u'dream', u'teeth', u'end/bottom', u'teeth', u'car', u'friend', u'dream', u'head']\n",
      "Counter({u'teeth': 2, u'dream': 2, u'friend': 2, u'head': 1, u'car': 1, u'end/bottom': 1, u'something': 1})\n",
      "\n",
      "\n",
      "Midwest teenagers (M) (83 dreams) [M]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'was', u'an', u'ant', u'.', u'The', u'queen', u'had', u'just', u'given', u'me', u'my', u'mission', u'.', u'I', u'was', u'to', u'collect', u'the', u'great', u'bounty', u'of', u'food', u'left', u'over', u'by', u'the', u'giants', u'.', u'When', u'I', u'reached', u'the', u'destination', u',', u'a', u'picnic', u'table', u',', u'I', u'saw', u'on', u'it', u'a', u'gleaming', u'piece', u'of', u'chocolate', u'with', u'crispy', u'rice', u'in', u'it', u'wrapped', u'with', u'silver', u'foil', u'and', u'a', u'blue', u'cover', u'.', u'As', u'I', u'went', u'to', u'collect', u'this', u'beauty', u',', u'I', u'noticed', u'one', u'of', u'the', u'smaller', u'giants', u'coming', u'after', u'me', u'.', u'He', u'was', u'shouting', u',', u'``', u'poopy', u'bug', u',', u'poopy', u'bug', u'.', u\"''\", u'I', u'tried', u'to', u'turn', u',', u'but', u'it', u'was', u'too', u'late', u'.', u'His', u'batman', u'shoe', u'cast', u'a', u'shadow', u'over', u'me', u'and', u'that', u'was', u'it', u'.']\n",
      "noun only: [u'ant', u'queen', u'mission', u'bounty', u'food', u'giants', u'destination', u'picnic', u'table', u'piece', u'chocolate', u'rice', u'silver', u'foil', u'cover', u'beauty', u'giants', u'poopy', u'bug', u'poopy', u'bug', u'batman', u'shadow']\n",
      "lemmatized: [u'ant', u'queen', u'mission', u'bounty', u'food', u'giant', u'destination', u'picnic', u'table', u'piece', u'chocolate', u'rice', u'silver', u'foil', u'cover', u'beauty', u'giant', u'poopy', u'bug', u'poopy', u'bug', u'batman', u'shadow']\n",
      "Counter({u'giant': 2, u'poopy': 2, u'bug': 2, u'destination': 1, u'picnic': 1, u'beauty': 1, u'food': 1, u'batman': 1, u'queen': 1, u'cover': 1, u'mission': 1, u'chocolate': 1, u'ant': 1, u'rice': 1, u'foil': 1, u'bounty': 1, u'table': 1, u'shadow': 1, u'piece': 1, u'silver': 1})\n",
      "\n",
      "\n",
      "West Coast teenage girls (89 dreams) [F]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'My', u'dream', u'was', u'about', u',', u'that', u'I', u'was', u'walking', u'home', u'one', u'day', u'and', u'this', u'boy', u'comes', u'out', u'of', u'nowhere', u'and', u'tells', u'me', u'to', u'be', u'careful', u'but', u'I', u'did', u\"n't\", u'listen', u'to', u'him', u'.', u'<', u'BR', u'>', u'&', u'nbsp', u';', u'&', u'nbsp', u';', u'&', u'nbsp', u';', u'&', u'nbsp', u';', u'&', u'nbsp', u';', u'&', u'nbsp', u';', u'But', u'then', u'all', u'of', u'a', u'sudden', u'it', u'starts', u'to', u'get', u'dark', u',', u'so', u'then', u'I', u'had', u'seen', u'an', u'alley', u'.', u'It', u\"'s\", u'a', u'shortcut', u'to', u'my', u'house', u',', u'I', u'thought', u'.', u'<', u'BR', u'>', u'&', u'nbsp', u';', u'&', u'nbsp', u';', u'&', u'nbsp', u';', u'&', u'nbsp', u';', u'&', u'nbsp', u';', u'&', u'nbsp', u';', u'But', u'then', u'all', u'of', u'a', u'sudden', u'a', u'man', u'was', u'following', u'me', u',', u'so', u'then', u'I', u'ran', u'faster', u'and', u'as', u'soon', u'as', u'I', u'got', u'home', u',', u'he', u'had', u'pulled', u'a', u'gun', u'on', u'me', u'and', u'shot', u'me', u'.']\n",
      "noun only: [u'dream', u'day', u'boy', u'BR', u'>', u'nbsp', u'nbsp', u'nbsp', u'nbsp', u'nbsp', u'nbsp', u'alley', u'shortcut', u'house', u'BR', u'>', u'nbsp', u'nbsp', u'nbsp', u'nbsp', u'nbsp', u'nbsp', u'man', u'gun']\n",
      "lemmatized: [u'dream', u'day', u'boy', u'BR', u'>', u'nbsp', u'nbsp', u'nbsp', u'nbsp', u'nbsp', u'nbsp', u'alley', u'shortcut', u'house', u'BR', u'>', u'nbsp', u'nbsp', u'nbsp', u'nbsp', u'nbsp', u'nbsp', u'man', u'gun']\n",
      "Counter({u'nbsp': 12, u'BR': 2, u'>': 2, u'boy': 1, u'house': 1, u'gun': 1, u'alley': 1, u'man': 1, u'shortcut': 1, u'dream': 1, u'day': 1})\n",
      "\n",
      "\n",
      "Toby: A friendly party animal (33 dreams) [M]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u\"'m\", u'in', u'a', u'strange', u'restaurant', u'on', u'some', u'tall', u'building', u'and', u'it', u\"'s\", u'raining', u'outside', u'.', u'I', u\"'m\", u'with', u'a', u'lot', u'of', u'people', u'who', u'I', u'know', u',', u'but', u'now', u'I', u'do', u\"n't\", u'think', u'I', u'did', u'know', u'them', u'.', u'I', u'have', u'to', u'use', u'the', u'bathroom', u',', u'so', u'I', u'leave', u'the', u'table', u'and', u'walk', u'through', u'the', u'kitchen', u'up', u'some', u'stairs', u'and', u'am', u'faced', u'with', u'a', u'bunch', u'of', u'doors', u'.', u'I', u'open', u'the', u'one', u'in', u'front', u'of', u'me', u'and', u'now', u'I', u\"'m\", u'in', u'some', u'strange', u'girl', u\"'s\", u'apartment', u'.', u'She', u\"'s\", u'in', u'a', u'towel', u'and', u'very', u'embarrassed', u'and', u'tells', u'me', u'to', u'get', u'out', u'.', u'Before', u'I', u'know', u'it', u',', u'I', u\"'m\", u'outside', u'of', u'her', u'room', u'and', u'faced', u'with', u'the', u'doors', u'again', u'.', u'This', u'time', u'I', u'choose', u'a', u'different', u'one', u'and', u'now', u'I', u\"'m\", u'in', u'a', u'long', u'hallway', u'.', u'On', u'the', u'walls', u'are', u'posters', u'on', u'naked', u'chicks', u'and', u'I', u'can', u'hear', u'people', u'having', u'sex', u'.', u'It', u\"'s\", u'so', u'loud', u'that', u'I', u\"'m\", u'sprinting', u'down', u'the', u'hallway', u'.', u'I', u'never', u'reached', u'the', u'end', u',', u'but', u'I', u'turn', u'to', u'the', u'right', u',', u'open', u'a', u'door', u'and', u'the', u'noises', u'in', u'my', u'head', u'stop', u'.', u'I', u'suddenly', u'remember', u'that', u'I', u'have', u'to', u'go', u'to', u'the', u'bathroom', u'and', u'as', u'I', u'enter', u'this', u'new', u'room', u',', u'I', u'see', u'arcade', u'machines', u',', u'basketball', u'hoops', u',', u'and', u'skee', u'ball', u'.', u'It', u'looks', u'like', u'the', u'Boardwalk', u'Super', u'Arcade', u'and', u',', u'sure', u'enough', u',', u'as', u'I', u'move', u'deeper', u'inside', u',', u'I', u'see', u'that', u'weird', u'game', u'with', u'the', u'fake', u'rifles', u'that', u'you', u'can', u'shoot', u'stuff', u'with', u'and', u'it', u\"'ll\", u'do', u'something', u'.', u'I', u'ask', u'the', u'guy', u'if', u'he', u'can', u'tell', u'me', u'where', u'the', u'bathroom', u'is', u'and', u'I', u'do', u\"n't\", u'remember', u'what', u'he', u'says', u'.', u'I', u'suddenly', u'get', u'a', u'really', u'weird', u'feeling', u'like', u',', u'``', u'I', u'need', u'to', u'get', u'out', u'of', u'here', u'!', u\"''\", u'I', u'turn', u'to', u'run', u',', u'but', u'I', u'trip', u'.', u'I', u'start', u'spinning', u'and', u'I', u'wake', u'up', u'.']\n",
      "noun only: [u'restaurant', u'building', u'lot', u'people', u'bathroom', u'table', u'walk', u'kitchen', u'stairs', u'bunch', u'doors', u'one', u'front', u'girl', u'apartment', u'towel', u'room', u'doors', u'time', u'hallway', u'walls', u'posters', u'chicks', u'people', u'sex', u'hallway', u'end', u'right', u'door', u'noises', u'head', u'stop', u'bathroom', u'room', u'machines', u'basketball', u'hoops', u'ball', u'Boardwalk', u'Super', u'Arcade', u'game', u'fake', u'stuff', u'something', u'guy', u'bathroom', u'feeling', u'trip']\n",
      "lemmatized: [u'restaurant', u'building', u'lot', u'people', u'bathroom', u'table', u'walk', u'kitchen', u'stair', u'bunch', u'door', u'one', u'front', u'girl', u'apartment', u'towel', u'room', u'door', u'time', u'hallway', u'wall', u'poster', u'chick', u'people', u'sex', u'hallway', u'end', u'right', u'door', u'noise', u'head', u'stop', u'bathroom', u'room', u'machine', u'basketball', u'hoop', u'ball', u'Boardwalk', u'Super', u'Arcade', u'game', u'fake', u'stuff', u'something', u'guy', u'bathroom', u'feeling', u'trip']\n",
      "Counter({u'bathroom': 3, u'door': 3, u'people': 2, u'hallway': 2, u'room': 2, u'right': 1, u'Arcade': 1, u'wall': 1, u'hoop': 1, u'one': 1, u'ball': 1, u'something': 1, u'fake': 1, u'table': 1, u'girl': 1, u'trip': 1, u'bunch': 1, u'apartment': 1, u'end': 1, u'stair': 1, u'machine': 1, u'lot': 1, u'Super': 1, u'head': 1, u'noise': 1, u'poster': 1, u'stop': 1, u'game': 1, u'basketball': 1, u'front': 1, u'walk': 1, u'kitchen': 1, u'building': 1, u'towel': 1, u'restaurant': 1, u'sex': 1, u'Boardwalk': 1, u'stuff': 1, u'time': 1, u'chick': 1, u'guy': 1, u'feeling': 1})\n",
      "\n",
      "\n",
      "Tom: An outgoing man (27 dreams) [M]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'am', u'in', u'an', u'unfamiliar', u'classroom', u'and', u'I', u'seem', u'to', u'be', u'taking', u'care', u'of', u'or', u'supervising', u'a', u'boy', u'about', u'the', u'age', u'of', u'five', u'.', u'He', u'has', u'blond', u'hair', u'and', u'seems', u'very', u'average', u'in', u'appearance', u'.', u'He', u'is', u'playing', u'quietly', u'by', u'himself', u'and', u'I', u'really', u'have', u'no', u'contact', u'with', u'him', u'at', u'all', u'.', u'There', u'are', u'many', u'toys', u'strewn', u'about', u'the', u'room', u'and', u'everything', u'is', u'multicolored', u'and', u'bright', u',', u'like', u'a', u'kindergarten', u'classroom', u'might', u'be', u'but', u'more', u'vivid', u'.', u'The', u'room', u'is', u'full', u'of', u'other', u'people', u'including', u'other', u'adults', u'and', u'children', u',', u'it', u'is', u'a', u'busy', u'atmosphere', u'with', u'movement', u'everywhere', u'.', u'Nevertheless', u',', u'my', u'attention', u'is', u'not', u'where', u'it', u'might', u'be', u'expected', u'.', u'I', u'am', u'sitting', u'on', u'the', u'floor', u'by', u'the', u'boy', u'and', u'near', u'me', u'are', u'two', u'very', u'good-looking', u',', u'dark-skinned', u'girls', u'.', u'They', u'tell', u'me', u'that', u'they', u'are', u'both', u'17', u'years', u'old', u'and', u'that', u'they', u'also', u'both', u'have', u'the', u'same', u'boyfriend', u'.', u'At', u'the', u'exact', u'same', u'time', u'all', u'three', u'of', u'us', u'say', u',', u'``', u'Lucky', u'guy', u\"''\", u'(', u'meaning', u'the', u'boyfriend', u'is', u'lucky', u')', u'.', u'I', u'remember', u'being', u'more', u'interested', u'in', u'the', u'two', u'girls', u'than', u'the', u'boy', u'I', u'am', u'supposed', u'to', u'watch', u'.', u'The', u'dream', u'seems', u'to', u'end', u'with', u'me', u'talking', u'to', u'the', u'two', u'girls', u'.']\n",
      "noun only: [u'classroom', u'care', u'boy', u'age', u'hair', u'appearance', u'contact', u'toys', u'room', u'everything', u'classroom', u'vivid', u'room', u'people', u'adults', u'children', u'atmosphere', u'movement', u'attention', u'floor', u'boy', u'girls', u'years', u'boyfriend', u'time', u'guy', u'boyfriend', u'girls', u'boy', u'dream', u'girls']\n",
      "lemmatized: [u'classroom', u'care', u'boy', u'age', u'hair', u'appearance', u'contact', u'toy', u'room', u'everything', u'classroom', u'vivid', u'room', u'people', u'adult', u'child', u'atmosphere', u'movement', u'attention', u'floor', u'boy', u'girl', u'year', u'boyfriend', u'time', u'guy', u'boyfriend', u'girl', u'boy', u'dream', u'girl']\n",
      "Counter({u'girl': 3, u'boy': 3, u'boyfriend': 2, u'classroom': 2, u'room': 2, u'atmosphere': 1, u'people': 1, u'hair': 1, u'year': 1, u'toy': 1, u'vivid': 1, u'floor': 1, u'everything': 1, u'care': 1, u'movement': 1, u'attention': 1, u'adult': 1, u'child': 1, u'age': 1, u'appearance': 1, u'contact': 1, u'time': 1, u'guy': 1, u'dream': 1})\n",
      "\n",
      "\n",
      "UCSC women, 1996 (81 dreams) [F]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u'had', u'this', u'dream', u'at', u'the', u'beginning', u'of', u'the', u'school', u'year', u',', u'which', u'is', u'my', u'first', u'year', u'here', u'at', u'UC', u'Santa', u'Cruz', u'.', u'In', u'my', u'dream', u'I', u\"'m\", u'leaving', u'a', u'party', u'or', u'gathering', u'of', u'some', u'type', u'that', u'was', u'being', u'held', u'at', u'the', u'Stevenson', u'dining', u'hall', u'.', u'I', u'told', u'my', u'friends', u',', u'Clara', u'and', u'Linda', u',', u'who', u'I', u'recently', u'met', u'at', u'the', u'time', u'that', u'I', u'was', u'going', u'home', u'to', u'get', u'something', u'that', u'I', u'had', u'forgotten', u'.', u'On', u'my', u'way', u'to', u'the', u'door', u',', u'I', u'ran', u'into', u'the', u'Stevenson', u'College', u'provost', u'who', u'asked', u'me', u'where', u'I', u'was', u'running', u'off', u'to', u'.', u'I', u'told', u'him', u'I', u\"'d\", u'be', u'back', u'.', u'I', u'remember', u'him', u'handing', u'me', u'a', u'banana', u'.', u'On', u'my', u'way', u'home', u',', u'I', u'was', u'walking', u'through', u'a', u'forest', u',', u'well', u'not', u'a', u'forest', u',', u'but', u'I', u'was', u'walking', u'through', u'some', u'trees', u'here', u'at', u'UCSC', u'.', u'At', u'one', u'point', u'I', u'was', u'walking', u'at', u'a', u'very', u'narrow', u'side', u'walk', u'with', u'nothing', u'on', u'each', u'side', u'of', u'me', u',', u'but', u'tall', u'trees', u'.', u'I', u'looked', u'to', u'my', u'right', u'and', u'there', u'was', u'a', u'huge', u'snake', u'.', u'I', u'was', u'very', u',', u'very', u'afraid', u'and', u'I', u'jumped', u'back', u'to', u'my', u'left', u'and', u'when', u'I', u'did', u'this', u'I', u'looked', u'down', u'and', u'I', u'was', u'stepping', u'in', u'a', u'pile', u'of', u'rattle', u'snakes', u',', u'so', u'I', u'ran', u'home', u'.', u'When', u'I', u'ran', u'home', u'it', u'ended', u'up', u'being', u'my', u'home', u'in', u'LA', u'.', u'And', u'I', u'felt', u'safe', u'there', u'.', u'The', u'last', u'thing', u'I', u'did', u'in', u'my', u'dream', u'was', u'call', u'my', u'new', u'friends', u'at', u'UCSC', u'and', u'tell', u'them', u'I', u'was', u\"n't\", u'going', u'back', u'because', u'of', u'the', u'big', u'snake', u'I', u'saw', u'.']\n",
      "noun only: [u'dream', u'beginning', u'school', u'year', u'year', u'UC', u'Santa', u'Cruz', u'dream', u'party', u'gathering', u'type', u'Stevenson', u'hall', u'friends', u'Clara', u'Linda', u'time', u'home', u'something', u'way', u'door', u'Stevenson', u'College', u'provost', u'banana', u'way', u'home', u'forest', u'forest', u'trees', u'UCSC', u'point', u'side', u'walk', u'nothing', u'side', u'trees', u'right', u'snake', u'left', u'pile', u'snakes', u'home', u'home', u'home', u'LA', u'thing', u'dream', u'friends', u'UCSC', u'snake']\n",
      "lemmatized: [u'dream', u'beginning', u'school', u'year', u'year', u'UC', u'Santa', u'Cruz', u'dream', u'party', u'gathering', u'type', u'Stevenson', u'hall', u'friend', u'Clara', u'Linda', u'time', u'home', u'something', u'way', u'door', u'Stevenson', u'College', u'provost', u'banana', u'way', u'home', u'forest', u'forest', u'tree', u'UCSC', u'point', u'side', u'walk', u'nothing', u'side', u'tree', u'right', u'snake', u'left', u'pile', u'snake', u'home', u'home', u'home', u'LA', u'thing', u'dream', u'friend', u'UCSC', u'snake']\n",
      "Counter({u'home': 5, u'snake': 3, u'dream': 3, u'tree': 2, u'year': 2, u'forest': 2, u'way': 2, u'friend': 2, u'Stevenson': 2, u'UCSC': 2, u'side': 2, u'gathering': 1, u'right': 1, u'point': 1, u'walk': 1, u'provost': 1, u'something': 1, u'Linda': 1, u'LA': 1, u'Cruz': 1, u'pile': 1, u'party': 1, u'type': 1, u'banana': 1, u'door': 1, u'Santa': 1, u'nothing': 1, u'beginning': 1, u'hall': 1, u'school': 1, u'Clara': 1, u'thing': 1, u'College': 1, u'time': 1, u'UC': 1, u'left': 1})\n",
      "\n",
      "\n",
      "Vickie: a 10-year-old girl (35 dreams) [F]\n",
      "  noun-only sample dream: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized: [u'My', u'mom', u'and', u'I', u'were', u'in', u'the', u'grocery', u'store', u'.', u'I', u'went', u'over', u'to', u'the', u'free', u'cookie', u'area', u'.', u'And', u'this', u'guy', u'gave', u'me', u'a', u'cookie', u'.', u'I', u'had', u'seen', u'the', u'cookies', u',', u'and', u'they', u'were', u'pretend', u'grasshoppers', u'.', u'I', u'saw', u'a', u'little', u'spider', u'go', u'by', u'(', u'on', u'the', u'cookie', u')', u'.', u'I', u'said', u',', u'``', u'Oh', u',', u'I', u'do', u\"n't\", u'like', u'this', u'cookie', u'.', u\"''\", u'I', u'thought', u'it', u'was', u'gross', u'.', u'But', u'then', u'all', u'these', u'other', u'kids', u'came', u'.', u'I', u'took', u'the', u'cookie', u'.', u'I', u'do', u\"n't\", u'know', u'what', u'happened', u'after', u'that', u'about', u'the', u'cookie', u'.', u'The', u'guy', u'behind', u'the', u'counter', u'gave', u'us', u'dolls', u'.', u'We', u'did', u'something', u'with', u'the', u'dolls', u',', u'and', u'then', u'we', u'put', u'them', u'back', u'.', u'I', u'walked', u'away', u'to', u'where', u'my', u'mom', u'was', u'.', u'This', u'lady', u'came', u'up', u'to', u'me', u'and', u'said', u',', u'``', u'Do', u'you', u'want', u'a', u'ride', u'someplace', u'in', u'the', u'university', u'?', u\"''\", u'I', u'said', u',', u'``', u'No', u',', u'we', u'need', u'a', u'ride', u'to', u'family', u'housing', u'.', u\"''\", u'I', u'remember', u'her', u'in', u'another', u'dream', u'in', u'a', u'grocery', u'store', u'asking', u'that', u'.', u'I', u'just', u'walked', u'off', u'with', u'my', u'mom', u'.']\n",
      "noun only: [u'mom', u'grocery', u'store', u'cookie', u'area', u'guy', u'cookie', u'cookies', u'grasshoppers', u'spider', u'cookie', u'cookie', u'kids', u'cookie', u'cookie', u'guy', u'counter', u'dolls', u'something', u'dolls', u'mom', u'lady', u'ride', u'someplace', u'university', u'ride', u'family', u'housing', u'dream', u'grocery', u'store', u'mom']\n",
      "lemmatized: [u'mom', u'grocery', u'store', u'cookie', u'area', u'guy', u'cookie', u'cooky', u'grasshopper', u'spider', u'cookie', u'cookie', u'kid', u'cookie', u'cookie', u'guy', u'counter', u'doll', u'something', u'doll', u'mom', u'lady', u'ride', u'someplace', u'university', u'ride', u'family', u'housing', u'dream', u'grocery', u'store', u'mom']\n",
      "Counter({u'cookie': 6, u'mom': 3, u'grocery': 2, u'ride': 2, u'doll': 2, u'guy': 2, u'store': 2, u'counter': 1, u'family': 1, u'area': 1, u'cooky': 1, u'dream': 1, u'grasshopper': 1, u'spider': 1, u'someplace': 1, u'something': 1, u'housing': 1, u'lady': 1, u'university': 1, u'kid': 1})\n",
      "\n",
      "\n",
      "Vietnam Vet: 1970-2008 war dreams (98 dreams) [M]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'Nightmare', u'in', u'Cambodia', u'.', u'In', u'the', u'dream', u'we', u'are', u'being', u'overrun', u'by', u'sappers', u'who', u'have', u'got', u'past', u'the', u'Night', u'Defensive', u'Perimeter', u'trips', u'and', u'claymores', u'and', u'now', u'crawl', u'forward', u'.', u'I', u'wake', u'up', u'and', u'see', u'a', u'boot', u'tread', u'close', u'to', u'my', u'face', u'.', u'I', u'slowly', u'withdraw', u'my', u'.45', u'from', u'its', u'holster', u',', u'pull', u'the', u'hammer', u'back', u',', u'then', u'aim', u'it', u'at', u'the', u'boot', u'.', u'Just', u'then', u'the', u'cloud-obscured', u'moon', u'comes', u'out', u'and', u'I', u'realize', u'the', u'boot', u'is', u'American', u'and', u'that', u'it', u'is', u'Jerry', u'B', u\"'s\", u'foot', u'.', u'In', u'the', u'pitch', u'stillness', u'I', u'point', u'the', u'.45', u'straight', u'up', u'in', u'the', u'air', u'.', u'Pinching', u'the', u'hammer', u'tightly', u'I', u'pull', u'the', u'trigger', u'and', u'settle', u'the', u'hammer', u'back', u'in', u'place', u'.', u'I', u're-holster', u'the', u'pistol', u'and', u'go', u'back', u'to', u'sleep', u'.', u'The', u'next', u'day', u',', u'after', u'a', u'very', u'difficult', u'march', u',', u'all', u'the', u'men', u'are', u'overjoyed', u'to', u'be', u'out', u'of', u'Cambodia', u'.', u'I', u'tell', u'no', u'one', u'what', u'almost', u'happened', u'.']\n",
      "noun only: [u'Nightmare', u'Cambodia', u'dream', u'sappers', u'Night', u'Defensive', u'Perimeter', u'trips', u'claymores', u'forward', u'boot', u'tread', u'face', u'.45', u'holster', u'hammer', u'boot', u'moon', u'boot', u'Jerry', u'B', u'foot', u'pitch', u'stillness', u'.45', u'air', u'hammer', u'trigger', u'hammer', u'place', u'pistol', u'day', u'march', u'men', u'Cambodia', u'one']\n",
      "lemmatized: [u'Nightmare', u'Cambodia', u'dream', u'sapper', u'Night', u'Defensive', u'Perimeter', u'trip', u'claymore', u'forward', u'boot', u'tread', u'face', u'.45', u'holster', u'hammer', u'boot', u'moon', u'boot', u'Jerry', u'B', u'foot', u'pitch', u'stillness', u'.45', u'air', u'hammer', u'trigger', u'hammer', u'place', u'pistol', u'day', u'march', u'men', u'Cambodia', u'one']\n",
      "Counter({u'hammer': 3, u'boot': 3, u'.45': 2, u'Cambodia': 2, u'claymore': 1, u'Nightmare': 1, u'Jerry': 1, u'one': 1, u'holster': 1, u'pitch': 1, u'air': 1, u'trip': 1, u'Perimeter': 1, u'tread': 1, u'moon': 1, u'trigger': 1, u'Defensive': 1, u'forward': 1, u'pistol': 1, u'B': 1, u'march': 1, u'men': 1, u'sapper': 1, u'Night': 1, u'foot': 1, u'day': 1, u'face': 1, u'stillness': 1, u'place': 1, u'dream': 1})\n",
      "\n",
      "\n",
      "Vietnam Vet: 2015 dreams (32 dreams) [M]\n",
      "  noun-only sample dream: \n",
      "tokenized: [u'I', u\"'m\", u'in', u'the', u'Army', u',', u'I', u'have', u'been', u'to', u'war', u',', u'and', u'now', u'am', u'in', u'a', u'large', u'military', u'warehouse', u'.', u'I', u\"'m\", u'climbing', u'up', u'a', u'huge', u'pile', u'of', u'boxes', u'that', u'are', u'packed', u'with', u'thousands', u'of', u'small', u'black', u'toy', u'trains', u'.', u'An', u'MP', u'comes', u'by', u'and', u'harasses', u'me', u'.', u'I', u'start', u'to', u'climb', u'down', u'from', u'the', u'boxes', u',', u'toppling', u'them', u',', u'causing', u'the', u'trains', u'to', u'spill', u'out', u'and', u'onto', u'the', u'floor', u'.', u'I', u'tell', u'the', u'MP', u'nobody', u'pushes', u'me', u'around', u'.', u'I', u'threaten', u'him', u',', u'and', u'he', u'leaves', u'.', u'I', u'walk', u'to', u'another', u'part', u'of', u'the', u'warehouse', u',', u'a', u'gymnasium', u'.', u'Off', u'to', u'one', u'side', u',', u'a', u'small', u'group', u'of', u'black', u'women', u'are', u'watching', u'TV', u'.', u'Otherwise', u',', u'the', u'area', u'is', u'vacant', u'.', u'In', u'my', u'bare', u'feet', u',', u'I', u'start', u'to', u'skate', u'on', u'the', u'wood', u'floor', u'.', u'I', u\"'m\", u'going', u'fast', u',', u'making', u'large', u'sweeping', u'turns', u'.', u'Then', u'a', u'man', u'with', u'movie', u'star', u'looks', u'grabs', u'me', u'.', u'He', u\"'s\", u'kidnapping', u'me', u'.', u'He', u'and', u'others', u'put', u'me', u'in', u'a', u'car', u'.', u'He', u'keeps', u'telling', u'me', u'not', u'to', u'fight', u'him', u',', u'but', u'I', u'wo', u\"n't\", u'let', u'him', u'go', u'.', u'They', u'drive', u'me', u'to', u'a', u'place', u'where', u'criminals', u'do', u'business', u'.', u'The', u'man', u'in', u'the', u'car', u'wants', u'to', u'kill', u'me', u'.', u'We', u'struggle', u'.', u'He', u'keeps', u'trying', u'to', u'break', u'free', u'.', u'I', u'call', u'to', u'the', u'criminals', u'as', u'they', u'walk', u'past', u',', u'but', u'it', u\"'s\", u'hard', u'to', u'speak', u',', u'I', u'make', u'only', u'garbled', u'sounds', u'.', u'Then', u'they', u'hear', u'and', u'see', u'me', u'and', u'approach', u',', u'and', u'I', u'say', u'to', u'them', u',', u'``', u'Kill', u'him', u'.', u\"''\", u'I', u'wake', u'up', u'moaning', u'those', u'words', u'.']\n",
      "noun only: [u'Army', u'war', u'warehouse', u'pile', u'boxes', u'thousands', u'toy', u'trains', u'MP', u'harasses', u'boxes', u'trains', u'floor', u'MP', u'nobody', u'part', u'warehouse', u'gymnasium', u'side', u'group', u'women', u'TV', u'area', u'bare', u'feet', u'wood', u'floor', u'turns', u'man', u'movie', u'star', u'others', u'car', u'place', u'criminals', u'business', u'man', u'car', u'criminals', u'sounds', u'approach', u'Kill', u'words']\n",
      "lemmatized: [u'Army', u'war', u'warehouse', u'pile', u'box', u'thousand', u'toy', u'train', u'MP', u'harasses', u'box', u'train', u'floor', u'MP', u'nobody', u'part', u'warehouse', u'gymnasium', u'side', u'group', u'woman', u'TV', u'area', u'bare', u'foot', u'wood', u'floor', u'turn', u'man', u'movie', u'star', u'others', u'car', u'place', u'criminal', u'business', u'man', u'car', u'criminal', u'sound', u'approach', u'Kill', u'word']\n",
      "Counter({u'floor': 2, u'warehouse': 2, u'criminal': 2, u'train': 2, u'man': 2, u'box': 2, u'car': 2, u'MP': 2, u'Army': 1, u'gymnasium': 1, u'harasses': 1, u'bare': 1, u'toy': 1, u'group': 1, u'TV': 1, u'movie': 1, u'wood': 1, u'pile': 1, u'Kill': 1, u'approach': 1, u'war': 1, u'sound': 1, u'woman': 1, u'star': 1, u'business': 1, u'thousand': 1, u'nobody': 1, u'others': 1, u'foot': 1, u'word': 1, u'area': 1, u'turn': 1, u'part': 1, u'place': 1, u'side': 1})\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reduce to noun-only and lemmatize\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "for dreamer in doc['dreambank']['collection']:\n",
    "    print dreamer['name'] + ' (' + str(len(dreamer['dream'][:])) + ' dreams)' + ' [' + dreamer['sex'] + ']'\n",
    "    \n",
    "    print '  noun-only sample dream: ' \n",
    "    \n",
    "    odict = dreamer['dream'][0]\n",
    "    \n",
    "    # (1) tokenize dream text\n",
    "    # (2) tag with PoS\n",
    "    # (3) reduce to noun-only\n",
    "    # (4) lemmatize nouns\n",
    "    # (5) get frequency counts of the lemmatized nouns\n",
    "    for key, value in odict.items():\n",
    "        if convert(key) == 'report':\n",
    "            text = nltk.tokenize.word_tokenize(value)\n",
    "            print \"tokenized: \" + str(text)\n",
    "            pos = nltk.pos_tag(text)            \n",
    "            noun_only = [w[0] for w in pos if w[1].startswith('N')]\n",
    "            print \"noun only: \" + str(noun_only)\n",
    "            lmtz_noun_only = [lmtzr.lemmatize(word) for word in noun_only]\n",
    "            print \"lemmatized: \" + str(lmtz_noun_only)\n",
    "            counts = Counter(lmtz_noun_only)\n",
    "            print counts\n",
    "        \n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Dreams: 26000\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>W266ID</th>\n",
       "      <th>DreamBankID</th>\n",
       "      <th>DreamNumber</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Dream</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>alta</td>\n",
       "      <td>1</td>\n",
       "      <td>Alta: a detailed dreamer</td>\n",
       "      <td>F</td>\n",
       "      <td>The one at the Meads's house, where it's bigge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>alta</td>\n",
       "      <td>2</td>\n",
       "      <td>Alta: a detailed dreamer</td>\n",
       "      <td>F</td>\n",
       "      <td>I'm at a family reunion in a large fine house ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>alta</td>\n",
       "      <td>3</td>\n",
       "      <td>Alta: a detailed dreamer</td>\n",
       "      <td>F</td>\n",
       "      <td>I watch a plane fly past and shortly realize i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>alta</td>\n",
       "      <td>4</td>\n",
       "      <td>Alta: a detailed dreamer</td>\n",
       "      <td>F</td>\n",
       "      <td>Me pulling the green leaves and berries off so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>alta</td>\n",
       "      <td>5</td>\n",
       "      <td>Alta: a detailed dreamer</td>\n",
       "      <td>F</td>\n",
       "      <td>I'm in a room that reminds me of (but definite...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID W266ID DreamBankID DreamNumber                      Name Sex  \\\n",
       "0  1      1        alta           1  Alta: a detailed dreamer   F   \n",
       "1  2      1        alta           2  Alta: a detailed dreamer   F   \n",
       "2  3      1        alta           3  Alta: a detailed dreamer   F   \n",
       "3  4      1        alta           4  Alta: a detailed dreamer   F   \n",
       "4  5      1        alta           5  Alta: a detailed dreamer   F   \n",
       "\n",
       "                                               Dream  \n",
       "0  The one at the Meads's house, where it's bigge...  \n",
       "1  I'm at a family reunion in a large fine house ...  \n",
       "2  I watch a plane fly past and shortly realize i...  \n",
       "3  Me pulling the green leaves and berries off so...  \n",
       "4  I'm in a room that reminds me of (but definite...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ID = 0\n",
    "W266ID = 0\n",
    "DreamBankID = ''\n",
    "DreamNumber = ''\n",
    "Name = ''\n",
    "Sex = ''\n",
    "Dream = ''\n",
    "HasDream = 0\n",
    "\n",
    "df = pd.DataFrame(columns=[\"ID\", \"W266ID\", \"DreamBankID\", \"DreamNumber\", \"Name\", \"Sex\", \"Dream\"])\n",
    "\n",
    "for dreamer in doc['dreambank']['collection']:\n",
    "    Name = dreamer['name'] \n",
    "    DreamBankID = dreamer['id']\n",
    "    W266ID = int(dreamer['w266ID'])\n",
    "    Sex = dreamer['sex']\n",
    "    \n",
    "    for odict in dreamer['dream']:\n",
    "        HasDream = 0\n",
    "        for key, value in odict.items():\n",
    "            if convert(key) == 'report':\n",
    "                Dream = convert(value)\n",
    "                HasDream = 1\n",
    "            if convert(key) == 'number':\n",
    "                DreamNumber = convert(value)\n",
    "        \n",
    "        if HasDream == 1:\n",
    "            ID += 1\n",
    "            df = df.append({\n",
    "                \"ID\": ID,\n",
    "                \"W266ID\": W266ID,\n",
    "                \"DreamBankID\": DreamBankID,\n",
    "                \"DreamNumber\": DreamNumber,\n",
    "                \"Name\": Name,\n",
    "                \"Sex\": Sex,\n",
    "                \"Dream\": Dream\n",
    "                }, ignore_index=True)        \n",
    "#     print '\\n'\n",
    "\n",
    "# print df\n",
    "print \"Total Dreams: \" + str(ID)\n",
    "print \"\\n\"\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#DreamerID = 1\n",
    "#df2 = df.loc[df['W266ID'] == DreamerID\n",
    "             \n",
    "#df2.head()\n",
    "\n",
    "#dreamsgraph = \" \".join(dreams)        \n",
    "#dreamsgraph = list(nltk.tokenize.word_tokenize(dreamsgraph))\n",
    "#dreamsgraph = [word for word in dreamsgraph if word not in stopwords.words('english')]\n",
    "#dreamsgraph = [word for word in dreamsgraph if word not in ['x94', 'x92t', 'ti', 'x92d', 'x92s', 'x85']]\n",
    "\n",
    "#counts = Counter(dreamsgraph)\n",
    "#for k in list(counts):\n",
    "#    if counts[k] < 40:\n",
    "#        del counts[k]\n",
    "\n",
    "#print counts\n",
    "\n",
    "#labels, values = zip(*counts.items())        \n",
    "\n",
    "## sort in descending order\n",
    "#indSort = np.argsort(values)[::-1]\n",
    "\n",
    "#labels = np.array(labels)[indSort]\n",
    "#values = np.array(values)[indSort]\n",
    "#indexes = np.arange(len(labels))\n",
    "\n",
    "#plt.bar(indexes, values, align='center', alpha=0.5, color='blue')\n",
    "#plt.xticks(indexes, labels, rotation=45) #+ bar_width\n",
    "\n",
    "## Get current size\n",
    "#fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "## print \"Current size:\", fig_size\n",
    "\n",
    "## Set figure width to 12 and height to 9\n",
    "#fig_size[0] = 16\n",
    "#fig_size[1] = 12\n",
    "#plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Males: 7813\n",
      "Females: 18187\n",
      "Total: 26000\n"
     ]
    }
   ],
   "source": [
    "print 'Males: ' + str(len(df[df['Sex']=='M']))\n",
    "print 'Females: ' + str(len(df[df['Sex']=='F']))\n",
    "print 'Total: ' + str(len(df[df['Sex']=='M']) + len(df[df['Sex']=='F']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "We want to create a different model for each dreamer (i.e. a classifier identifying one vs all-others for each dreamer). This will allow us to identify the most predictive words in identifying each dreamer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ptomeny', 'grandniece', '1,800', 'wrought-iron', 'Poetry', \"'glue\", 'woods', 'clotted', 'spiders', 'hanging', 'woody', 'comically', 'localized', 'Prone', 'sevens', 'disobeying', 'caner', 'canes', 'DISAPPOINTED', 'unpaused', 'scold', 'jell-o', 'Vulcan', 'Western', 'unhistoric', 'Retreat', 'bocks', 'PETRIFIED', 'higher-ups', 'button-down', 'Blade', 'lockmold*', 'pigment', 'capoeira', \"'Love\", 'screaming', 'revelers', 'wooded', 'spacy', 'broiler', 'wooden', 'something-anything', 'Saco', 'back-handsprings', 'circuitry', 'crotch', 'stereotypical', 'seven-', 'guardsmen', 'cash/copy', 'them-but', 'undead', 'Print-Shop', 'snuggles', 'scrapes', 'scraper', 'Balling', 'feasibility', 'miniatures', '273', 'Diapering', 'Pierce-Gage', '276', 'Jiminy', 'spindly', 'scraped', 'snuggled', 'inanimate', 'errors', 'dialogs', 'semicircular', 'Initially', \"'eigh\", 'cooking', \"'score\", '6^X2', 'cores', 'Hamilton', 'Puddleduck', 'designing', 'College', 'spidery', 'hallucinating', 'succumb', 'kid.', \"'pleasure\", 'Steinway', 'flying/swimming', 'spatial-patterned', 'crouch', 'Happening', 'chins', 'Foundation', 'fire-', \"'argh\", 'denominations', 'perforations', 'brainwashed', '47', 'perfunctorily', 'china', 'Armature', 'confronts', '-they', 'Manger', 'reggae', 'kids', 'uplifting', 'naturel', 'climbed', 'Ginger', 'BANQUET', 'weamies', 'Farther', 'natures', 'neurologist', 'spotty', 'Isles', 'kidn', 'rebel', 'golden', 'Dexter', 'Awakens', 'foreground.', 'projection', 'squattier', 'badge-and', 'Harvey', 'Clove', 'orangish', 'lower-middle', 'stern', 'forbore', 'families/opponent', 'frosty', 'catchy', 'insecurity', 'abbreviations', 'GranMere', 'cannibal', 'Leilani', 'Vernon', 'music', 'therefore', 'mystic', 'dusting', 'ungulates', 'happening.', 'show-', 'Buon', 'monster-like', 'vocation', 'homily', 'Dumps', 'UNITY', 'Lancaster/Archie', 'schoolboys', 'unpack', 'circumstances', 'morally', 'locked', 'top-like', 'cataract', 'Zanuzez', 'locker', 'locket', 'WAVER', 'Boys-', 'RLP', 'Ficur', 'Ficus', 'Deamland', 'Le', 'sworls', 'rocks.', 'wand', 'La', 'Lo', 'Li', 'American-style', 'titanium', 'white-glazed', 'want', 'this..shift', 'rearranges/reorders', 'killed..', 'bus/class', 'absolute', 'LG', 'sugarless', 'reenact', 'LA', 'indifferent.', 'travel', 'Attention', \"'borders\", 'copious', 'LI', 'LV', 'LW', 'LT', 'playback', 'LR', 'strip.', 'LP', 'double-barreled', 'Falcon', 'canaries', 'ethylene', 'over-sized', '100-150', 'uselessness', 'L.', 'assimilated', 'Pinalecki', 'Elem', 'dinosaurs', 'wrong', 'HEAR', \"'complex\", 'Hurriedly', 'sentencing', \"'Candyman\", 'domed', 'Addam', 'matter-of-fact', 'colorfully', 'Zhaan', 'Hoping', 'romped', 'CFC', \"'new\", 'sickening', 'tulip', '18th', 'Mabel', 'perpetrator', 'farmlands', 'nonsensical', 'Willy', 'Even', 'snugly', 'welcomed', 'Blackstone', 'canyon-like', 'whizzing', 'partnered', '620', 'bomb-like', 'Robertson', 'rewarded', 'stabbed', 'playhouse', 'fir', 'Abbott', 'His', 'wickedly', 'fit', 'Schroeder', 'bringing', 'fix', '624', 'Smiths', 'so-and-so', 'Vaguely', 'anticipations', 'Hid', 'reentered', 'pisses', 'fin', 'Him', 'noises..', 'Dizy', 'Furnishings', 'Hickory', 'perching', 'songwriter', 'stovepipe', 'Diakun', 'caramels', 'LIGHTNING', 'becaUse', 'sodium', 'effects', 'Alot', 'Will.', 'sixteen', 'undeveloped', 'saddened', 'restaurant/lounge', 'Mechanics', 'stair-steps', 'dungeon-like', 'defecate', 'whacking', 'celebrating.', 'Shame', 'Snowblower', 'arrow', 'Lan', 'windmill', 'telescope', 'Sneaky', 'lolly/chocolate', 'Yojimbo', 'Insult', 'comprises', 'grapefruits', '395', 'worn-out', 'Crowds', \"'Lose\", 'drag-queen', 'forwarding', 'Susie', 'smirk', 'indiscretion', 'mason', 'encourage', 'adapt', 'Basement', 'Suckers', 'Carebears', 'smart-ass', 'nightdress', 'stamping', 'colouring-ins', 'hover-car', 'underfoot', 'strata', 'Schwarz', 'Corinne', 'sinuate', 'pumpkins', 'Ego', 'corrects', 'Transformers', 'estimate', 'chlorine', 'jugs', 'husbands', 'competes', 'Nigel', 'criminologists', 'hella', 'semi-trailer', 'connive', 'Nigee', 'Mushrooms', 'Cuzco', 'Peterson-', 'disturbed', 'competed', 'dentures', 'Facts', \"'appropriately\", '19.18', 'Degeneres', 'breed', 'paddy-like', 'callous', 'disfigured', 'Traveller', 'weekdays', 'cop/security', 'Luria', 'Duchovny', 'reward-', 'seizures', 'olds', 'LAND', 'renovated', 'service', 'Tyrone', 'needed', 'master', 'critter', \"'Book\", 'College.', 'normal/baggy', 'rewards', 'Hood-like', 'complainers', 'Sexuality', 'exulted', 'tidepool', 'motionless', 'mutilated', 'noise/speaking', 'Daddy', 'positively', 'Repair', 'spayed', 're-recording', 'handcuffs', \"'madly\", 'idly', 'regulator', 'idle', 'exclaimed', 'Disneyland-like', 'Blu-Ray', 'mauve-pink', 'Zeta', 'feeling', 'old.', 'mitres', 'assertiveness', 'Chicago', 'codeine', 'Preludes', 'Generally', 'spectrum', 'dozed', 'increment', '279', 'arousal', 'pomp', 'urinate', 'landlord/housemate', 'dozen', 'Theo', 'Then', 'Them', 'wholesome', 'Britney', 'Fontaine', 'stuiff', 'Thee', 'uncouth', '2.75', 'Thea', 'racers', 'cheap-looking', 'They', 'horrible-looking', 'concedes', 'committing', 'diminishing', 'contest..', 'disjointed', 'tensely', 'mouth', 'reverence', 'conceded', 'transverse', 'Mahler', 'singer', 'singes', '3DE', 'SKILLS', '10-page', '10:30am', 'Nouy', 'tech', 'paneling', 'fiberfill', 'scream', 'butt-just', 'Molehill', 'saying', 'Curly', 'shed/warehouse', 'Breakup', 'Curls', 'Jacobson', 'padded', 'hillbillies', 'ballfield', 'Butterfinger', 'Marks', 'fifta', 'Beerfest', 'tempted', 'Daryl', 'Civilisation', 'Marky', 'hounded', '14-15', 'clicked', 'Ceil', '105', 'Reiser', 'LIVE', 'drills.', 'bullshitter', 'rich', 'Crappie', 'rice', 'Mylar', 'plate', 'partial-walls', 'mouths', 'Tricks', 'ride-like', 'Crile', 'umbilical', '7-11', 'Classmate-F-17']\n"
     ]
    }
   ],
   "source": [
    "# randomly shuffle dataframe\n",
    "# set seed for consistency while running\n",
    "np.random.seed(0)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "# add 41 flag columns denoting dreamers (one col per dreamer) \n",
    "dreamer_flag = pd.get_dummies(df['W266ID'], prefix='Dreamer')\n",
    "df = pd.concat([df, dreamer_flag], axis=1)\n",
    "\n",
    "# create vocab from all dreams\n",
    "dreams_flat = df['Dream'].values.flatten().tolist()\n",
    "dreams_list = \" \".join(dreams_flat)\n",
    "vocab = list(set(nltk.tokenize.word_tokenize(dreams_list)))\n",
    "\n",
    "# Why does this blow up the shape later?\n",
    "#pos = nltk.pos_tag(vocab)            \n",
    "#noun_only = [w[0] for w in pos if w[1].startswith('N')]\n",
    "#vocab = [lmtzr.lemmatize(word) for word in noun_only]\n",
    "#vocab = list(set(vocab)) # get rid of duplicates\n",
    "\n",
    "print vocab[1:500]\n",
    "\n",
    "def split_data(df, W266ID, train=0.6):        \n",
    "    # column for \"our\" dreamer\n",
    "    dreamer_label = 'Dreamer_' + str(W266ID)\n",
    "\n",
    "    # make 60/40 split of train/test\n",
    "    # test will be evenly split between dev and test in the next step\n",
    "    num_train = int(len(df) * train)\n",
    "    num_test = int(len(df) * (1-train)) \n",
    "\n",
    "    train_data, train_labels = df['Dream'][:num_train], df[dreamer_label][:num_train]\n",
    "    dev_data, dev_labels = df['Dream'][-num_test : -num_test//2], df[dreamer_label][-num_test : -num_test//2] \n",
    "    test_data, test_labels = df['Dream'][-num_test//2:], df[dreamer_label][-num_test//2:]\n",
    "\n",
    "    return train_data, train_labels, dev_data, dev_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jay\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Reg:\t C=0.0001\t BOW: F1-score=0.496\t TFIDF: F1-score=0.496\n",
      "Logistic Reg:\t C=0.0010\t BOW: F1-score=0.778\t TFIDF: F1-score=0.496\n",
      "Logistic Reg:\t C=0.0100\t BOW: F1-score=0.878\t TFIDF: F1-score=0.496\n",
      "Logistic Reg:\t C=0.1000\t BOW: F1-score=0.916\t TFIDF: F1-score=0.496\n",
      "Logistic Reg:\t C=1.0000\t BOW: F1-score=0.933\t TFIDF: F1-score=0.712\n",
      "Logistic Reg:\t C=10.0000\t BOW: F1-score=0.930\t TFIDF: F1-score=0.872\n",
      "Logistic Reg:\t C=100.0000\t BOW: F1-score=0.934\t TFIDF: F1-score=0.912\n",
      "Logistic Reg:\t C=500.0000\t BOW: F1-score=0.931\t TFIDF: F1-score=0.909\n",
      "\n",
      "Best model:\t C=100.0000\t vectorizer = BOW\t F1-score=0.934\n"
     ]
    }
   ],
   "source": [
    "# split data for dreamer1 - Alta\n",
    "train_data, train_labels, dev_data, dev_labels, test_data, test_labels = split_data(df, W266ID=1)\n",
    "    \n",
    "# Create a Bag-of-Words Vectorizer\n",
    "vec = CountVectorizer(vocabulary=vocab)\n",
    "vec_bow_train_data = vec.fit_transform(train_data)\n",
    "vec_bow_dev_data = vec.transform(dev_data)  \n",
    "\n",
    "# Create a Tfidf Vectorizer\n",
    "vec_tfidf = TfidfVectorizer(stop_words='english')\n",
    "vec_tfidf_train_data = vec_tfidf.fit_transform(train_data)\n",
    "vec_tfidf_dev_data   = vec_tfidf.transform(dev_data)\n",
    "\n",
    "best_lr_score = 0\n",
    "\n",
    "## Logistic reg\n",
    "for c in (0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 500):\n",
    "    bow_log = LogisticRegression(C = c)\n",
    "    bow_log.fit(vec_bow_train_data, train_labels)\n",
    "    \n",
    "    tfidf_logit_model = LogisticRegression(C=c)\n",
    "    tfidf_logit_model.fit(vec_tfidf_train_data, train_labels)\n",
    "    \n",
    "    f1_bow_lr_score = metrics.f1_score(dev_labels, bow_log.predict(vec_bow_dev_data), average='macro')    \n",
    "    f1_tfidf_lr_score = metrics.f1_score(dev_labels, tfidf_logit_model.predict(vec_tfidf_dev_data), average='macro') \n",
    "\n",
    "    print 'Logistic Reg:\\t C=%3.4f\\t BOW: F1-score=%3.3f\\t TFIDF: F1-score=%3.3f' % (c, f1_bow_lr_score, f1_tfidf_lr_score)\n",
    "\n",
    "    if f1_bow_lr_score > best_lr_score:\n",
    "        best_lr_score = f1_bow_lr_score\n",
    "        best_C = c \n",
    "        vectorizer = 'BOW'\n",
    "        \n",
    "    if f1_tfidf_lr_score > best_lr_score:\n",
    "        best_lr_score = f1_tfidf_lr_score\n",
    "        best_C = c\n",
    "        vectorizer = 'TFIDF'\n",
    "\n",
    "print ''\n",
    "print 'Best model:\\t C=%3.4f\\t vectorizer = %s\\t F1-score=%3.3f' % (best_C, vectorizer, best_lr_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Reg:\t C=0.0001\t BOW: F1-score=0.500\t TFIDF: F1-score=0.500\n",
      "Logistic Reg:\t C=0.0010\t BOW: F1-score=0.500\t TFIDF: F1-score=0.500\n",
      "Logistic Reg:\t C=0.0100\t BOW: F1-score=0.500\t TFIDF: F1-score=0.500\n",
      "Logistic Reg:\t C=0.1000\t BOW: F1-score=0.500\t TFIDF: F1-score=0.500\n",
      "Logistic Reg:\t C=1.0000\t BOW: F1-score=0.500\t TFIDF: F1-score=0.500\n",
      "Logistic Reg:\t C=10.0000\t BOW: F1-score=0.590\t TFIDF: F1-score=0.500\n",
      "Logistic Reg:\t C=100.0000\t BOW: F1-score=0.590\t TFIDF: F1-score=0.500\n",
      "Logistic Reg:\t C=500.0000\t BOW: F1-score=0.590\t TFIDF: F1-score=0.500\n",
      "\n",
      "Best model:\t C=10.0000\t vectorizer = BOW\t F1-score=0.590\n"
     ]
    }
   ],
   "source": [
    "# split data for dreamer2 - Angie\n",
    "train_data, train_labels, dev_data, dev_labels, test_data, test_labels = split_data(df, W266ID=2)\n",
    "    \n",
    "# Create a Bag-of-Words Vectorizer\n",
    "vec = CountVectorizer(vocabulary=vocab)\n",
    "vec_bow_train_data = vec.fit_transform(train_data)\n",
    "vec_bow_dev_data = vec.transform(dev_data)  \n",
    "\n",
    "# Create a Tfidf Vectorizer\n",
    "vec_tfidf = TfidfVectorizer(stop_words='english')\n",
    "vec_tfidf_train_data = vec_tfidf.fit_transform(train_data)\n",
    "vec_tfidf_dev_data   = vec_tfidf.transform(dev_data)\n",
    "\n",
    "best_lr_score = 0\n",
    "\n",
    "## Logistic reg\n",
    "for c in (0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 500):\n",
    "    bow_log = LogisticRegression(C = c)\n",
    "    bow_log.fit(vec_bow_train_data, train_labels)\n",
    "    \n",
    "    tfidf_logit_model = LogisticRegression(C=c)\n",
    "    tfidf_logit_model.fit(vec_tfidf_train_data, train_labels)\n",
    "    \n",
    "    f1_bow_lr_score = metrics.f1_score(dev_labels, bow_log.predict(vec_bow_dev_data), average='macro')    \n",
    "    f1_tfidf_lr_score = metrics.f1_score(dev_labels, tfidf_logit_model.predict(vec_tfidf_dev_data), average='macro') \n",
    "\n",
    "    print 'Logistic Reg:\\t C=%3.4f\\t BOW: F1-score=%3.3f\\t TFIDF: F1-score=%3.3f' % (c, f1_bow_lr_score, f1_tfidf_lr_score)\n",
    "\n",
    "    if f1_bow_lr_score > best_lr_score:\n",
    "        best_lr_score = f1_bow_lr_score\n",
    "        best_C = c \n",
    "        vectorizer = 'BOW'\n",
    "        \n",
    "    if f1_tfidf_lr_score > best_lr_score:\n",
    "        best_lr_score = f1_tfidf_lr_score\n",
    "        best_C = c\n",
    "        vectorizer = 'TFIDF'\n",
    "\n",
    "print ''\n",
    "print 'Best model:\\t C=%3.4f\\t vectorizer = %s\\t F1-score=%3.3f' % (best_C, vectorizer, best_lr_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Reg:\t C=0.0001\t BOW: F1-score=0.498\t TFIDF: F1-score=0.498\n",
      "Logistic Reg:\t C=0.0010\t BOW: F1-score=0.498\t TFIDF: F1-score=0.498\n",
      "Logistic Reg:\t C=0.0100\t BOW: F1-score=0.527\t TFIDF: F1-score=0.498\n",
      "Logistic Reg:\t C=0.1000\t BOW: F1-score=0.672\t TFIDF: F1-score=0.498\n",
      "Logistic Reg:\t C=1.0000\t BOW: F1-score=0.765\t TFIDF: F1-score=0.498\n",
      "Logistic Reg:\t C=10.0000\t BOW: F1-score=0.773\t TFIDF: F1-score=0.580\n",
      "Logistic Reg:\t C=100.0000\t BOW: F1-score=0.780\t TFIDF: F1-score=0.689\n",
      "Logistic Reg:\t C=500.0000\t BOW: F1-score=0.780\t TFIDF: F1-score=0.708\n",
      "\n",
      "Best model:\t C=100.0000\t vectorizer = BOW\t F1-score=0.780\n"
     ]
    }
   ],
   "source": [
    "# split data for dreamer3 - Arlie\n",
    "train_data, train_labels, dev_data, dev_labels, test_data, test_labels = split_data(df, W266ID=3)\n",
    "    \n",
    "# Create a Bag-of-Words Vectorizer\n",
    "vec = CountVectorizer(vocabulary=vocab)\n",
    "vec_bow_train_data = vec.fit_transform(train_data)\n",
    "vec_bow_dev_data = vec.transform(dev_data)  \n",
    "\n",
    "# Create a Tfidf Vectorizer\n",
    "vec_tfidf = TfidfVectorizer(stop_words='english')\n",
    "vec_tfidf_train_data = vec_tfidf.fit_transform(train_data)\n",
    "vec_tfidf_dev_data   = vec_tfidf.transform(dev_data)\n",
    "\n",
    "best_lr_score = 0\n",
    "\n",
    "## Logistic reg\n",
    "for c in (0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 500):\n",
    "    bow_log = LogisticRegression(C = c)\n",
    "    bow_log.fit(vec_bow_train_data, train_labels)\n",
    "    \n",
    "    tfidf_logit_model = LogisticRegression(C=c)\n",
    "    tfidf_logit_model.fit(vec_tfidf_train_data, train_labels)\n",
    "    \n",
    "    f1_bow_lr_score = metrics.f1_score(dev_labels, bow_log.predict(vec_bow_dev_data), average='macro')    \n",
    "    f1_tfidf_lr_score = metrics.f1_score(dev_labels, tfidf_logit_model.predict(vec_tfidf_dev_data), average='macro') \n",
    "\n",
    "    print 'Logistic Reg:\\t C=%3.4f\\t BOW: F1-score=%3.3f\\t TFIDF: F1-score=%3.3f' % (c, f1_bow_lr_score, f1_tfidf_lr_score)\n",
    "\n",
    "    if f1_bow_lr_score > best_lr_score:\n",
    "        best_lr_score = f1_bow_lr_score\n",
    "        best_C = c \n",
    "        vectorizer = 'BOW'\n",
    "        \n",
    "    if f1_tfidf_lr_score > best_lr_score:\n",
    "        best_lr_score = f1_tfidf_lr_score\n",
    "        best_C = c\n",
    "        vectorizer = 'TFIDF'\n",
    "\n",
    "print ''\n",
    "print 'Best model:\\t C=%3.4f\\t vectorizer = %s\\t F1-score=%3.3f' % (best_C, vectorizer, best_lr_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # split data for dreamer1 - Alta\n",
    "# train_data, train_labels, dev_data, dev_labels, test_data, test_labels = split_data(df, W266ID=1)\n",
    "    \n",
    "# # Create a Bag-of-Words Vectorizer\n",
    "# vec = CountVectorizer(vocabulary=vocab)\n",
    "# vec_train_data = vec.fit_transform(train_data)\n",
    "# vec_dev_data = vec.transform(dev_data)  \n",
    "\n",
    "# log = LogisticRegression(C = 100)\n",
    "# log.fit(vec_train_data, train_labels)\n",
    "\n",
    "# test_df = pd.DataFrame('Labels': [dev_labels], 'Prediction': log.predict(vec_dev_data), 'Correct_Pred': dev_labels==log.predict(vec_dev_data))\n",
    "# data = {'Labels': dev_labels, 'Prediction': log.predict(vec_dev_data), 'Correct_Pred': dev_labels==log.predict(vec_dev_data)}\n",
    "# test_df = pd.DataFrame(data)\n",
    "# test_df.head(30)\n",
    "\n",
    "# print confusion_matrix(dev_labels, log.predict(vec_dev_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a model for each dreamer\n",
    "\n",
    "We need to run a separate model for each dreamer. The models will predict if a dream comes from that dreamer or from \"all-others\". Since bag-of-words was working best in the above models, we will continue to use that for our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run Logisitic Regression for each dreamer\n",
    "\n",
    "models = {}\n",
    "\n",
    "for i in range(0,41):\n",
    "    # split data\n",
    "    train_data, train_labels, dev_data, dev_labels, test_data, test_labels = split_data(df, W266ID=i)\n",
    "\n",
    "    # Create a Bag-of-Words Vectorizer\n",
    "    vec = CountVectorizer(vocabulary=vocab)\n",
    "    vec_train_data = vec.fit_transform(train_data)\n",
    "    vec_dev_data = vec.transform(dev_data)  \n",
    "\n",
    "    ## Logistic reg\n",
    "    log = LogisticRegression(C = 100)\n",
    "    log.fit(vec_bow_train_data, train_labels)\n",
    "\n",
    "    # score model\n",
    "    f1_score = metrics.f1_score(dev_labels, log.predict(vec_dev_data), average='macro')\n",
    "    \n",
    "    # find the most-predictive features\n",
    "    ### we're not using the weights currently, but might be useful/interesting later ###    \n",
    "    best_feature_positions = log.coef_.argsort()[0][-5::]\n",
    "    best_feature_weights = log.coef_[0][best_feature_positions.astype(int)]\n",
    "\n",
    "    # get word labels for our features\n",
    "    words = []\n",
    "    for ft in best_feature_positions.astype(int):\n",
    "        words.append(vec.get_feature_names()[ft])\n",
    "    \n",
    "    models[i] = (log, f1_score, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Predictive Words\n",
    "\n",
    "Now that we have a separate model for each dreamer, we can pull out the most predictive words from each model. This shows, for a given dreamer, which words are most predictive of their dreams as opposed to someone else's dream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W266ID=0 \tMost Predictive ['instructor', 'border', 'characters', 'setting', 'dreamed']\n",
      "W266ID=1 \tMost Predictive ['rather', 'somewhere', 'somebody', 'here', 're']\n",
      "W266ID=2 \tMost Predictive ['children', 'quarter', 'campus', 'meet', 'pancakes']\n",
      "W266ID=3 \tMost Predictive ['poison', 'hometown', 'picture', 'husband', 'model']\n",
      "W266ID=4 \tMost Predictive ['helpful', 'nightmares', 'nightmare', 'neat', 'scary']\n",
      "W266ID=5 \tMost Predictive ['stands', 'leading', 'red', 'ghost', 'world']\n",
      "W266ID=6 \tMost Predictive ['street', 'bunk', 'note', 'stranger', 'reach']\n",
      "W266ID=7 \tMost Predictive ['surprise', 'standing', 'real', 'burglar', 'few']\n",
      "W266ID=8 \tMost Predictive ['looked', 'toilet', 'red', 'bathroom', 'period']\n",
      "W266ID=9 \tMost Predictive ['definitely', 'stack', 'abstract', 'buick', 'insured']\n",
      "W266ID=10 \tMost Predictive ['mixed', 'preaching', 'teaching', 'miss', 'hats']\n",
      "W266ID=11 \tMost Predictive ['dream', 'mood', 'both', 'clearly', 'looks']\n",
      "W266ID=12 \tMost Predictive ['laid', 'medicine', 'chest', 'radio', 'pill']\n",
      "W266ID=13 \tMost Predictive ['sheets', 'witnesses', 'repaired', 'email', 'waking']\n",
      "W266ID=14 \tMost Predictive ['visit', 'toes', 'painting', 'shift', 'frank']\n",
      "W266ID=15 \tMost Predictive ['beside', 'seven', 'joint', 'suddenly', 'army']\n",
      "W266ID=16 \tMost Predictive ['ex', 'slammed', 'huge', 'hospital', 'fight']\n",
      "W266ID=17 \tMost Predictive ['mom', 'video', 'promptly', 'diary', 'god']\n",
      "W266ID=18 \tMost Predictive ['sound', 'laugh', 'tape', 'woke', 'kind']\n",
      "W266ID=19 \tMost Predictive ['sat', 'thought', 'physics', 'remember', 'scores']\n",
      "W266ID=20 \tMost Predictive ['sets', 'ago', 'gift', 'plastic', 'sad']\n",
      "W266ID=21 \tMost Predictive ['grandpa', 'girlfriend', 'may', 'fellatio', 'missing']\n",
      "W266ID=22 \tMost Predictive ['rubbing', 'dreamt', 'cowboys', 'boyfriend', 'observatory']\n",
      "W266ID=23 \tMost Predictive ['supposedly', 'cheeseburger', 'marketed', 'chicken', 'belt']\n",
      "W266ID=24 \tMost Predictive ['tests', 'comic', 'john', 'lost', 'tooth']\n",
      "W266ID=25 \tMost Predictive ['accident', 'end', 'hugged', 'grade', 'mike']\n",
      "W266ID=26 \tMost Predictive ['mumbling', 'freezer', 'drunk', 'restaurant', 'art']\n",
      "W266ID=27 \tMost Predictive ['spotting', 'giraffes', 'pond', 'sort', 'husband']\n",
      "W266ID=28 \tMost Predictive ['thru', 'although', 'grain', 'recall', 'wife']\n",
      "W266ID=29 \tMost Predictive ['drunk', '24', 'hell', 'team', 'softball']\n",
      "W266ID=30 \tMost Predictive ['match', 'museum', 'though', 'fragment', 'hometown']\n",
      "W266ID=31 \tMost Predictive ['patient', 'printing', 'certain', 'ward', 'sister']\n",
      "W266ID=32 \tMost Predictive ['ft', 'thru', 'trouble', 'saw', 'winner']\n",
      "W266ID=33 \tMost Predictive ['agency', 'educational', 'wife', 'apparently', 'assassinated']\n",
      "W266ID=34 \tMost Predictive ['stood', 'earnestly', 'woke', 'much', 'morning']\n",
      "W266ID=35 \tMost Predictive ['police', 'after', 'started', 'set', 'away']\n",
      "W266ID=36 \tMost Predictive ['created', 'circus', 'dances', 'occaisionaly', 'travelling']\n",
      "W266ID=37 \tMost Predictive ['lights', 'everyone', 'now', 're', 'wake']\n",
      "W266ID=38 \tMost Predictive ['alarm', 'should', 'because', 'ever', 'talking']\n",
      "W266ID=39 \tMost Predictive ['everyplace', 'long', 'white', 'scary', 'corridor']\n",
      "W266ID=40 \tMost Predictive ['life', 'ends', 'begin', 'wake', 'recollection']\n"
     ]
    }
   ],
   "source": [
    "for key, (model, score, predictive_words) in models.iteritems():\n",
    "    print 'W266ID='+str(key), '\\tMost Predictive', predictive_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Each Dreamer\n",
    "\n",
    "We can use our models to predict which person a given dream came from. If we take a dream from our test set and run all 41 models on that dream, we will get probabilities of the dream coming from that person. We can then take the highest probability and make that our prediction for who the dream came from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jay\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.814038461538\n"
     ]
    }
   ],
   "source": [
    "# predict dreamer for each test dream\n",
    "vec_test_data = vec.transform(test_data)\n",
    "\n",
    "preds = []\n",
    "# for dream in range(len(test_labels)):\n",
    "for dream in range(len(test_labels)):\n",
    "    highest_prob = 0\n",
    "    \n",
    "    # predicted probability of the correct label for each model\n",
    "    for key, (model, score, w) in models.iteritems():\n",
    "        prob_correct =  model.predict_proba(vec_test_data[dream])[0][1]\n",
    "        if prob_correct > highest_prob:\n",
    "            highest_prob = prob_correct\n",
    "            pred = key\n",
    "    \n",
    "    preds.append(pred)\n",
    "    \n",
    "print sum(preds == df['W266ID'][20800:]) / len(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got an 82% success rate on our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>W266ID</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11171</th>\n",
       "      <td>1</td>\n",
       "      <td>Alta: a detailed dreamer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7842</th>\n",
       "      <td>2</td>\n",
       "      <td>Angie: age 18 &amp; 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15181</th>\n",
       "      <td>3</td>\n",
       "      <td>Arlie: a middle-aged woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8222</th>\n",
       "      <td>4</td>\n",
       "      <td>Barb Sanders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8209</th>\n",
       "      <td>4</td>\n",
       "      <td>Barb Sanders #2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15894</th>\n",
       "      <td>5</td>\n",
       "      <td>Robert Bosnak: A dream analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24910</th>\n",
       "      <td>6</td>\n",
       "      <td>Chris: a transvestite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11756</th>\n",
       "      <td>7</td>\n",
       "      <td>Chuck: a physical scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545</th>\n",
       "      <td>8</td>\n",
       "      <td>Dahlia: concerns with appearance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16737</th>\n",
       "      <td>9</td>\n",
       "      <td>David: teenage dreams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24578</th>\n",
       "      <td>10</td>\n",
       "      <td>Dorothea: 53 years of dreams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22189</th>\n",
       "      <td>11</td>\n",
       "      <td>Ed: dreams of his late wife</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9440</th>\n",
       "      <td>12</td>\n",
       "      <td>Edna: a blind woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20762</th>\n",
       "      <td>13</td>\n",
       "      <td>Elizabeth: a woman in her 40s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2533</th>\n",
       "      <td>14</td>\n",
       "      <td>Emma: 48 years of dreams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1542</th>\n",
       "      <td>15</td>\n",
       "      <td>Emma's Husband</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12206</th>\n",
       "      <td>16</td>\n",
       "      <td>Esther: an adolescent girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13520</th>\n",
       "      <td>17</td>\n",
       "      <td>Izzy (all)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4040</th>\n",
       "      <td>18</td>\n",
       "      <td>Jasmine (all)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21971</th>\n",
       "      <td>19</td>\n",
       "      <td>Jeff: a lucid dreamer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17734</th>\n",
       "      <td>20</td>\n",
       "      <td>Joan: a lesbian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20988</th>\n",
       "      <td>21</td>\n",
       "      <td>Kenneth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20664</th>\n",
       "      <td>22</td>\n",
       "      <td>Madeline 2: College Dorms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12230</th>\n",
       "      <td>22</td>\n",
       "      <td>Madeline 4: After College</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5429</th>\n",
       "      <td>22</td>\n",
       "      <td>Madeline 3: Off-Campus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845</th>\n",
       "      <td>22</td>\n",
       "      <td>Madeline 1: High School</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13054</th>\n",
       "      <td>23</td>\n",
       "      <td>Mack: A poor recaller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>24</td>\n",
       "      <td>Mark: a young boy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24940</th>\n",
       "      <td>25</td>\n",
       "      <td>Melissa: a young girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15369</th>\n",
       "      <td>26</td>\n",
       "      <td>Merri: an artist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4425</th>\n",
       "      <td>27</td>\n",
       "      <td>Melora (Melvin's wife)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21467</th>\n",
       "      <td>28</td>\n",
       "      <td>Melvin (Melora's husband)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11189</th>\n",
       "      <td>29</td>\n",
       "      <td>Nancy: Caring &amp; headstrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13952</th>\n",
       "      <td>30</td>\n",
       "      <td>The Natural Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>31</td>\n",
       "      <td>Norman: a child molester</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12478</th>\n",
       "      <td>32</td>\n",
       "      <td>Pegasus: a factory worker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>33</td>\n",
       "      <td>Phil 1: teens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25460</th>\n",
       "      <td>33</td>\n",
       "      <td>Phil 3: retirement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16377</th>\n",
       "      <td>33</td>\n",
       "      <td>Phil 2: late 20s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21450</th>\n",
       "      <td>34</td>\n",
       "      <td>The Physiologist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23211</th>\n",
       "      <td>35</td>\n",
       "      <td>Ringo: from the 1960s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7533</th>\n",
       "      <td>36</td>\n",
       "      <td>Samantha: in her 20s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15916</th>\n",
       "      <td>37</td>\n",
       "      <td>Toby: A friendly party animal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6821</th>\n",
       "      <td>38</td>\n",
       "      <td>Tom: An outgoing man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1783</th>\n",
       "      <td>39</td>\n",
       "      <td>Vickie: a 10-year-old girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22462</th>\n",
       "      <td>40</td>\n",
       "      <td>Vietnam Vet: 1970-2008 war dreams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13673</th>\n",
       "      <td>40</td>\n",
       "      <td>Vietnam Vet: 2015 dreams</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      W266ID                               Name\n",
       "11171      1           Alta: a detailed dreamer\n",
       "7842       2                 Angie: age 18 & 20\n",
       "15181      3         Arlie: a middle-aged woman\n",
       "8222       4                       Barb Sanders\n",
       "8209       4                    Barb Sanders #2\n",
       "15894      5     Robert Bosnak: A dream analyst\n",
       "24910      6              Chris: a transvestite\n",
       "11756      7        Chuck: a physical scientist\n",
       "1545       8   Dahlia: concerns with appearance\n",
       "16737      9              David: teenage dreams\n",
       "24578     10       Dorothea: 53 years of dreams\n",
       "22189     11        Ed: dreams of his late wife\n",
       "9440      12                Edna: a blind woman\n",
       "20762     13      Elizabeth: a woman in her 40s\n",
       "2533      14           Emma: 48 years of dreams\n",
       "1542      15                     Emma's Husband\n",
       "12206     16         Esther: an adolescent girl\n",
       "13520     17                         Izzy (all)\n",
       "4040      18                      Jasmine (all)\n",
       "21971     19              Jeff: a lucid dreamer\n",
       "17734     20                    Joan: a lesbian\n",
       "20988     21                            Kenneth\n",
       "20664     22          Madeline 2: College Dorms\n",
       "12230     22          Madeline 4: After College\n",
       "5429      22             Madeline 3: Off-Campus\n",
       "3845      22            Madeline 1: High School\n",
       "13054     23              Mack: A poor recaller\n",
       "451       24                  Mark: a young boy\n",
       "24940     25              Melissa: a young girl\n",
       "15369     26                   Merri: an artist\n",
       "4425      27             Melora (Melvin's wife)\n",
       "21467     28          Melvin (Melora's husband)\n",
       "11189     29         Nancy: Caring & headstrong\n",
       "13952     30              The Natural Scientist\n",
       "309       31           Norman: a child molester\n",
       "12478     32          Pegasus: a factory worker\n",
       "972       33                      Phil 1: teens\n",
       "25460     33                 Phil 3: retirement\n",
       "16377     33                   Phil 2: late 20s\n",
       "21450     34                   The Physiologist\n",
       "23211     35              Ringo: from the 1960s\n",
       "7533      36               Samantha: in her 20s\n",
       "15916     37      Toby: A friendly party animal\n",
       "6821      38               Tom: An outgoing man\n",
       "1783      39         Vickie: a 10-year-old girl\n",
       "22462     40  Vietnam Vet: 1970-2008 war dreams\n",
       "13673     40           Vietnam Vet: 2015 dreams"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"W266ID\"]!=0][['W266ID', 'Name']].sort_values(by= [\"W266ID\"]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-b7707d65e875>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdreams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdreams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Convert to lowercase.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mdreams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdreams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Split into words.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mdreams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdreams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# tag with PoS\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mdreams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdreams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'N'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m   \u001b[1;31m# only keep nouns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Jay\\Anaconda2\\lib\\site-packages\\nltk\\tag\\__init__.pyc\u001b[0m in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    125\u001b[0m     \"\"\"\n\u001b[0;32m    126\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Jay\\Anaconda2\\lib\\site-packages\\nltk\\tag\\__init__.pyc\u001b[0m in \u001b[0;36m_pos_tag\u001b[1;34m(tokens, tagset, tagger)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m     \u001b[0mtagged_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mtagged_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en-ptb'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtagged_tokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Jay\\Anaconda2\\lib\\site-packages\\nltk\\tag\\perceptron.pyc\u001b[0m in \u001b[0;36mtag\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    154\u001b[0m             \u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m                 \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m                 \u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m             \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Jay\\Anaconda2\\lib\\site-packages\\nltk\\tag\\perceptron.pyc\u001b[0m in \u001b[0;36m_get_features\u001b[1;34m(self, i, word, context, prev, prev2)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i-2 word'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i+1 word'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i+1 suffix'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m         \u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i+2 word'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Jay\\Anaconda2\\lib\\site-packages\\nltk\\tag\\perceptron.pyc\u001b[0m in \u001b[0;36madd\u001b[1;34m(name, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m         '''\n\u001b[0;32m    250\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m             \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSTART\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dreams = list(df['Dream'])\n",
    "\n",
    "# Split the documents into tokens.\n",
    "for idx in range(len(dreams)):\n",
    "    dreams[idx] = dreams[idx].lower()  # Convert to lowercase.\n",
    "    dreams[idx] = nltk.tokenize.word_tokenize(dreams[idx])  # Split into words.\n",
    "    dreams[idx] = nltk.pos_tag(dreams[idx])  # tag with PoS\n",
    "    dreams[idx] = [token for token, tag in dreams[idx] if tag.startswith('N')]   # only keep nouns \n",
    "    \n",
    "# Remove numbers, but not words that contain numbers.\n",
    "dreams = [[token for token in dream if not token.isdigit()] for dream in dreams]\n",
    "\n",
    "# Remove words that are only one or two characters.\n",
    "dreams = [[token for token in dream if len(token) > 2] for dream in dreams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lemmatize the dreams.\n",
    "lmtzr = WordNetLemmatizer()\n",
    "dreams = [[lmtzr.lemmatize(token) for token in dream] for dream in dreams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rare and common tokens.\n",
    "\n",
    "# Create a dictionary representation of the dreams.\n",
    "dictionary = Dictionary(dreams)\n",
    "\n",
    "# Filter out words that occur less than 10 dreams, or more than 60% of the dreams.\n",
    "#dictionary.filter_extremes(no_below=10, no_above=0.6)\n",
    "\n",
    "# Vectorize data.\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(dream) for dream in dreams]\n",
    "\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of dreams: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__BELOW IS FROM: https://markroxor.github.io/gensim/static/notebooks/lda_training_tips.html__\n",
    "\n",
    "__HELPS EXPLAIN THE PARAMETERS__\n",
    "\n",
    "## Training\n",
    "\n",
    "*We are ready to train the LDA model. We will first discuss how to set some of the training parameters.*\n",
    "\n",
    "*First of all, the elephant in the room: how many topics do I need? There is really no easy answer for this, it will depend on both your data and your application. I have used 10 topics here because I wanted to have a few topics that I could interpret and \"label\", and because that turned out to give me reasonably good results. You might not need to interpret all your topics, so you could use a large number of topics, for example 100.*\n",
    "\n",
    "*The `chunksize` controls how many documents are processed at a time in the training algorithm. Increasing chunksize will speed up training, at least as long as the chunk of documents easily fit into memory. I've set chunksize = 2000, which is more than the amount of documents, so I process all the data in one go. Chunksize can however influence the quality of the model, as discussed in Hoffman and co-authors [2], but the difference was not substantial in this case.*\n",
    "\n",
    "*`passes` controls how often we train the model on the entire corpus. Another word for passes might be \"epochs\". iterations is somewhat technical, but essentially it controls how often we repeat a particular loop over each document. It is important to set the number of \"passes\" and \"iterations\" high enough.*\n",
    "\n",
    "*I suggest the following way to choose iterations and passes. First, enable logging (as described in many Gensim tutorials), and set eval_every = 1 in LdaModel. When training the model look for a line in the log that looks something like this: *\n",
    "\n",
    "    `2016-06-21 15:40:06,753 - gensim.models.ldamodel - DEBUG - 68/1566 documents converged within 400 iterations`\n",
    "\n",
    "*If you set `passes` = 20 you will see this line 20 times. Make sure that by the final passes, most of the documents have converged. So you want to choose both passes and iterations to be high enough for this to happen.*\n",
    "\n",
    "*We set `alpha = 'auto'` and `eta = 'auto'`. Again this is somewhat technical, but essentially we are automatically learning two parameters in the model that we usually would have to specify explicitly. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA model.\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 20 \n",
    "chunksize = 20000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "%time model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \\\n",
    "                       alpha='auto', eta='auto', iterations=iterations, num_topics=num_topics, \\\n",
    "                       passes=passes, eval_every=eval_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics = model.top_topics(corpus, topn=10)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape: num_topics x vocabulary_size\n",
    "# print model.get_topics()\n",
    "\n",
    "model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.groupby('W266ID').agg(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run Topic Modeling on each dreamer separately\n",
    "import time\n",
    "\n",
    "# dict to store topic models for each dreamer\n",
    "topic_models = dict()\n",
    "\n",
    "for dreamer_id in range(4,5):\n",
    "    loop_start_time = time.time()\n",
    "    \n",
    "    dreams = list(df[df['W266ID'] == dreamer_id]['Dream'])\n",
    "    print \"dreams: \" + str(dreams[0:10])\n",
    "    \n",
    "    # Split the documents into tokens.\n",
    "    for idx in range(len(dreams)):\n",
    "        dreams[idx] = dreams[idx].lower()  # Convert to lowercase.\n",
    "        dreams[idx] = nltk.tokenize.word_tokenize(dreams[idx])  # Split into words.\n",
    "        dreams[idx] = nltk.pos_tag(dreams[idx])  # tag with PoS\n",
    "        dreams[idx] = [token for token, tag in dreams[idx] if tag.startswith('N')]   # only keep nouns \n",
    "\n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    dreams = [[token for token in dream if not token.isdigit()] for dream in dreams]\n",
    "\n",
    "    # Remove words that are only one or two characters.\n",
    "    dreams = [[token for token in dream if len(token) > 2] for dream in dreams]\n",
    "\n",
    "    # Lemmatize the dreams.\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    dreams = [[lmtzr.lemmatize(token) for token in dream] for dream in dreams]\n",
    "\n",
    "    print \"\\n\"\n",
    "    print \"lemmatized dreams: \" + str(dreams[0:10])\n",
    "    \n",
    "    ## Remove rare and common tokens.\n",
    "\n",
    "    # Create a dictionary representation of the dreams.\n",
    "    dictionary = Dictionary(dreams)\n",
    "\n",
    "    # Filter out words that occur less than 4 dreams\n",
    "    dictionary.filter_extremes(no_below=3, no_above=0.3)\n",
    "\n",
    "    # Vectorize data.\n",
    "    # Bag-of-words representation of the documents.\n",
    "    corpus = [dictionary.doc2bow(dream) for dream in dreams]\n",
    "\n",
    "    print \"\\n\"\n",
    "    print \"corpus: \" + str(corpus[0:10])\n",
    "    \n",
    "    # Set training parameters.\n",
    "    num_topics = 50  # Was 10\n",
    "    chunksize = 3000\n",
    "    passes = 20\n",
    "    iterations = 400\n",
    "    eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "    # Make a index to word dictionary.\n",
    "    temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "    id2word = dictionary.id2token\n",
    "\n",
    "    mode_start_time = time.time()\n",
    "    model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \\\n",
    "                           alpha='auto', eta='auto', iterations=iterations, num_topics=num_topics, \\\n",
    "                           passes=passes, eval_every=eval_every)\n",
    "    print 'DreamerID', str(dreamer_id) + ': Total cell time=' + str(time.time() - loop_start_time), \\\n",
    "            '\\t Model build time=' + str(time.time() - mode_start_time)\n",
    "    \n",
    "    topic_models[dreamer_id] = (model, corpus, dictionary)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in topic_models.iteritems():\n",
    "    print \"\\nDreamer\", k\n",
    "    pprint(v[0].top_topics(corpus = v[1], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dreams: ['I dreamt that I am Sebastian, the demon butler from the anime Black Butler.', 'I dreamt that I am Sebastian, the demon butler from the anime Black Butler.', 'I dreamt that I am Sebastian, the demon butler from the anime Black Butler.', 'Can you tell I like anime?', 'I dreamt that I am Sebastian, the demon butler from the anime Black Butler.', 'I dreamt that I am Sebastian, the demon butler from the anime Black Butler.', 'I dreamt that I am Sebastian, the demon butler from the anime Black Butler.', 'I dreamt that I am Sebastian, the demon butler from the anime Black Butler.', 'I dreamt that I am Sebastian, the demon butler from the anime Black Butler.', 'I dreamt that I am Sebastian, the demon butler from the anime Black Butler.', 'Okay, I am also crazy about the video game Perfect World.']\n",
      "\n",
      "\n",
      "lemmatized dreams: [['demon', 'butler', 'butler'], ['demon', 'butler', 'butler'], ['demon', 'butler', 'butler'], ['anime'], ['demon', 'butler', 'butler'], ['demon', 'butler', 'butler'], ['demon', 'butler', 'butler'], ['demon', 'butler', 'butler'], ['demon', 'butler', 'butler'], ['demon', 'butler', 'butler'], ['okay', 'video', 'game', 'world']]\n",
      "\n",
      "Dreamer 666\n",
      "[([(0.55731887253135459, u'butler'),\n",
      "   (0.28067511894251368, u'demon'),\n",
      "   (0.032664463848481137, u'anime'),\n",
      "   (0.032338535819832592, u'okay'),\n",
      "   (0.032338213524431336, u'video')],\n",
      "  -21.504835420904651),\n",
      " ([(0.55768824077722878, u'butler'),\n",
      "   (0.28071390781555161, u'demon'),\n",
      "   (0.03256347599025456, u'anime'),\n",
      "   (0.032261512740158864, u'okay'),\n",
      "   (0.032261214058138643, u'video')],\n",
      "  -21.504835420904651)]\n"
     ]
    }
   ],
   "source": [
    "## Toy Example starts here...\n",
    "from pprint import pprint\n",
    "\n",
    "# dict to store topic models for each dreamer\n",
    "topic_models = dict()\n",
    "\n",
    "dreams = [\"I dreamt that I am Sebastian, the demon butler from the anime Black Butler.\",\"I dreamt that I am Sebastian, the demon butler from the anime Black Butler.\",\"I dreamt that I am Sebastian, the demon butler from the anime Black Butler.\",\"Can you tell I like anime?\", \"I dreamt that I am Sebastian, the demon butler from the anime Black Butler.\",\"I dreamt that I am Sebastian, the demon butler from the anime Black Butler.\",\"I dreamt that I am Sebastian, the demon butler from the anime Black Butler.\",\"I dreamt that I am Sebastian, the demon butler from the anime Black Butler.\",\"I dreamt that I am Sebastian, the demon butler from the anime Black Butler.\",\"I dreamt that I am Sebastian, the demon butler from the anime Black Butler.\", \"Okay, I am also crazy about the video game Perfect World.\"]\n",
    "print \"dreams: \" + str(dreams[0:20])\n",
    "\n",
    "# Split the documents into tokens.\n",
    "for idx in range(len(dreams)):\n",
    "    dreams[idx] = dreams[idx].lower()  # Convert to lowercase.\n",
    "    dreams[idx] = nltk.tokenize.word_tokenize(dreams[idx])  # Split into words.\n",
    "    dreams[idx] = nltk.pos_tag(dreams[idx])  # tag with PoS\n",
    "    dreams[idx] = [token for token, tag in dreams[idx] if tag.startswith('N')]   # only keep nouns \n",
    "\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "dreams = [[token for token in dream if not token.isdigit()] for dream in dreams]\n",
    "\n",
    "# Remove words that are only one or two characters.\n",
    "dreams = [[token for token in dream if len(token) > 2] for dream in dreams]\n",
    "\n",
    "# Lemmatize the dreams.\n",
    "lmtzr = WordNetLemmatizer()\n",
    "dreams = [[lmtzr.lemmatize(token) for token in dream] for dream in dreams]\n",
    "\n",
    "print \"\\n\"\n",
    "print \"lemmatized dreams: \" + str(dreams[0:20])\n",
    "\n",
    "# Create a dictionary representation of the dreams.\n",
    "dictionary = Dictionary(dreams)\n",
    "\n",
    "# Filter out words that occur less than 4 dreams\n",
    "#dictionary.filter_extremes(no_below=3, no_above=0.3)\n",
    "\n",
    "# Vectorize data.\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(dream) for dream in dreams]\n",
    "\n",
    "#print \"corpus: \" + str(corpus[0:100])\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 2  # Was 10\n",
    "chunksize = 3000\n",
    "passes = 200\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "mode_start_time = time.time()\n",
    "model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \\\n",
    "                       alpha='auto', eta='auto', iterations=iterations, num_topics=num_topics, \\\n",
    "                       passes=passes, eval_every=eval_every)\n",
    "\n",
    "topic_models[666] = (model, corpus, dictionary)\n",
    "    \n",
    "for k, v in topic_models.iteritems():\n",
    "    print \"\\nDreamer\", k\n",
    "    pprint(v[0].top_topics(corpus = v[1], topn=5))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Dream collections from individuals---\n",
      "\n",
      "{alta} Alta: a detailed dreamer (422 dreams) [F]\n",
      "w266 ID: 1\n",
      "\n",
      "\n",
      "{angie} Angie: age 18 & 20 (48 dreams) [F]\n",
      "w266 ID: 2\n",
      "\n",
      "\n",
      "{arlie} Arlie: a middle-aged woman (212 dreams) [F]\n",
      "w266 ID: 3\n",
      "\n",
      "\n",
      "{b} Barb Sanders (3116 dreams) [F]\n",
      "w266 ID: 4\n",
      "{b2} Barb Sanders #2 (1138 dreams) [F]\n",
      "w266 ID: 4\n",
      "\n",
      "\n",
      "{bosnak} Robert Bosnak: A dream analyst (53 dreams) [M]\n",
      "w266 ID: 5\n",
      "\n",
      "\n",
      "{chris} Chris: a transvestite (100 dreams) [M]\n",
      "w266 ID: 6\n",
      "\n",
      "\n",
      "{chuck} Chuck: a physical scientist (75 dreams) [M]\n",
      "w266 ID: 7\n",
      "\n",
      "\n",
      "{dahlia} Dahlia: concerns with appearance (24 dreams) [F]\n",
      "w266 ID: 8\n",
      "\n",
      "\n",
      "{david} David: teenage dreams (166 dreams) [M]\n",
      "w266 ID: 9\n",
      "\n",
      "\n",
      "{dorothea} Dorothea: 53 years of dreams (900 dreams) [F]\n",
      "w266 ID: 10\n",
      "\n",
      "\n",
      "{ed} Ed: dreams of his late wife (143 dreams) [M]\n",
      "w266 ID: 11\n",
      "\n",
      "\n",
      "{edna} Edna: a blind woman (19 dreams) [F]\n",
      "w266 ID: 12\n",
      "\n",
      "\n",
      "{elizabeth} Elizabeth: a woman in her 40s (1707 dreams) [F]\n",
      "w266 ID: 13\n",
      "\n",
      "\n",
      "{emma} Emma: 48 years of dreams (1521 dreams) [F]\n",
      "w266 ID: 14\n",
      "\n",
      "\n",
      "{emmas_husband} Emma's Husband (72 dreams) [M]\n",
      "w266 ID: 15\n",
      "\n",
      "\n",
      "{esther} Esther: an adolescent girl (110 dreams) [F]\n",
      "w266 ID: 16\n",
      "\n",
      "\n",
      "{izzy} Izzy (all) (4352 dreams) [F]\n",
      "w266 ID: 17\n",
      "\n",
      "\n",
      "{jasmine} Jasmine (all) (664 dreams) [F]\n",
      "w266 ID: 18\n",
      "\n",
      "\n",
      "{jeff} Jeff: a lucid dreamer (87 dreams) [M]\n",
      "w266 ID: 19\n",
      "\n",
      "\n",
      "{joan} Joan: a lesbian (42 dreams) [F]\n",
      "w266 ID: 20\n",
      "\n",
      "\n",
      "{kenneth} Kenneth (2022 dreams) [M]\n",
      "w266 ID: 21\n",
      "\n",
      "\n",
      "{madeline1-hs} Madeline 1: High School (98 dreams) [F]\n",
      "w266 ID: 22\n",
      "{madeline2-dorms} Madeline 2: College Dorms (186 dreams) [F]\n",
      "w266 ID: 22\n",
      "{madeline3-offcampus} Madeline 3: Off-Campus (348 dreams) [F]\n",
      "w266 ID: 22\n",
      "{madeline4-postgrad} Madeline 4: After College (294 dreams) [F]\n",
      "w266 ID: 22\n",
      "\n",
      "\n",
      "{mack} Mack: A poor recaller (38 dreams) [M]\n",
      "w266 ID: 23\n",
      "\n",
      "\n",
      "{mark} Mark: a young boy (23 dreams) [M]\n",
      "w266 ID: 24\n",
      "\n",
      "\n",
      "{melissa} Melissa: a young girl (89 dreams) [F]\n",
      "w266 ID: 25\n",
      "\n",
      "\n",
      "{merri} Merri: an artist (315 dreams) [F]\n",
      "w266 ID: 26\n",
      "\n",
      "\n",
      "{melora} Melora (Melvin's wife) (211 dreams) [F]\n",
      "w266 ID: 27\n",
      "\n",
      "\n",
      "{melvin} Melvin (Melora's husband) (128 dreams) [M]\n",
      "w266 ID: 28\n",
      "\n",
      "\n",
      "{nancy} Nancy: Caring & headstrong (44 dreams) [F]\n",
      "w266 ID: 29\n",
      "\n",
      "\n",
      "{natural_scientist} The Natural Scientist (234 dreams) [M]\n",
      "w266 ID: 30\n",
      "\n",
      "\n",
      "{norman} Norman: a child molester (1235 dreams) [M]\n",
      "w266 ID: 31\n",
      "\n",
      "\n",
      "{pegasus} Pegasus: a factory worker (1093 dreams) [M]\n",
      "w266 ID: 32\n",
      "\n",
      "\n",
      "{phil1} Phil 1: teens (106 dreams) [M]\n",
      "w266 ID: 33\n",
      "{phil2} Phil 2: late 20s (220 dreams) [M]\n",
      "w266 ID: 33\n",
      "{phil3} Phil 3: retirement (180 dreams) [M]\n",
      "w266 ID: 33\n",
      "\n",
      "\n",
      "{physiologist} The Physiologist (86 dreams) [M]\n",
      "w266 ID: 34\n",
      "\n",
      "\n",
      "{ringo} Ringo: from the 1960s (16 dreams) [M]\n",
      "w266 ID: 35\n",
      "\n",
      "\n",
      "{samantha} Samantha: in her 20s (63 dreams) [F]\n",
      "w266 ID: 36\n",
      "\n",
      "\n",
      "{toby} Toby: A friendly party animal (33 dreams) [M]\n",
      "w266 ID: 37\n",
      "\n",
      "\n",
      "{tom} Tom: An outgoing man (27 dreams) [M]\n",
      "w266 ID: 38\n",
      "\n",
      "\n",
      "{vickie} Vickie: a 10-year-old girl (35 dreams) [F]\n",
      "w266 ID: 39\n",
      "\n",
      "\n",
      "{vietnam_vet} Vietnam Vet: 1970-2008 war dreams (98 dreams) [M]\n",
      "w266 ID: 40\n",
      "{vietnam_vet2} Vietnam Vet: 2015 dreams (32 dreams) [M]\n",
      "w266 ID: 40\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#BoW Toy example\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from __future__ import division\n",
    "from random import shuffle\n",
    "import xmltodict\n",
    "import json\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import csv\n",
    "\n",
    "# view all columns of pandas df\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "def convert(data):\n",
    "    if isinstance(data, basestring):\n",
    "        return str(data)\n",
    "    elif isinstance(data, collections.Mapping):\n",
    "        return dict(map(convert, data.iteritems()))\n",
    "    elif isinstance(data, collections.Iterable):\n",
    "        return type(data)(map(convert, data))\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "def left(s, amount):\n",
    "    return s[:amount]\n",
    "\n",
    "def right(s, amount):\n",
    "    return s[-amount:]\n",
    "\n",
    "def mid(s, offset, amount):\n",
    "    return s[offset:offset+amount]\n",
    "\n",
    "with open (\"dreambank-public.xml\") as f:\n",
    "    doc = xmltodict.parse(f.read())\n",
    "\n",
    "print '---Dream collections from individuals---' + '\\n'\n",
    "MultIDs = ['b', 'madeline1-hs', 'madeline2-dorms', 'madeline3-offcampus', 'phil1', 'phil2', 'vietnam_vet']\n",
    "NumberOfSeries = 1\n",
    "\n",
    "for dreamer in doc['dreambank']['collection']:\n",
    "    if dreamer['type'] == 'series':\n",
    "        print '{' + dreamer['id']  + '} ' + dreamer['name'] + ' (' + str(len(dreamer['dream'][:])) + ' dreams)' + ' [' + dreamer['sex'] + ']'    \n",
    "        dreamer['w266ID'] = NumberOfSeries\n",
    "        \n",
    "        # Assign a dreamer ID that groups the same dreamers together \n",
    "        # and skips the dream collections of multiple dreamers\n",
    "        print 'w266 ID: ' + str(dreamer['w266ID'])\n",
    "        if dreamer['id'] not in MultIDs:\n",
    "            print '\\n'\n",
    "            NumberOfSeries += 1\n",
    "    else:\n",
    "        dreamer['w266ID'] = 0\n",
    "\n",
    "ID = 0\n",
    "W266ID = 0\n",
    "DreamBankID = ''\n",
    "DreamNumber = ''\n",
    "Name = ''\n",
    "Sex = ''\n",
    "Dream = ''\n",
    "HasDream = 0\n",
    "\n",
    "df = pd.DataFrame(columns=[\"ID\", \"W266ID\", \"DreamBankID\", \"DreamNumber\", \"Name\", \"Sex\", \"Dream\"])\n",
    "\n",
    "for dreamer in doc['dreambank']['collection']:\n",
    "    Name = dreamer['name'] \n",
    "    DreamBankID = dreamer['id']\n",
    "    W266ID = int(dreamer['w266ID'])\n",
    "    Sex = dreamer['sex']\n",
    "    \n",
    "    for odict in dreamer['dream']:\n",
    "        HasDream = 0\n",
    "        for key, value in odict.items():\n",
    "            if convert(key) == 'report':\n",
    "                Dream = convert(value)\n",
    "                HasDream = 1\n",
    "            if convert(key) == 'number':\n",
    "                DreamNumber = convert(value)\n",
    "        \n",
    "        if HasDream == 1:\n",
    "            ID += 1\n",
    "            df = df.append({\n",
    "                \"ID\": ID,\n",
    "                \"W266ID\": W266ID,\n",
    "                \"DreamBankID\": DreamBankID,\n",
    "                \"DreamNumber\": DreamNumber,\n",
    "                \"Name\": Name,\n",
    "                \"Sex\": Sex,\n",
    "                \"Dream\": Dream\n",
    "                }, ignore_index=True)        \n",
    "#     print '\\n'\n",
    "\n",
    "# print df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'black': 143, 'butler': 123, 'dad': 104, 'one': 99, 'said': 89, 'mom': 89, 'people': 79, 'got': 73, 'back': 70, 'sebastian': 66, 'found': 65, 'like': 64, 'us': 61, 'could': 58, 'girl': 57, 'world': 55, 'came': 53, 'house': 53, 'time': 51, 'something': 51, 'room': 48, 'water': 46, 'made': 44, 'school': 43, 'went': 43, 'around': 43, 'two': 43, 'told': 42})\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6gAAALRCAYAAABWG46VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x319a3e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "#nltk.download()  #Need to download for the stopwords\n",
    "\n",
    "Name = \"Sebastian\"\n",
    "DreamBankID = 666\n",
    "W266ID = 666\n",
    "Sex = \"M\"\n",
    "dreams = []\n",
    "\n",
    "#for n in range(50):\n",
    "#    dreams.append(\"I dreamt that I am Sebastian, the demon butler from the anime Black Butler.\")\n",
    "    \n",
    "#dreams.append(\"Okay, I am also crazy about the video game Perfect World.\")\n",
    "\n",
    "with open('BassyDreamJournal2.txt') as inputfile:\n",
    "    for row in csv.reader(inputfile):\n",
    "        strRow = str(row).lower()\n",
    "        strRow = re.sub('[\\W_]+', ' ', strRow, flags=re.UNICODE) \n",
    "        \n",
    "        dreams.append(str(strRow).lower())\n",
    "\n",
    "dreamsgraph = \" \".join(dreams)        \n",
    "dreamsgraph = list(nltk.tokenize.word_tokenize(dreamsgraph))\n",
    "dreamsgraph = [word for word in dreamsgraph if word not in stopwords.words('english')]\n",
    "dreamsgraph = [word for word in dreamsgraph if word not in ['x94', 'x92t', 'ti', 'x92d', 'x92s', 'x85']]\n",
    "\n",
    "counts = Counter(dreamsgraph)\n",
    "for k in list(counts):\n",
    "    if counts[k] < 40:\n",
    "        del counts[k]\n",
    "\n",
    "print counts\n",
    "\n",
    "labels, values = zip(*counts.items())        \n",
    "\n",
    "# sort in descending order\n",
    "indSort = np.argsort(values)[::-1]\n",
    "\n",
    "labels = np.array(labels)[indSort]\n",
    "values = np.array(values)[indSort]\n",
    "indexes = np.arange(len(labels))\n",
    "\n",
    "plt.bar(indexes, values, align='center', alpha=0.5, color='blue')\n",
    "plt.xticks(indexes, labels, rotation=45) #+ bar_width\n",
    "\n",
    "# Get current size\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "# print \"Current size:\", fig_size\n",
    "\n",
    "# Set figure width to 12 and height to 9\n",
    "fig_size[0] = 16\n",
    "fig_size[1] = 12\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Dreams: 26859\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TempDream = []\n",
    "\n",
    "for Dream in dreams:\n",
    "    ID += 1\n",
    "    \n",
    "    TempDream = list(nltk.tokenize.word_tokenize(Dream))\n",
    "    TempDream = [word for word in TempDream if word not in stopwords.words('english')]\n",
    "    TempDream = [word for word in TempDream if word not in ['x93i', 'x94', 'x92t', 'ti', 'x92d', 'x92s', 'x85']]\n",
    "        \n",
    "    TempDream = \" \".join(TempDream)                \n",
    "    \n",
    "    # print TempDream\n",
    "    \n",
    "    Dream = TempDream\n",
    "\n",
    "    df = df.append({\n",
    "        \"ID\": ID,\n",
    "        \"W266ID\": W266ID,\n",
    "        \"DreamBankID\": DreamBankID,\n",
    "        \"DreamNumber\": 665 + ID,\n",
    "        \"Name\": Name,\n",
    "        \"Sex\": Sex,\n",
    "        \"Dream\": Dream\n",
    "        }, ignore_index=True)        \n",
    "\n",
    "    \n",
    "# randomly shuffle dataframe\n",
    "# set seed for consistency while running\n",
    "np.random.seed(0)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "# add 41 flag columns denoting dreamers (one col per dreamer) \n",
    "dreamer_flag = pd.get_dummies(df['W266ID'], prefix='Dreamer')\n",
    "df = pd.concat([df, dreamer_flag], axis=1)\n",
    "\n",
    "# create vocab from all dreams\n",
    "dreams_flat = df['Dream'].values.flatten().tolist()\n",
    "dreams_list = \" \".join(dreams_flat)\n",
    "vocab = list(set(nltk.tokenize.word_tokenize(dreams_list)))\n",
    "\n",
    "print \"Total Dreams: \" + str(ID)\n",
    "print \"\\n\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dreams: 680\n",
      "Total number in DreamBank (training): 21487\n",
      "\n",
      "\n",
      "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None,\n",
      "        vocabulary=['fawn', 'Ptomeny', 'grandniece', '1,800', 'wrought-iron', 'Poetry', \"'glue\", 'woods', 'clotted', 'spiders', 'hanging', 'woody', 'comically', 'localized', 'Prone', 'sevens', 'disobeying', 'caner', 'canes', 'DISAPPOINTED', 'unpaused', 'scold', 'jell-o', 'tingle', 'Western', 'Retreat', 'boc... 'flatfish', 'up-side-down', 'Palides', 'Choquehuanca', 'her/or', 'architect', 'expands', 'jawbone'])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-f45b577fa095>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m## Logistic reg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mbow_log\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mbow_log\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec_bow_train_data\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec_bow_train_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m# find the most-predictive features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Jay\\Anaconda2\\lib\\site-packages\\numpy\\core\\fromnumeric.pyc\u001b[0m in \u001b[0;36mstd\u001b[1;34m(a, axis, dtype, out, ddof, keepdims)\u001b[0m\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m     return _methods._std(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n\u001b[1;32m-> 3027\u001b[1;33m                          **kwargs)\n\u001b[0m\u001b[0;32m   3028\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3029\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Jay\\Anaconda2\\lib\\site-packages\\numpy\\core\\_methods.pyc\u001b[0m in \u001b[0;36m_std\u001b[1;34m(a, axis, dtype, out, ddof, keepdims)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_std\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mddof\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n\u001b[1;32m--> 135\u001b[1;33m                keepdims=keepdims)\n\u001b[0m\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Jay\\Anaconda2\\lib\\site-packages\\numpy\\core\\_methods.pyc\u001b[0m in \u001b[0;36m_var\u001b[1;34m(a, axis, dtype, out, ddof, keepdims)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[0mrcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_count_reduce_items\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m     \u001b[1;31m# Make this warning show up on top.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mddof\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mrcount\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Jay\\Anaconda2\\lib\\site-packages\\numpy\\core\\_methods.pyc\u001b[0m in \u001b[0;36m_count_reduce_items\u001b[1;34m(arr, axis)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mitems\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "def split_data(df, W266ID, train=0.80):        \n",
    "    # column for \"our\" dreamer\n",
    "    dreamer_label = 'Dreamer_' + str(W266ID)\n",
    "\n",
    "    # make 60/40 split of train/test\n",
    "    # test will be evenly split between dev and test in the next step\n",
    "    num_train = int(len(df) * train)\n",
    "    num_test = int(len(df) * (1-train)) \n",
    "\n",
    "    train_data, train_labels = df['Dream'][:num_train], df[dreamer_label][:num_train]\n",
    "    dev_data, dev_labels = df['Dream'][-num_test : -num_test//2], df[dreamer_label][-num_test : -num_test//2] \n",
    "    test_data, test_labels = df['Dream'][-num_test//2:], df[dreamer_label][-num_test//2:]\n",
    "\n",
    "    return train_data, train_labels, dev_data, dev_labels, test_data, test_labels\n",
    "\n",
    "ToyModel = {}\n",
    "\n",
    "# split data for dreamer666 - Sebastian\n",
    "DreamerID = 666\n",
    "\n",
    "train_data, train_labels, dev_data, dev_labels, test_data, test_labels = split_data(df, W266ID=DreamerID)\n",
    "\n",
    "print 'Number of dreams: ' + str(sum(train_labels))\n",
    "print 'Total number in DreamBank (training): ' + str(len(train_labels))\n",
    "print '\\n'\n",
    "\n",
    "# Create a Bag-of-Words Vectorizer\n",
    "vec = CountVectorizer(vocabulary=vocab)\n",
    "print vec\n",
    "\n",
    "vec_bow_train_data = vec.fit_transform(train_data)\n",
    "vec_bow_dev_data = vec.transform(dev_data)  \n",
    "\n",
    "## Logistic reg\n",
    "bow_log = LogisticRegression(C = 0.01)\n",
    "bow_log.fit(vec_bow_train_data / np.std(vec_bow_train_data, 0), train_labels)\n",
    "\n",
    "# find the most-predictive features\n",
    "### we're not using the weights currently, but might be useful/interesting later ###    \n",
    "best_feature_positions = bow_log.coef_.argsort()[0][-10::]\n",
    "best_feature_weights = bow_log.coef_[0][best_feature_positions.astype(int)]\n",
    "\n",
    "print best_feature_positions\n",
    "print best_feature_weights\n",
    "\n",
    "f1_bow_lr_score = metrics.f1_score(dev_labels, bow_log.predict(vec_bow_dev_data), average='macro')\n",
    "\n",
    "# get word labels for our features\n",
    "words = []\n",
    "for ft in best_feature_positions.astype(int):\n",
    "    words.append(vec.get_feature_names()[ft])\n",
    "\n",
    "ToyModel[DreamerID] = (bow_log, f1_bow_lr_score, words)\n",
    "\n",
    "for key, (model, score, predictive_words) in ToyModel.iteritems():\n",
    "    print model \n",
    "    print '\\n'\n",
    "    print 'F1 score: ' + str(score)\n",
    "    print 'W266ID='+str(key), '\\tMost Predictive', predictive_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progression: Logistic Regression for dreamer  666\n",
      "58623\n",
      "(21487, 58623)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "Cell Runtime: 9.0 seconds\n"
     ]
    }
   ],
   "source": [
    "#==================================================================\n",
    "# Cell Purpose:  Run Logisitic Regression for each dreamer\n",
    "#                This creates a logistic Regression for Each Dreamer\n",
    "#                with a \"one\" versus \"rest\" methodology\n",
    "#==================================================================\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "PROGRESSION_PRINTOUTS_ON = True\n",
    "\n",
    "cell_start = time.time()\n",
    "\n",
    "models = {}\n",
    "\n",
    "for i in range(666,667):\n",
    "    if PROGRESSION_PRINTOUTS_ON:\n",
    "        print \"Progression: Logistic Regression for dreamer \", i\n",
    "    # split data\n",
    "    train_data, train_labels, dev_data, dev_labels, test_data, test_labels = split_data(df, W266ID=i)\n",
    "    \n",
    "    # Create a Bag-of-Words Vectorizer\n",
    "    vec_bow = CountVectorizer(vocabulary=vocab)\n",
    "    print len(vocab)\n",
    "    \n",
    "    vec_bow_train_data = vec_bow.fit_transform(train_data)\n",
    "    # vec_bow_train_data = preprocessing.scale(vec_bow_train_data)  # how to do this?\n",
    "    vec_bow_dev_data = vec_bow.transform(dev_data)  \n",
    "    print vec_bow_train_data.shape\n",
    "    print type(vec_bow_train_data)\n",
    "    \n",
    "    #scaler = StandardScaler(with_mean=False)\n",
    "    #vec_bow_train_data = scaler.fit(vec_bow_train_data)\n",
    "    \n",
    "    # Create a TF Vectorizer\n",
    "    vec_tf = TfidfTransformer(use_idf=False, smooth_idf=False)\n",
    "    vec_tf_train_data = vec_tf.fit_transform(vec_bow_train_data)\n",
    "    vec_tf_dev_data   = vec_tf.transform(vec_bow_dev_data)     \n",
    "\n",
    "    # Create a TF-IDF Vectorizer\n",
    "    vec_tfidf = TfidfVectorizer(stop_words='english')\n",
    "    vec_tfidf_train_data = vec_tfidf.fit_transform(train_data)\n",
    "    vec_tfidf_dev_data   = vec_tfidf.transform(dev_data)    \n",
    "\n",
    "    ## Logistic reg\n",
    "    bow_log = LogisticRegression(C = 100)\n",
    "    #print vec_bow_train_data\n",
    "    \n",
    "    # std_bow_train_data = vec_bow_train_data / np.std(vec_bow_train_data, 0) 'Tuple out of range\n",
    "    bow_log.fit(vec_bow_train_data, train_labels)\n",
    "\n",
    "    tf_log = LogisticRegression(C = 100)\n",
    "    tf_log.fit(vec_tf_train_data, train_labels)\n",
    "    \n",
    "    tfidf_log = LogisticRegression(C = 100)\n",
    "    tfidf_log.fit(vec_tfidf_train_data, train_labels)    \n",
    "    \n",
    "    # score models\n",
    "    f1_bow = metrics.f1_score(dev_labels, bow_log.predict(vec_bow_dev_data), average='macro')\n",
    "    f1_tf = metrics.f1_score(dev_labels, tf_log.predict(vec_tf_dev_data), average='macro')    \n",
    "    f1_tfidf = metrics.f1_score(dev_labels, tfidf_log.predict(vec_tfidf_dev_data), average='macro')\n",
    "\n",
    "    \n",
    "    # find the most-predictive features\n",
    "    ### we're not using the weights currently, but might be useful/interesting later ###    \n",
    "    best_feature_positions_bow = bow_log.coef_.argsort()[0][-10::]\n",
    "    best_feature_weights_bow = bow_log.coef_[0][best_feature_positions_bow.astype(int)]\n",
    "    \n",
    "    best_feature_positions_tf = tf_log.coef_.argsort()[0][-10::]\n",
    "    best_feature_weights_tf = tf_log.coef_[0][best_feature_positions_tf.astype(int)]  \n",
    "    \n",
    "    best_feature_positions_tfidf = tfidf_log.coef_.argsort()[0][-10::]\n",
    "    best_feature_weights_tfidf = tfidf_log.coef_[0][best_feature_positions_tfidf.astype(int)]    \n",
    "\n",
    "    # get word labels for our features\n",
    "    bow_best_words = []\n",
    "    for ft in best_feature_positions_bow.astype(int):\n",
    "        bow_best_words.append(vec_bow.get_feature_names()[ft])\n",
    "        \n",
    "    tf_best_words = []\n",
    "    for ft in best_feature_positions_tf.astype(int):\n",
    "        tf_best_words.append(vec_bow.get_feature_names()[ft])\n",
    "        \n",
    "    tfidf_best_words = []\n",
    "    for ft in best_feature_positions_tfidf.astype(int):\n",
    "        tfidf_best_words.append(vec_tfidf.get_feature_names()[ft])\n",
    "    \n",
    "    # reverse-sort lists so they are in proper order\n",
    "    bow_best_words.reverse()\n",
    "    tf_best_words.reverse()\n",
    "    tfidf_best_words.reverse()    \n",
    "    \n",
    "    models[i] = (bow_log, tf_log, tfidf_log, bow_best_words, tf_best_words, tfidf_best_words)\n",
    "    \n",
    "print 'Cell Runtime:', round(time.time() - cell_start), 'seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W266ID=666\n",
      "Most Predictive BOW ['everybody', 'jaylah', 'black', 'mom', 'piano', 'dad', 'art', 'dance', 'could', 'staring']\n",
      "Most Predictive TF ['everybody', 'black', 'dad', 'mom', 'jaylah', 'girl', 'could', 'butler', 'grammy', 'staring']\n",
      "Most Predictive TF-IDF [u'butler', u'jaylah', u'grammy', u'staring', u'micheo', u'everybody', u'dad', u'parents', u'deciding', u'robo'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#==================================================================\n",
    "# Cell purpse: Show the most predictive words from models per dreamer\n",
    "#              BOW\n",
    "#              TF (term frequence)\n",
    "#              TFIDF\n",
    "#==================================================================\n",
    "\n",
    "for key, (bow_mod, tf_mod, tfidf_mod, bow_best_words, tf_best_words, tfidf_best_words) in models.iteritems():\n",
    "    print 'W266ID='+str(key)\n",
    "    print 'Most Predictive BOW', bow_best_words\n",
    "    print 'Most Predictive TF', tf_best_words    \n",
    "    print 'Most Predictive TF-IDF', tfidf_best_words, '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#==================================================================\n",
    "# Cell Purpose: Set up the df to do Topic Modeling \n",
    "#               Split the dream documents into tokens\n",
    "#==================================================================\n",
    "\n",
    "dreams = list(df['Dream'])\n",
    "\n",
    "# Split the documents into tokens.\n",
    "for idx in range(len(dreams)):\n",
    "    dreams[idx] = dreams[idx].lower()  # Convert to lowercase.\n",
    "    dreams[idx] = nltk.tokenize.word_tokenize(dreams[idx])  # Split into words.\n",
    "    dreams[idx] = nltk.pos_tag(dreams[idx])  # tag with PoS\n",
    "    dreams[idx] = [token for token, tag in dreams[idx] if tag.startswith('N')]   # only keep nouns \n",
    "    \n",
    "# Remove numbers, but not words that contain numbers.\n",
    "dreams = [[token for token in dream if not token.isdigit()] for dream in dreams]\n",
    "\n",
    "# Remove words that are only one or two characters.\n",
    "dreams = [[token for token in dream if len(token) > 2] for dream in dreams]\n",
    "\n",
    "# Lemmatize the dreams.\n",
    "lmtzr = WordNetLemmatizer()\n",
    "dreams = [[lmtzr.lemmatize(token) for token in dream] for dream in dreams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 8025\n",
      "Number of dreams: 26859\n"
     ]
    }
   ],
   "source": [
    "# Remove rare and common tokens.\n",
    "# Create a dictionary representation of the dreams.\n",
    "dictionary = Dictionary(dreams)\n",
    "\n",
    "# Filter out words that occur less than 5 dreams, or more than 60% of the dreams.\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.6)\n",
    "\n",
    "# Vectorize data.\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(dream) for dream in dreams]\n",
    "\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of dreams: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24min 29s\n"
     ]
    }
   ],
   "source": [
    "# Train LDA model.\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 50\n",
    "chunksize = 20000\n",
    "passes = 30\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "%time model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \\\n",
    "                       alpha='auto', eta='auto', iterations=iterations, num_topics=num_topics, \\\n",
    "                       passes=passes, eval_every=eval_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -3.2884.\n",
      "[([(0.081193713009566806, u'/td'),\n",
      "   (0.076299563195520864, u'question'),\n",
      "   (0.065797518368660504, u'answer'),\n",
      "   (0.048316795999480994, u'top'),\n",
      "   (0.047715057179341165, u'table'),\n",
      "   (0.046807409069452514, u'interpretation'),\n",
      "   (0.042630346650579277, u'cellpadding='),\n",
      "   (0.042630346562144796, u'cellspacing='),\n",
      "   (0.042630346550083083, u'border='),\n",
      "   (0.042630346529837201, u'valign=')],\n",
      "  -0.4496401556876049),\n",
      " ([(0.098768273053005423, u'/li'),\n",
      "   (0.087843874831823821, u'feeling'),\n",
      "   (0.068235128596770431, u'character'),\n",
      "   (0.066262788129135941, u'thought'),\n",
      "   (0.060894074846381624, u'setting'),\n",
      "   (0.0419289547951225, u'friend'),\n",
      "   (0.035680204931752543, u'/ul'),\n",
      "   (0.031443061109124738, u'dream'),\n",
      "   (0.019848146942933347, u'school'),\n",
      "   (0.017836319716013238, u'mom')],\n",
      "  -1.1783086339887989),\n",
      " ([(0.12795201629778055, u'guy'),\n",
      "   (0.037657976636868651, u'people'),\n",
      "   (0.036978979804724359, u'mom'),\n",
      "   (0.033517459848246701, u'something'),\n",
      "   (0.024158197460760322, u'thing'),\n",
      "   (0.021793570545332708, u'girl'),\n",
      "   (0.021276271999509891, u'someone'),\n",
      "   (0.019532201153711092, u'ezra'),\n",
      "   (0.017565069913702956, u'car'),\n",
      "   (0.016633530923287385, u'place')],\n",
      "  -1.5022727452941445),\n",
      " ([(0.038328536477745909, u'something'),\n",
      "   (0.026200800990536628, u'people'),\n",
      "   (0.021311486503643652, u'building'),\n",
      "   (0.017758186787523191, u'way'),\n",
      "   (0.017012625597553638, u'girl'),\n",
      "   (0.016893935808862236, u'boy'),\n",
      "   (0.016655077022664604, u'sort'),\n",
      "   (0.016169329924704365, u'part'),\n",
      "   (0.016069129121161544, u'thing'),\n",
      "   (0.015931598702813018, u'kind')],\n",
      "  -1.6474765384215264),\n",
      " ([(0.030390866650720386, u'something'),\n",
      "   (0.027410214046901481, u'door'),\n",
      "   (0.01897901774004019, u'finger'),\n",
      "   (0.015613535294398543, u'message'),\n",
      "   (0.015230191962156199, u'way'),\n",
      "   (0.01511439693113902, u'room'),\n",
      "   (0.01422786985831812, u'wall'),\n",
      "   (0.013318745759653122, u'time'),\n",
      "   (0.012872093260386895, u'piece'),\n",
      "   (0.012729431761733443, u'thing')],\n",
      "  -1.8252505976803055),\n",
      " ([(0.059202907729055725, u'book'),\n",
      "   (0.034505804546012861, u'people'),\n",
      "   (0.034289366011436798, u'room'),\n",
      "   (0.028905656571274831, u'elevator'),\n",
      "   (0.02251060644736529, u'floor'),\n",
      "   (0.01907853373350097, u'door'),\n",
      "   (0.018607991566454707, u'hotel'),\n",
      "   (0.013696964496502945, u'something'),\n",
      "   (0.013633287829521461, u'paper'),\n",
      "   (0.013328288416969439, u'woman')],\n",
      "  -1.8658770135541372),\n",
      " ([(0.059274857436847024, u'mom'),\n",
      "   (0.037845594421355579, u'eugene'),\n",
      "   (0.037560293232619162, u'shop'),\n",
      "   (0.03669229936590139, u'something'),\n",
      "   (0.036438187410230409, u'school'),\n",
      "   (0.036054065701546331, u'calvin'),\n",
      "   (0.034926469044744432, u'ezra'),\n",
      "   (0.031821265980884285, u'movie'),\n",
      "   (0.031427664331960876, u'photo'),\n",
      "   (0.026141058340647776, u'thing')],\n",
      "  -1.9407410001811876),\n",
      " ([(0.088155349047228521, u'kind'),\n",
      "   (0.06368620115534461, u'thing'),\n",
      "   (0.033113444968140761, u'dream'),\n",
      "   (0.030606014946229089, u'stuff'),\n",
      "   (0.030338105726424324, u'mom'),\n",
      "   (0.028135096626412913, u'something'),\n",
      "   (0.022525735724776022, u'tape'),\n",
      "   (0.016624896940636232, u'sound'),\n",
      "   (0.014637735403240808, u'reason'),\n",
      "   (0.014631825124650623, u'time')],\n",
      "  -1.9525327806966937),\n",
      " ([(0.052778308075119518, u'something'),\n",
      "   (0.042597605022394472, u'thing'),\n",
      "   (0.03707079589405101, u'someone'),\n",
      "   (0.032611155192027091, u'people'),\n",
      "   (0.030382852963828744, u'time'),\n",
      "   (0.025632613461635152, u'kind'),\n",
      "   (0.021457882944462253, u'child'),\n",
      "   (0.016714973782736984, u'home'),\n",
      "   (0.016021272937174192, u'way'),\n",
      "   (0.013683271908571479, u'matthew')],\n",
      "  -2.0233433466501745),\n",
      " ([(0.21286122284964626, u'car'),\n",
      "   (0.034809190738408359, u'street'),\n",
      "   (0.030580806596833907, u'truck'),\n",
      "   (0.027930143607627026, u'seat'),\n",
      "   (0.020589170492862123, u'road'),\n",
      "   (0.019190154544633783, u'side'),\n",
      "   (0.015825362411674405, u'home'),\n",
      "   (0.014748023366521807, u'way'),\n",
      "   (0.014568430493398388, u'driver'),\n",
      "   (0.014209134208964389, u'station')],\n",
      "  -2.0916403184978218),\n",
      " ([(0.16765935136454144, u'room'),\n",
      "   (0.079492711523471524, u'bed'),\n",
      "   (0.047192961716566457, u'door'),\n",
      "   (0.039482850212888297, u'bathroom'),\n",
      "   (0.027368417912322276, u'floor'),\n",
      "   (0.022317634544059214, u'bedroom'),\n",
      "   (0.018902056125905418, u'toilet'),\n",
      "   (0.016903198010454753, u'wall'),\n",
      "   (0.015621531551042733, u'window'),\n",
      "   (0.012697940398915218, u'hall')],\n",
      "  -2.134712747460207),\n",
      " ([(0.086918781439990919, u'sort'),\n",
      "   (0.05992328895703071, u'something'),\n",
      "   (0.046044985478744992, u'thing'),\n",
      "   (0.03233583001431551, u'husband'),\n",
      "   (0.025284170727146716, u'place'),\n",
      "   (0.024974944825505274, u'way'),\n",
      "   (0.024968846360862121, u'people'),\n",
      "   (0.02164721865052532, u'wife'),\n",
      "   (0.019820667637907054, u'somebody'),\n",
      "   (0.019031376941722713, u'daughter')],\n",
      "  -2.1606038038745745),\n",
      " ([(0.093776667172859929, u'lady'),\n",
      "   (0.035611482509514618, u'people'),\n",
      "   (0.027582092874822852, u'friend'),\n",
      "   (0.023477784086372681, u'bar'),\n",
      "   (0.022222478666118043, u'everyone'),\n",
      "   (0.020097089161940605, u'guy'),\n",
      "   (0.014274214333558456, u'ben'),\n",
      "   (0.013432559684645805, u'movie'),\n",
      "   (0.012613197199787632, u'someone'),\n",
      "   (0.011637980157303754, u'house')],\n",
      "  -2.2629250089380872),\n",
      " ([(0.12601114815982009, u'building'),\n",
      "   (0.065537513260602309, u'apartment'),\n",
      "   (0.044508529996831261, u'door'),\n",
      "   (0.036121737859517861, u'stair'),\n",
      "   (0.025746635456446797, u'floor'),\n",
      "   (0.022614189508760187, u'cake'),\n",
      "   (0.020350907942565454, u'people'),\n",
      "   (0.014256786084986832, u'level'),\n",
      "   (0.011666516303615567, u'part'),\n",
      "   (0.010347051940388494, u'wall')],\n",
      "  -2.2816133649274133),\n",
      " ([(0.12496232965435057, u'office'),\n",
      "   (0.059413060071060488, u'work'),\n",
      "   (0.057377847246766568, u'computer'),\n",
      "   (0.047605298160770561, u'desk'),\n",
      "   (0.036141361971615472, u'something'),\n",
      "   (0.025668928420987738, u'word'),\n",
      "   (0.022812333472400723, u'thing'),\n",
      "   (0.019029710355120126, u'file'),\n",
      "   (0.016983957349909692, u'time'),\n",
      "   (0.016206166614831569, u'information')],\n",
      "  -2.3144689027617082),\n",
      " ([(0.2558552766589553, u'house'),\n",
      "   (0.037043446226894809, u'sister'),\n",
      "   (0.034935142351343339, u'mother'),\n",
      "   (0.028201384471020254, u'window'),\n",
      "   (0.026285985726469577, u'door'),\n",
      "   (0.024768265278409553, u'home'),\n",
      "   (0.021300372818533982, u'room'),\n",
      "   (0.018572768279769038, u'aunt'),\n",
      "   (0.018526208367269942, u'brother'),\n",
      "   (0.018054760915025829, u'cousin')],\n",
      "  -2.318921392402415),\n",
      " ([(0.19055294263542671, u'dream'),\n",
      "   (0.12984307637124612, u'friend'),\n",
      "   (0.028139681894068316, u'brother'),\n",
      "   (0.023612632000863047, u'mine'),\n",
      "   (0.022722750393911573, u'time'),\n",
      "   (0.018499112775101679, u'name'),\n",
      "   (0.015857782538691786, u'place'),\n",
      "   (0.014455398233465823, u'horse'),\n",
      "   (0.014403268756772595, u'night'),\n",
      "   (0.01318274657003861, u'year')],\n",
      "  -2.3219999328219907),\n",
      " ([(0.14980930763562528, u'class'),\n",
      "   (0.1387306547062542, u'school'),\n",
      "   (0.066607309924003613, u'teacher'),\n",
      "   (0.048967457702271407, u'student'),\n",
      "   (0.029943556687224891, u'test'),\n",
      "   (0.024475320931566055, u'classroom'),\n",
      "   (0.018300967422591054, u'time'),\n",
      "   (0.017799738055694958, u'paper'),\n",
      "   (0.017068441796063104, u'grade'),\n",
      "   (0.015688431371371075, u'university')],\n",
      "  -2.4398886206442234),\n",
      " ([(0.049315540588694591, u'road'),\n",
      "   (0.027153865065551973, u'hill'),\n",
      "   (0.022869641077889781, u'side'),\n",
      "   (0.021276596318574363, u'way'),\n",
      "   (0.021091904604574978, u'area'),\n",
      "   (0.020203076575125937, u'tree'),\n",
      "   (0.019056627686837146, u'foot'),\n",
      "   (0.016115013124807429, u'path'),\n",
      "   (0.014877846447654372, u'mountain'),\n",
      "   (0.013091346263241942, u'edge')],\n",
      "  -2.4661629536436043),\n",
      " ([(0.051599478336957681, u'woman'),\n",
      "   (0.02065416417634694, u'group'),\n",
      "   (0.02032654978897501, u'stage'),\n",
      "   (0.019654510176668968, u'play'),\n",
      "   (0.019042485190551725, u'people'),\n",
      "   (0.018747518708078257, u'man'),\n",
      "   (0.013881247021006802, u'chair'),\n",
      "   (0.01387710975475066, u'audience'),\n",
      "   (0.0118374180286086, u'wheelchair'),\n",
      "   (0.011351042008606814, u'seat')],\n",
      "  -2.4711575218424069),\n",
      " ([(0.2026004329846004, u'man'),\n",
      "   (0.061401265681866733, u'woman'),\n",
      "   (0.051164519517707265, u'men'),\n",
      "   (0.036462913992823393, u'nbsp'),\n",
      "   (0.019394048425015038, u'gun'),\n",
      "   (0.017264345913598225, u'hand'),\n",
      "   (0.011913541073833536, u'head'),\n",
      "   (0.0091693424866168272, u'arm'),\n",
      "   (0.0080702550883288565, u'face'),\n",
      "   (0.0068525911968165022, u'love')],\n",
      "  -2.6438527851741971),\n",
      " ([(0.10836220616410398, u'room'),\n",
      "   (0.08941800808272099, u'table'),\n",
      "   (0.025483150332150057, u'people'),\n",
      "   (0.021271662785364978, u'man'),\n",
      "   (0.019175615398561118, u'dinner'),\n",
      "   (0.018271373063608778, u'dining'),\n",
      "   (0.016352458968869579, u'place'),\n",
      "   (0.015790429357180703, u'chair'),\n",
      "   (0.015492160112925471, u'dish'),\n",
      "   (0.012811543705701036, u'food')],\n",
      "  -2.6515312808185296),\n",
      " ([(0.22972818427277664, u'water'),\n",
      "   (0.059926403979147183, u'pool'),\n",
      "   (0.036067483573447787, u'river'),\n",
      "   (0.028113798414603982, u'lake'),\n",
      "   (0.027376092193034166, u'wave'),\n",
      "   (0.019179263733595835, u'beach'),\n",
      "   (0.01728005384807306, u'ocean'),\n",
      "   (0.016577820182553004, u'fish'),\n",
      "   (0.014101557488460509, u'swimming'),\n",
      "   (0.011230324588325425, u'foot')],\n",
      "  -2.7905146320225054),\n",
      " ([(0.14383451883427514, u'father'),\n",
      "   (0.13818191199766822, u'mother'),\n",
      "   (0.068059749654142021, u'church'),\n",
      "   (0.02370062972011653, u'frank'),\n",
      "   (0.020600804576445423, u'andrew'),\n",
      "   (0.016017604714499201, u'shift'),\n",
      "   (0.01505764280627778, u'home'),\n",
      "   (0.012951031125159062, u'time'),\n",
      "   (0.012442537080010528, u'child'),\n",
      "   (0.011323395064481313, u'mary')],\n",
      "  -2.7937460495572197),\n",
      " ([(0.070695579562734021, u'friend'),\n",
      "   (0.029625963778294451, u'classmate'),\n",
      "   (0.027908495303500931, u'people'),\n",
      "   (0.027302960500342491, u'school'),\n",
      "   (0.016958140514491682, u'mom'),\n",
      "   (0.015687054423258986, u'stephen'),\n",
      "   (0.014112055603989784, u'woman'),\n",
      "   (0.013892208193745557, u'others'),\n",
      "   (0.013356248810816934, u'beer'),\n",
      "   (0.012239212407489807, u'annie')],\n",
      "  -2.8367996265620423),\n",
      " ([(0.14638290251706101, u'store'),\n",
      "   (0.022237731090038536, u'counter'),\n",
      "   (0.02048280581380043, u'grocery'),\n",
      "   (0.018198298366578092, u'woman'),\n",
      "   (0.017865148610646815, u'money'),\n",
      "   (0.014366702294373002, u'food'),\n",
      "   (0.014146248499627066, u'manager'),\n",
      "   (0.013501977335340243, u'department'),\n",
      "   (0.012746248656036827, u'order'),\n",
      "   (0.012595668290863518, u'line')],\n",
      "  -2.8429221629254942),\n",
      " ([(0.075741377865515949, u'hair'),\n",
      "   (0.065321200781011945, u'dress'),\n",
      "   (0.055300098932794778, u'picture'),\n",
      "   (0.033154200741609408, u'wedding'),\n",
      "   (0.029514268319822203, u'eye'),\n",
      "   (0.026357955043974937, u'job'),\n",
      "   (0.023829356897775135, u'teeth'),\n",
      "   (0.01828983120467988, u'mirror'),\n",
      "   (0.01768583379648573, u'color'),\n",
      "   (0.017134152074416781, u'face')],\n",
      "  -3.0199313610362397),\n",
      " ([(0.10944038437523747, u'door'),\n",
      "   (0.058338950971630957, u'key'),\n",
      "   (0.03955720113506301, u'garage'),\n",
      "   (0.033137234191873449, u'mark'),\n",
      "   (0.019263019696353769, u'something'),\n",
      "   (0.01664984485841554, u'bos'),\n",
      "   (0.012752882591281626, u'car'),\n",
      "   (0.0096815251467532604, u'shop'),\n",
      "   (0.0092830205802123377, u'chuck'),\n",
      "   (0.0083247132710593461, u'side')],\n",
      "  -3.0199774582257488),\n",
      " ([(0.11363484209165864, u'phone'),\n",
      "   (0.042999233677437, u'number'),\n",
      "   (0.032839131599201439, u'home'),\n",
      "   (0.031994274792349495, u'camera'),\n",
      "   (0.03137568591453397, u'call'),\n",
      "   (0.029933459491967268, u'john'),\n",
      "   (0.020981141085406791, u'day'),\n",
      "   (0.015535191176071234, u'telephone'),\n",
      "   (0.015293329227323904, u'song'),\n",
      "   (0.014777549848634953, u'picture')],\n",
      "  -3.1237948226613432),\n",
      " ([(0.3736299263083378, u'girl'),\n",
      "   (0.038511087855941303, u'boy'),\n",
      "   (0.037753849425654698, u'girlfriend'),\n",
      "   (0.01967073164999731, u'year'),\n",
      "   (0.019101433362553374, u'jack'),\n",
      "   (0.016626652704574061, u'love'),\n",
      "   (0.011458013800733954, u'group'),\n",
      "   (0.0097851349748105075, u'god'),\n",
      "   (0.009758773629572233, u'list'),\n",
      "   (0.0094634822563289221, u'hair')],\n",
      "  -3.3905805276813044),\n",
      " ([(0.083804620315842016, u'game'),\n",
      "   (0.081640932647440448, u'train'),\n",
      "   (0.062017887642610868, u'ball'),\n",
      "   (0.033464738947768927, u'team'),\n",
      "   (0.029400308094547651, u'track'),\n",
      "   (0.021494455730092121, u'field'),\n",
      "   (0.020613440681514755, u'player'),\n",
      "   (0.018552536974859674, u'bear'),\n",
      "   (0.017767124278430677, u'football'),\n",
      "   (0.01659201649611269, u'basketball')],\n",
      "  -3.4267185569175385),\n",
      " ([(0.056213573536139275, u'woman'),\n",
      "   (0.056097559052327384, u'hospital'),\n",
      "   (0.047478994948127018, u'party'),\n",
      "   (0.039611758678647239, u'body'),\n",
      "   (0.029339024673194707, u'patient'),\n",
      "   (0.023230608671651498, u'blood'),\n",
      "   (0.019466582881537502, u'pain'),\n",
      "   (0.017021740722714913, u'bed'),\n",
      "   (0.015739962108990844, u'nurse'),\n",
      "   (0.01269047990446282, u'officer')],\n",
      "  -3.4684879159521618),\n",
      " ([(0.065014824390645543, u'boyfriend'),\n",
      "   (0.048975640138643556, u'food'),\n",
      "   (0.03863359692934825, u'jeremy'),\n",
      "   (0.024377108989818222, u'plate'),\n",
      "   (0.020684107784517042, u'candy'),\n",
      "   (0.020469519431659013, u'something'),\n",
      "   (0.02042041038552941, u'egg'),\n",
      "   (0.016310609159773955, u'bowl'),\n",
      "   (0.015170584275266394, u'kitchen'),\n",
      "   (0.012802581825456641, u'chocolate')],\n",
      "  -3.6601354020050443),\n",
      " ([(0.10581804357716997, u'boat'),\n",
      "   (0.044152526079879253, u'money'),\n",
      "   (0.034375632414730163, u'ticket'),\n",
      "   (0.031483621752608465, u'ship'),\n",
      "   (0.026490054760621516, u'island'),\n",
      "   (0.025884295162712755, u'dollar'),\n",
      "   (0.020509848389432421, u'city'),\n",
      "   (0.01557938220625546, u'people'),\n",
      "   (0.013484624547116053, u'sea'),\n",
      "   (0.013285198794783916, u'coin')],\n",
      "  -3.688669639302629),\n",
      " ([(0.1245348987260259, u'cat'),\n",
      "   (0.10405482981297512, u'shoe'),\n",
      "   (0.054197082042960448, u'pair'),\n",
      "   (0.04287209703541596, u'kitten'),\n",
      "   (0.040440757649605759, u'pant'),\n",
      "   (0.023489800765134964, u'orange'),\n",
      "   (0.021385299269734655, u'short'),\n",
      "   (0.021303182802494644, u'sock'),\n",
      "   (0.015206473691151343, u'watch'),\n",
      "   (0.011523609098198334, u'leg')],\n",
      "  -3.7423388869181249),\n",
      " ([(0.17644045956699297, u'baby'),\n",
      "   (0.037086533944347841, u'box'),\n",
      "   (0.0362198813765717, u'boy'),\n",
      "   (0.026037624462187964, u'chicken'),\n",
      "   (0.024420199960173108, u'knife'),\n",
      "   (0.01868325501466361, u'board'),\n",
      "   (0.017565168962262505, u'doll'),\n",
      "   (0.016880334244946584, u'child'),\n",
      "   (0.011952614464447111, u'one'),\n",
      "   (0.011683375085344127, u'george')],\n",
      "  -3.792051410134992),\n",
      " ([(0.13017387739706202, u'dog'),\n",
      "   (0.02739942249306989, u'dora'),\n",
      "   (0.01676465768040198, u'cow'),\n",
      "   (0.014999732182808201, u'animal'),\n",
      "   (0.014224339891293949, u'time'),\n",
      "   (0.013881759681902799, u'fence'),\n",
      "   (0.012496770005067269, u'pet'),\n",
      "   (0.012151221400665735, u'mother'),\n",
      "   (0.01092805906215041, u'guy'),\n",
      "   (0.010005122779972297, u'rudy')],\n",
      "  -4.183231588633336),\n",
      " ([(0.040810580590974442, u'restaurant'),\n",
      "   (0.039319125755527949, u'table'),\n",
      "   (0.030065790257623282, u'lunch'),\n",
      "   (0.018655576453522792, u'purse'),\n",
      "   (0.015075863915431297, u'school'),\n",
      "   (0.013073916515117546, u'cafe'),\n",
      "   (0.011955763578070395, u'gym'),\n",
      "   (0.011024589110047726, u'clothes'),\n",
      "   (0.010412255523735991, u'lot'),\n",
      "   (0.0098939231963192098, u'jerry')],\n",
      "  -4.1925174923054174),\n",
      " ([(0.062200905633755542, u'rock'),\n",
      "   (0.048645029682081058, u'hole'),\n",
      "   (0.046644947085335647, u'bird'),\n",
      "   (0.039509237091769241, u'horse'),\n",
      "   (0.030218243595284168, u'race'),\n",
      "   (0.024996591426524089, u'sand'),\n",
      "   (0.023556629428002133, u'soldier'),\n",
      "   (0.01500781776087402, u'lion'),\n",
      "   (0.01314756284259124, u'air'),\n",
      "   (0.012809435676279506, u'pile')],\n",
      "  -4.2306006944235319),\n",
      " ([(0.17506371538685103, u'dad'),\n",
      "   (0.13062697400176659, u'kid'),\n",
      "   (0.031302047174631926, u'mom'),\n",
      "   (0.020221735632522875, u'home'),\n",
      "   (0.019858518289824763, u'thing'),\n",
      "   (0.017497175363760126, u'ghost'),\n",
      "   (0.016367789124352442, u'suitcase'),\n",
      "   (0.014362282530893491, u'something'),\n",
      "   (0.013767715456588475, u'michael'),\n",
      "   (0.010318705688061728, u'show')],\n",
      "  -4.2682589489327798),\n",
      " ([(0.069067506792716962, u'machine'),\n",
      "   (0.041583490655958295, u'sky'),\n",
      "   (0.0362702433347988, u'coffee'),\n",
      "   (0.029873754882979112, u'alien'),\n",
      "   (0.028862669547751095, u'cup'),\n",
      "   (0.025059287085803347, u'cloud'),\n",
      "   (0.024863889538596359, u'center'),\n",
      "   (0.024289322604622705, u'star'),\n",
      "   (0.021684956281729072, u'gift'),\n",
      "   (0.020313366819753445, u'dwight')],\n",
      "  -4.4337374848285318),\n",
      " ([(0.086690809067356711, u'card'),\n",
      "   (0.037778391920623176, u'coat'),\n",
      "   (0.022075541757421505, u'basket'),\n",
      "   (0.021342626028690831, u'ladder'),\n",
      "   (0.019175216079863208, u'dovre'),\n",
      "   (0.0189745661053001, u'roof'),\n",
      "   (0.01491488506120702, u'paulina'),\n",
      "   (0.012594977774236923, u'hand'),\n",
      "   (0.012073934976656951, u'rack'),\n",
      "   (0.011006789898065894, u'hat')],\n",
      "  -4.5860695001996259),\n",
      " ([(0.21817073049348101, u'bus'),\n",
      "   (0.024729878273447175, u'driver'),\n",
      "   (0.024680694055079808, u'stop'),\n",
      "   (0.020757324279232707, u'time'),\n",
      "   (0.020352362825645516, u'record'),\n",
      "   (0.014626475428673546, u'article'),\n",
      "   (0.012788613557103107, u'fraternity'),\n",
      "   (0.012293563291753493, u'seat'),\n",
      "   (0.011422883372648242, u'street'),\n",
      "   (0.011031849115701434, u'campus')],\n",
      "  -4.6020567457906596),\n",
      " ([(0.099621668306969363, u'plane'),\n",
      "   (0.093237420331391888, u'fire'),\n",
      "   (0.039126038931572911, u'beach'),\n",
      "   (0.032017205388855004, u'airport'),\n",
      "   (0.029053908014250108, u'tunnel'),\n",
      "   (0.027262094716265336, u'airplane'),\n",
      "   (0.019049558773315566, u'flame'),\n",
      "   (0.018862972969536748, u'people'),\n",
      "   (0.018643234090247884, u'spider'),\n",
      "   (0.018045366970965732, u'pilot')],\n",
      "  -5.4897951977727963),\n",
      " ([(0.057977654346951998, u'ice'),\n",
      "   (0.037242157171981087, u'howard'),\n",
      "   (0.035196607504958288, u'cream'),\n",
      "   (0.018373661381273915, u'space'),\n",
      "   (0.017193446399237113, u'apple'),\n",
      "   (0.013548545249590697, u'drawer'),\n",
      "   (0.011054003934415736, u'time'),\n",
      "   (0.010320900484352259, u'thing'),\n",
      "   (0.010215359959444302, u'animal'),\n",
      "   (0.009715006589904214, u'ditch')],\n",
      "  -5.6982239552879594),\n",
      " ([(0.046339582301779488, u'uncle'),\n",
      "   (0.025795923877654506, u'neighbor'),\n",
      "   (0.02548116710147753, u'deer'),\n",
      "   (0.023994903958775543, u'farm'),\n",
      "   (0.020435941808867753, u'redding'),\n",
      "   (0.020049106638758907, u'alley'),\n",
      "   (0.016515303656909945, u'ray'),\n",
      "   (0.016319910454600288, u'photograph'),\n",
      "   (0.015753927586994777, u'garden'),\n",
      "   (0.015372359501735927, u'tim')],\n",
      "  -5.9122435915377913),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ([(0.11397408256234777, u'bill'),\n",
      "   (0.046628939126542569, u'letter'),\n",
      "   (0.030357247490719563, u'grandma'),\n",
      "   (0.021567854616785991, u'jane'),\n",
      "   (0.017555431287358217, u'mail'),\n",
      "   (0.01638135093069111, u'something'),\n",
      "   (0.015043640219355036, u'grandpa'),\n",
      "   (0.013062038434679804, u'butler'),\n",
      "   (0.012237208946547876, u'relationship'),\n",
      "   (0.011412843205990578, u'someone')],\n",
      "  -6.1882190680166076),\n",
      " ([(0.058598351556828208, u'tree'),\n",
      "   (0.034924413197091018, u'snake'),\n",
      "   (0.032546920975558259, u'ann'),\n",
      "   (0.027360416227497981, u'bob'),\n",
      "   (0.024467924484172471, u'guard'),\n",
      "   (0.023078753708982488, u'barn'),\n",
      "   (0.016669671353834365, u'pig'),\n",
      "   (0.016205954639605315, u'head'),\n",
      "   (0.014895910449412434, u'prisoner'),\n",
      "   (0.011983396318817244, u'judy')],\n",
      "  -6.3621887539401492),\n",
      " ([(0.11205766184434905, u'doctor'),\n",
      "   (0.043895087226125507, u'bottle'),\n",
      "   (0.035271202921130021, u'shower'),\n",
      "   (0.025627191288982332, u'milk'),\n",
      "   (0.016584958109564122, u'result'),\n",
      "   (0.014981884968144078, u'time'),\n",
      "   (0.014313923638224731, u'tea'),\n",
      "   (0.014041931307403404, u'medicine'),\n",
      "   (0.013760707751934078, u'benjamin'),\n",
      "   (0.012596598475986535, u'dr.')],\n",
      "  -6.5249626274714121),\n",
      " ([(0.027164810984228474, u'locker'),\n",
      "   (0.020819720517896483, u'president'),\n",
      "   (0.019141024274693691, u'piano'),\n",
      "   (0.017628903177665148, u'laura'),\n",
      "   (0.016720486674363461, u'cigarette'),\n",
      "   (0.016247922020061679, u'glass'),\n",
      "   (0.013587597597199347, u'soap'),\n",
      "   (0.013280375225652319, u'pot'),\n",
      "   (0.012695097018574934, u'balloon'),\n",
      "   (0.011913988801668291, u'music')],\n",
      "  -7.2067385943537943)]\n"
     ]
    }
   ],
   "source": [
    "top_topics = model.top_topics(corpus, topn=10)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dreamer 666\n",
      "[([(0.55731887253135459, u'butler'),\n",
      "   (0.28067511894251368, u'demon'),\n",
      "   (0.032664463848481137, u'anime'),\n",
      "   (0.032338535819832592, u'okay'),\n",
      "   (0.032338213524431336, u'video'),\n",
      "   (0.032334322382916551, u'world'),\n",
      "   (0.032330472950470139, u'game')],\n",
      "  -17.868381313196068),\n",
      " ([(0.55768824077722878, u'butler'),\n",
      "   (0.28071390781555161, u'demon'),\n",
      "   (0.03256347599025456, u'anime'),\n",
      "   (0.032261512740158864, u'okay'),\n",
      "   (0.032261214058138643, u'video'),\n",
      "   (0.032257608007180548, u'world'),\n",
      "   (0.032254040611487067, u'game')],\n",
      "  -17.868381313196068)]\n"
     ]
    }
   ],
   "source": [
    "# show top topics for dreamerID #666\n",
    "for key, (mod, corp, dic) in topic_models.iteritems():\n",
    "    if key == 666:\n",
    "        print \"\\nDreamer\", key\n",
    "        pprint(mod.top_topics(corpus = corp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'roble': 7,\n",
       "         'yellow': 1,\n",
       "         'four': 12,\n",
       "         'circuitry': 1,\n",
       "         'sleep': 15,\n",
       "         'spiders': 5,\n",
       "         'hanging': 11,\n",
       "         'mansion': 3,\n",
       "         'oldest': 1,\n",
       "         'saved': 3,\n",
       "         'worked': 2,\n",
       "         'marching': 2,\n",
       "         'looking': 20,\n",
       "         'dalek': 1,\n",
       "         'replacements': 1,\n",
       "         'chinese': 11,\n",
       "         'electricity': 3,\n",
       "         'blacking': 1,\n",
       "         'x93otokonoko': 1,\n",
       "         'neighbours': 3,\n",
       "         'hallucinate': 1,\n",
       "         'teaching': 1,\n",
       "         'regan': 1,\n",
       "         'onstage': 3,\n",
       "         'playground': 3,\n",
       "         'sinking': 1,\n",
       "         'x93then': 1,\n",
       "         'rescue': 1,\n",
       "         'blanket': 2,\n",
       "         'mata': 1,\n",
       "         'every': 19,\n",
       "         'lecture': 2,\n",
       "         'jacob': 2,\n",
       "         'stabbed': 1,\n",
       "         'fireplace': 1,\n",
       "         'excited': 2,\n",
       "         'look': 13,\n",
       "         'doumo': 1,\n",
       "         'school': 43,\n",
       "         'basics': 1,\n",
       "         'prize': 1,\n",
       "         'x93like': 1,\n",
       "         'called': 9,\n",
       "         'wooden': 1,\n",
       "         'red': 13,\n",
       "         'x93walker': 1,\n",
       "         'pentagram': 2,\n",
       "         'heading': 1,\n",
       "         'asian': 1,\n",
       "         'carnivore': 1,\n",
       "         'x93humans': 1,\n",
       "         'force': 1,\n",
       "         'leaders': 1,\n",
       "         'tired': 1,\n",
       "         'bacon': 1,\n",
       "         'japanese': 26,\n",
       "         'dish': 3,\n",
       "         'second': 6,\n",
       "         'louder': 1,\n",
       "         'scraped': 1,\n",
       "         'machines': 1,\n",
       "         'x93go': 1,\n",
       "         'blue': 17,\n",
       "         'fugs': 1,\n",
       "         'hide': 5,\n",
       "         'trapping': 1,\n",
       "         'hate': 3,\n",
       "         'exactly': 7,\n",
       "         'cooking': 1,\n",
       "         'fingers': 4,\n",
       "         'poison': 2,\n",
       "         'spokesman': 1,\n",
       "         'lights': 3,\n",
       "         'cars': 4,\n",
       "         'zoomed': 2,\n",
       "         'new': 27,\n",
       "         'net': 3,\n",
       "         'ever': 15,\n",
       "         'told': 42,\n",
       "         'injections': 1,\n",
       "         'kicked': 1,\n",
       "         'hero': 1,\n",
       "         'screams': 1,\n",
       "         'pancakes': 1,\n",
       "         'never': 13,\n",
       "         'drew': 1,\n",
       "         'herd': 1,\n",
       "         'met': 11,\n",
       "         'protection': 1,\n",
       "         '100': 1,\n",
       "         'plex': 1,\n",
       "         'mey': 22,\n",
       "         'celebration': 1,\n",
       "         'luther': 1,\n",
       "         'kids': 20,\n",
       "         'daughter': 2,\n",
       "         'items': 3,\n",
       "         'study': 1,\n",
       "         'changed': 7,\n",
       "         'swam': 6,\n",
       "         'x93uh': 1,\n",
       "         'smoke': 1,\n",
       "         'x93um': 1,\n",
       "         'symmetrical': 1,\n",
       "         'changes': 1,\n",
       "         'punishment': 1,\n",
       "         'york': 2,\n",
       "         'eyebrows': 1,\n",
       "         'besides': 2,\n",
       "         'stray': 1,\n",
       "         'slices': 1,\n",
       "         'straw': 1,\n",
       "         'explained': 8,\n",
       "         'brought': 6,\n",
       "         'mecha': 1,\n",
       "         'drowned': 1,\n",
       "         'atrium': 3,\n",
       "         'total': 1,\n",
       "         'sarah': 1,\n",
       "         'spoke': 10,\n",
       "         'would': 36,\n",
       "         'hospital': 1,\n",
       "         'beetles': 1,\n",
       "         'arms': 10,\n",
       "         'call': 6,\n",
       "         'lazenby': 1,\n",
       "         'survive': 2,\n",
       "         'type': 1,\n",
       "         'tell': 13,\n",
       "         'mystical': 1,\n",
       "         'x93hello': 1,\n",
       "         'lungs': 2,\n",
       "         'successful': 1,\n",
       "         'chubby': 1,\n",
       "         'drowning': 1,\n",
       "         'award': 1,\n",
       "         'hurt': 3,\n",
       "         'phone': 11,\n",
       "         'warm': 2,\n",
       "         'pinching': 2,\n",
       "         'adult': 1,\n",
       "         'lord': 1,\n",
       "         'hole': 1,\n",
       "         'hold': 4,\n",
       "         '95': 1,\n",
       "         'blocking': 1,\n",
       "         'must': 6,\n",
       "         'shoot': 1,\n",
       "         'join': 4,\n",
       "         'room': 48,\n",
       "         'work': 7,\n",
       "         'adjusting': 1,\n",
       "         'roof': 1,\n",
       "         'anglerfish': 1,\n",
       "         'leysin': 3,\n",
       "         'ms': 8,\n",
       "         'mr': 4,\n",
       "         'crackers': 1,\n",
       "         'could': 58,\n",
       "         'gloria': 5,\n",
       "         'shook': 1,\n",
       "         'casita': 1,\n",
       "         'movieclowns': 1,\n",
       "         'give': 4,\n",
       "         'dolphin': 2,\n",
       "         'dragons': 1,\n",
       "         'mermaids': 2,\n",
       "         'overheard': 3,\n",
       "         'want': 9,\n",
       "         'times': 9,\n",
       "         'tornadoes': 1,\n",
       "         'end': 10,\n",
       "         'turn': 8,\n",
       "         'travel': 4,\n",
       "         'drying': 1,\n",
       "         'damage': 3,\n",
       "         'machine': 8,\n",
       "         'x93pizza': 2,\n",
       "         'hop': 6,\n",
       "         'interview': 4,\n",
       "         'gate': 4,\n",
       "         'scooters': 1,\n",
       "         'classify': 1,\n",
       "         'pizza': 3,\n",
       "         'classifying': 1,\n",
       "         'mouths': 1,\n",
       "         'transforming': 2,\n",
       "         'earlier': 2,\n",
       "         'hourglass': 1,\n",
       "         'confirmed': 1,\n",
       "         'lab': 1,\n",
       "         'diagram': 1,\n",
       "         'wrong': 3,\n",
       "         'jumping': 2,\n",
       "         'curiosity': 1,\n",
       "         'passageway': 1,\n",
       "         'president': 1,\n",
       "         'ziplining': 1,\n",
       "         'meaningful': 1,\n",
       "         'parallel': 1,\n",
       "         'shuo1': 1,\n",
       "         'third': 1,\n",
       "         'childhood': 1,\n",
       "         'scout': 3,\n",
       "         'green': 7,\n",
       "         'ultimate': 1,\n",
       "         'enter': 2,\n",
       "         'worst': 2,\n",
       "         'harlem': 1,\n",
       "         'order': 5,\n",
       "         'wind': 3,\n",
       "         'blind': 3,\n",
       "         'office': 6,\n",
       "         'cheese': 1,\n",
       "         'zen': 2,\n",
       "         'namae': 1,\n",
       "         'elevated': 1,\n",
       "         'x93goodnight': 1,\n",
       "         'japan': 6,\n",
       "         'elena': 7,\n",
       "         'ima': 1,\n",
       "         'fit': 3,\n",
       "         'personal': 1,\n",
       "         'soaking': 1,\n",
       "         'backpack': 2,\n",
       "         'writing': 7,\n",
       "         'destroyed': 1,\n",
       "         'crea': 1,\n",
       "         'wen4': 1,\n",
       "         'carton': 1,\n",
       "         'easier': 1,\n",
       "         'eventually': 5,\n",
       "         'woken': 1,\n",
       "         'thrones': 1,\n",
       "         'safe': 6,\n",
       "         'break': 4,\n",
       "         'bang': 2,\n",
       "         'effects': 1,\n",
       "         'plates': 3,\n",
       "         'jesus': 1,\n",
       "         'silver': 10,\n",
       "         'represents': 1,\n",
       "         'alex': 8,\n",
       "         'rocky': 1,\n",
       "         'l': 1,\n",
       "         'x93well': 5,\n",
       "         'choir': 7,\n",
       "         'frantically': 2,\n",
       "         'arrow': 4,\n",
       "         'alek': 1,\n",
       "         'volcano': 1,\n",
       "         'went': 43,\n",
       "         'x92ve': 8,\n",
       "         '375': 1,\n",
       "         'side': 8,\n",
       "         'bone': 1,\n",
       "         'mean': 2,\n",
       "         'assumingly': 1,\n",
       "         'lifted': 2,\n",
       "         'snakes': 3,\n",
       "         'series': 3,\n",
       "         'emeralds': 1,\n",
       "         'spider': 18,\n",
       "         'taught': 6,\n",
       "         'stealing': 1,\n",
       "         'lawyers': 1,\n",
       "         'dawn': 2,\n",
       "         'ring': 3,\n",
       "         'whip': 1,\n",
       "         'large': 5,\n",
       "         'wolfpacks': 1,\n",
       "         'driving': 7,\n",
       "         'sang': 1,\n",
       "         'open': 12,\n",
       "         'psychic': 1,\n",
       "         'mayan': 1,\n",
       "         'washed': 1,\n",
       "         'forty': 3,\n",
       "         'tomorrow': 3,\n",
       "         'flooded': 1,\n",
       "         'laid': 2,\n",
       "         'adjust': 1,\n",
       "         'assignment': 1,\n",
       "         'medicine': 1,\n",
       "         'got': 73,\n",
       "         'forth': 1,\n",
       "         'sliding': 1,\n",
       "         'arizona': 1,\n",
       "         'turning': 1,\n",
       "         'threatened': 3,\n",
       "         'little': 16,\n",
       "         'free': 9,\n",
       "         'x93her': 1,\n",
       "         'checked': 1,\n",
       "         'wanted': 21,\n",
       "         'treeant': 5,\n",
       "         'monkey': 1,\n",
       "         'aisle': 1,\n",
       "         'ate': 13,\n",
       "         'crushed': 2,\n",
       "         'created': 4,\n",
       "         'x93once': 1,\n",
       "         'raisin': 1,\n",
       "         'days': 4,\n",
       "         'commented': 1,\n",
       "         'bears': 1,\n",
       "         'heck': 1,\n",
       "         'unknown': 1,\n",
       "         'arrived': 8,\n",
       "         'clothes': 5,\n",
       "         'loud': 4,\n",
       "         'caught': 8,\n",
       "         'cleaning': 2,\n",
       "         'enraged': 1,\n",
       "         'rang': 1,\n",
       "         'already': 9,\n",
       "         'grade': 3,\n",
       "         'wasp': 2,\n",
       "         'hearing': 1,\n",
       "         'fantasy': 1,\n",
       "         'seinfeld': 1,\n",
       "         'jjj': 1,\n",
       "         'another': 25,\n",
       "         'rats': 2,\n",
       "         'comic': 1,\n",
       "         'x93by': 1,\n",
       "         'feeding': 1,\n",
       "         'basketball': 3,\n",
       "         'ten': 6,\n",
       "         'lego': 1,\n",
       "         'top': 6,\n",
       "         'neck': 4,\n",
       "         'girls': 11,\n",
       "         'twisted': 1,\n",
       "         'elevator': 6,\n",
       "         'needed': 4,\n",
       "         'master': 8,\n",
       "         'legs': 12,\n",
       "         'bitter': 2,\n",
       "         'dogs': 2,\n",
       "         'damaging': 1,\n",
       "         'caitlyn': 5,\n",
       "         'totter': 1,\n",
       "         'ceiling': 3,\n",
       "         'murder': 3,\n",
       "         'bouncing': 1,\n",
       "         'frowned': 1,\n",
       "         'took': 33,\n",
       "         'trippy': 1,\n",
       "         'tardis': 2,\n",
       "         'vankowemberg': 2,\n",
       "         'x93hi': 4,\n",
       "         'somewhat': 1,\n",
       "         'x93he': 1,\n",
       "         'kept': 18,\n",
       "         'crawl': 1,\n",
       "         'peculiar': 1,\n",
       "         'distance': 4,\n",
       "         'perfectly': 1,\n",
       "         'lizzy': 3,\n",
       "         'showed': 11,\n",
       "         'hike': 1,\n",
       "         'morphed': 8,\n",
       "         'older': 6,\n",
       "         'tree': 9,\n",
       "         'board': 4,\n",
       "         'medley': 1,\n",
       "         'final': 1,\n",
       "         'shower': 4,\n",
       "         'classes': 1,\n",
       "         'speaking': 9,\n",
       "         'feeling': 2,\n",
       "         'powers': 2,\n",
       "         'hologram': 1,\n",
       "         'mini': 1,\n",
       "         'mermaid': 8,\n",
       "         'smashable': 1,\n",
       "         'hue': 1,\n",
       "         'ran': 38,\n",
       "         'mind': 2,\n",
       "         'mine': 6,\n",
       "         'talking': 19,\n",
       "         'rat': 2,\n",
       "         'pompoms': 1,\n",
       "         'vacuumed': 1,\n",
       "         'corny': 1,\n",
       "         'seen': 11,\n",
       "         'seem': 2,\n",
       "         'mint': 1,\n",
       "         'alive': 4,\n",
       "         'eila': 3,\n",
       "         'forced': 1,\n",
       "         'strength': 1,\n",
       "         'soaked': 1,\n",
       "         'snow': 5,\n",
       "         'bengal': 1,\n",
       "         'chest': 3,\n",
       "         'chess': 1,\n",
       "         'doors': 10,\n",
       "         'laughed': 3,\n",
       "         'grips': 1,\n",
       "         'x91natural': 1,\n",
       "         'rides': 3,\n",
       "         'completed': 6,\n",
       "         'tunnels': 2,\n",
       "         'even': 17,\n",
       "         'shallow': 2,\n",
       "         'though': 7,\n",
       "         'replenished': 1,\n",
       "         'object': 1,\n",
       "         'victoria': 3,\n",
       "         'previously': 1,\n",
       "         'involving': 2,\n",
       "         'mouth': 6,\n",
       "         'letter': 1,\n",
       "         'dummy': 2,\n",
       "         'episode': 3,\n",
       "         'released': 3,\n",
       "         'professor': 1,\n",
       "         'camp': 2,\n",
       "         'metal': 9,\n",
       "         'dog': 4,\n",
       "         'freeway': 1,\n",
       "         'swamp': 2,\n",
       "         'points': 2,\n",
       "         'aunt': 3,\n",
       "         'principle': 3,\n",
       "         'thunder': 1,\n",
       "         'x93martin': 1,\n",
       "         'responded': 3,\n",
       "         'came': 53,\n",
       "         'saying': 18,\n",
       "         'beaten': 2,\n",
       "         'random': 6,\n",
       "         'bonnie': 3,\n",
       "         'colors': 2,\n",
       "         'chandelier': 1,\n",
       "         'solutions': 1,\n",
       "         'stepping': 1,\n",
       "         'sneaking': 1,\n",
       "         'earth': 1,\n",
       "         'fennec': 3,\n",
       "         'x93and': 3,\n",
       "         'sharks': 2,\n",
       "         'busy': 4,\n",
       "         'crossfit': 1,\n",
       "         'x93thank': 1,\n",
       "         'cousins': 4,\n",
       "         'explain': 6,\n",
       "         'folded': 1,\n",
       "         'ocd': 1,\n",
       "         'theme': 3,\n",
       "         'abs': 4,\n",
       "         'deepest': 1,\n",
       "         'rich': 1,\n",
       "         'folder': 1,\n",
       "         'wearing': 1,\n",
       "         'plate': 3,\n",
       "         'sunglasses': 1,\n",
       "         'colorful': 1,\n",
       "         '10': 3,\n",
       "         'agni': 1,\n",
       "         'stop': 7,\n",
       "         'criticized': 1,\n",
       "         'haunted': 2,\n",
       "         'despite': 1,\n",
       "         'defending': 2,\n",
       "         'happiest': 1,\n",
       "         'dr': 1,\n",
       "         'tips': 1,\n",
       "         'hall': 7,\n",
       "         'bar': 7,\n",
       "         'spilling': 1,\n",
       "         'covering': 2,\n",
       "         'hallucinating': 1,\n",
       "         'emo': 1,\n",
       "         'bay': 2,\n",
       "         'twice': 3,\n",
       "         'bad': 6,\n",
       "         'magoon': 1,\n",
       "         'x93billy': 1,\n",
       "         'steak': 3,\n",
       "         'steal': 4,\n",
       "         'ears': 1,\n",
       "         'headed': 1,\n",
       "         'fair': 2,\n",
       "         'testing': 2,\n",
       "         'outgoing': 1,\n",
       "         'unexpectedly': 2,\n",
       "         'atherton': 1,\n",
       "         'depends': 1,\n",
       "         'decided': 5,\n",
       "         'tortured': 1,\n",
       "         'best': 7,\n",
       "         'subject': 1,\n",
       "         'brazil': 1,\n",
       "         '00': 4,\n",
       "         'said': 89,\n",
       "         'waiting': 7,\n",
       "         'away': 27,\n",
       "         'feline': 1,\n",
       "         'rings': 1,\n",
       "         'bies': 2,\n",
       "         'avery': 1,\n",
       "         'protected': 1,\n",
       "         'finger': 2,\n",
       "         'pets': 3,\n",
       "         'drawn': 1,\n",
       "         'caves': 3,\n",
       "         'reddit': 1,\n",
       "         'men': 2,\n",
       "         'khaleesi': 1,\n",
       "         'disband': 1,\n",
       "         'wa': 1,\n",
       "         'nature': 1,\n",
       "         'rolled': 3,\n",
       "         'smelled': 3,\n",
       "         'however': 13,\n",
       "         'boss': 2,\n",
       "         'nightmares': 1,\n",
       "         'devour': 3,\n",
       "         'wear': 2,\n",
       "         'recent': 1,\n",
       "         'tyrion': 1,\n",
       "         'cousin': 2,\n",
       "         'hundreds': 1,\n",
       "         'roller': 1,\n",
       "         'kitchen': 4,\n",
       "         'faced': 1,\n",
       "         'protect': 2,\n",
       "         'accident': 1,\n",
       "         'cow': 2,\n",
       "         'reported': 1,\n",
       "         'country': 2,\n",
       "         'pits': 1,\n",
       "         'adventures': 1,\n",
       "         'edgar': 1,\n",
       "         'players': 1,\n",
       "         'chandeliers': 1,\n",
       "         'games': 2,\n",
       "         'planned': 1,\n",
       "         'asked': 27,\n",
       "         'spear': 1,\n",
       "         'appeared': 7,\n",
       "         'x93it': 6,\n",
       "         '58lbs': 1,\n",
       "         'x93shark': 1,\n",
       "         'tons': 1,\n",
       "         'x93if': 1,\n",
       "         'blonde': 4,\n",
       "         'speak': 6,\n",
       "         'indistinctive': 1,\n",
       "         'bathroom': 11,\n",
       "         'meowing': 2,\n",
       "         'angel': 3,\n",
       "         'buter': 1,\n",
       "         'cockroaches': 2,\n",
       "         'three': 14,\n",
       "         'tiny': 4,\n",
       "         'quickly': 4,\n",
       "         'x93low': 1,\n",
       "         'sleepover': 10,\n",
       "         'much': 15,\n",
       "         'miraculously': 2,\n",
       "         'expected': 1,\n",
       "         'flexible': 1,\n",
       "         'entered': 3,\n",
       "         'containers': 1,\n",
       "         'threw': 7,\n",
       "         'website': 1,\n",
       "         'life': 12,\n",
       "         'x93we': 2,\n",
       "         'goodnight': 2,\n",
       "         'attacked': 3,\n",
       "         'mcg': 2,\n",
       "         'glided': 1,\n",
       "         'attacker': 1,\n",
       "         'toboso': 1,\n",
       "         'lift': 2,\n",
       "         'child': 3,\n",
       "         'catch': 1,\n",
       "         'unexperienced': 1,\n",
       "         'spin': 1,\n",
       "         'costanza': 1,\n",
       "         'wildcat': 1,\n",
       "         'applied': 2,\n",
       "         'tonight': 2,\n",
       "         'save': 2,\n",
       "         'tank': 3,\n",
       "         'raspy': 1,\n",
       "         'newest': 1,\n",
       "         'air': 8,\n",
       "         'strangest': 1,\n",
       "         'near': 8,\n",
       "         'teachers': 3,\n",
       "         'cracked': 2,\n",
       "         'voice': 8,\n",
       "         'mistake': 1,\n",
       "         'remembering': 2,\n",
       "         'seven': 4,\n",
       "         'climbed': 7,\n",
       "         'played': 9,\n",
       "         'dumped': 1,\n",
       "         'sting': 1,\n",
       "         'ii': 1,\n",
       "         'need': 3,\n",
       "         'dizzy': 1,\n",
       "         'cans': 1,\n",
       "         'io': 4,\n",
       "         'jesu': 1,\n",
       "         'faked': 1,\n",
       "         'madonna': 1,\n",
       "         'mouse': 2,\n",
       "         'disappear': 2,\n",
       "         'damaged': 1,\n",
       "         'brenna': 1,\n",
       "         '1812': 1,\n",
       "         'perform': 2,\n",
       "         'things': 14,\n",
       "         'make': 14,\n",
       "         'phantomhivers': 2,\n",
       "         'x93best': 1,\n",
       "         'stickies': 1,\n",
       "         'complex': 1,\n",
       "         'split': 5,\n",
       "         'x93combine': 1,\n",
       "         'bunny': 1,\n",
       "         'gaaaaaea': 1,\n",
       "         'bells': 1,\n",
       "         'fairly': 1,\n",
       "         'raid': 1,\n",
       "         'indentation': 1,\n",
       "         'social': 1,\n",
       "         'rail': 1,\n",
       "         'rain': 2,\n",
       "         'hand': 14,\n",
       "         'nightshade': 1,\n",
       "         'waterfall': 1,\n",
       "         'portal': 1,\n",
       "         'characters': 6,\n",
       "         'closet': 2,\n",
       "         'kid': 14,\n",
       "         'x93don': 1,\n",
       "         'practiced': 2,\n",
       "         'pressed': 1,\n",
       "         'echoed': 1,\n",
       "         'ocean': 9,\n",
       "         'x93boo': 1,\n",
       "         'contact': 1,\n",
       "         'resuming': 1,\n",
       "         'mother': 1,\n",
       "         'alarms': 1,\n",
       "         '800': 1,\n",
       "         'left': 12,\n",
       "         'rubies': 1,\n",
       "         'adelboden': 1,\n",
       "         'behind': 12,\n",
       "         'sentence': 3,\n",
       "         'zipline': 3,\n",
       "         'gods': 1,\n",
       "         'forsaken': 1,\n",
       "         'gravity': 1,\n",
       "         'assigned': 1,\n",
       "         'disguise': 2,\n",
       "         'goggles': 3,\n",
       "         'x93want': 1,\n",
       "         'human': 9,\n",
       "         'yes': 4,\n",
       "         'yet': 4,\n",
       "         'elk': 3,\n",
       "         'boars': 1,\n",
       "         'hills': 4,\n",
       "         'x93since': 1,\n",
       "         'dining': 4,\n",
       "         'character': 2,\n",
       "         'bubbas': 1,\n",
       "         'spread': 1,\n",
       "         'kelp': 1,\n",
       "         'teenager': 1,\n",
       "         'easy': 1,\n",
       "         'openings': 2,\n",
       "         'plasma': 1,\n",
       "         'gave': 18,\n",
       "         'viking': 1,\n",
       "         'graders': 1,\n",
       "         'maya': 1,\n",
       "         'casually': 1,\n",
       "         'intercom': 3,\n",
       "         'breaks': 1,\n",
       "         'possible': 2,\n",
       "         'possibly': 1,\n",
       "         'wanting': 2,\n",
       "         'dreamt': 4,\n",
       "         'shadow': 2,\n",
       "         'dreams': 1,\n",
       "         'sliced': 2,\n",
       "         '58': 1,\n",
       "         'gift': 3,\n",
       "         'contradict': 1,\n",
       "         'grizzly': 3,\n",
       "         '50': 1,\n",
       "         '53': 1,\n",
       "         'specific': 2,\n",
       "         'remind': 1,\n",
       "         'officer': 1,\n",
       "         'zombies': 3,\n",
       "         'night': 24,\n",
       "         'hung': 2,\n",
       "         'security': 2,\n",
       "         'sebastian': 66,\n",
       "         'webb': 9,\n",
       "         'right': 17,\n",
       "         'old': 20,\n",
       "         'finny': 5,\n",
       "         'crowd': 6,\n",
       "         'people': 79,\n",
       "         'successfully': 1,\n",
       "         'somehow': 11,\n",
       "         'dead': 20,\n",
       "         'jennifer': 1,\n",
       "         'crows': 1,\n",
       "         'escape': 5,\n",
       "         'x91sad': 1,\n",
       "         'tripped': 1,\n",
       "         'shapes': 1,\n",
       "         'humor': 1,\n",
       "         'tracked': 1,\n",
       "         'bottom': 6,\n",
       "         'purple': 4,\n",
       "         'rehearse': 1,\n",
       "         'fox': 7,\n",
       "         'ice': 3,\n",
       "         'rhino': 1,\n",
       "         'everything': 10,\n",
       "         'icy': 2,\n",
       "         'asking': 8,\n",
       "         'orbited': 1,\n",
       "         'x93natasha': 1,\n",
       "         'pondered': 1,\n",
       "         'beating': 1,\n",
       "         'plu': 2,\n",
       "         'backed': 1,\n",
       "         'disappeared': 3,\n",
       "         'binder': 1,\n",
       "         'lochmur': 1,\n",
       "         'flaws': 1,\n",
       "         'super': 3,\n",
       "         'chapter': 1,\n",
       "         'nihongo': 1,\n",
       "         'terminators': 1,\n",
       "         'attacks': 1,\n",
       "         'dollars': 2,\n",
       "         'barbarians': 5,\n",
       "         'choke': 1,\n",
       "         'elephants': 1,\n",
       "         'harassing': 1,\n",
       "         'dinner': 3,\n",
       "         'plus': 2,\n",
       "         'anime': 5,\n",
       "         'islands': 2,\n",
       "         'disqualified': 1,\n",
       "         'slightly': 1,\n",
       "         'automatically': 2,\n",
       "         'printed': 1,\n",
       "         'nerve': 1,\n",
       "         'raised': 1,\n",
       "         'coins': 1,\n",
       "         'lied': 1,\n",
       "         'raisen': 1,\n",
       "         'puzzle': 1,\n",
       "         'float': 1,\n",
       "         'son': 1,\n",
       "         'recognize': 1,\n",
       "         'lies': 1,\n",
       "         'bats': 1,\n",
       "         'carving': 1,\n",
       "         'wrap': 1,\n",
       "         'petting': 2,\n",
       "         'shoots': 1,\n",
       "         'cafeteria': 1,\n",
       "         'stumble': 1,\n",
       "         'envision': 1,\n",
       "         'strangely': 3,\n",
       "         'east': 1,\n",
       "         'deal': 2,\n",
       "         'flying': 13,\n",
       "         'sunlight': 1,\n",
       "         'fight': 9,\n",
       "         'stuck': 11,\n",
       "         'fought': 3,\n",
       "         'way': 14,\n",
       "         'triplets': 1,\n",
       "         'music': 2,\n",
       "         'factions': 2,\n",
       "         'halls': 4,\n",
       "         'happy': 7,\n",
       "         'lowest': 1,\n",
       "         'head': 18,\n",
       "         'medium': 1,\n",
       "         'form': 4,\n",
       "         'snowy': 4,\n",
       "         'attempted': 1,\n",
       "         'barrelling': 1,\n",
       "         'landing': 3,\n",
       "         'taken': 8,\n",
       "         'heat': 2,\n",
       "         'hear': 3,\n",
       "         'synesthete': 1,\n",
       "         'bern': 1,\n",
       "         'true': 2,\n",
       "         'fort': 2,\n",
       "         'absent': 1,\n",
       "         'flavour': 1,\n",
       "         'captive': 1,\n",
       "         'inside': 7,\n",
       "         'attached': 1,\n",
       "         'ruined': 1,\n",
       "         'crystal': 1,\n",
       "         'centipede': 1,\n",
       "         'torturing': 1,\n",
       "         'sapphire': 1,\n",
       "         'smashed': 2,\n",
       "         'daddy': 1,\n",
       "         'breathe': 8,\n",
       "         'poufy': 1,\n",
       "         'stronger': 1,\n",
       "         'thanksgiving': 1,\n",
       "         'promised': 1,\n",
       "         'curious': 1,\n",
       "         'sprouts': 1,\n",
       "         'ship': 4,\n",
       "         'annoying': 1,\n",
       "         'check': 5,\n",
       "         'dimly': 1,\n",
       "         'protested': 1,\n",
       "         'saber': 1,\n",
       "         'floor': 11,\n",
       "         'glacier': 1,\n",
       "         'na': 1,\n",
       "         'handed': 2,\n",
       "         'reality': 2,\n",
       "         'flood': 1,\n",
       "         'meows': 2,\n",
       "         'role': 1,\n",
       "         'holding': 6,\n",
       "         'digital': 1,\n",
       "         'test': 11,\n",
       "         'tie': 1,\n",
       "         'vanished': 2,\n",
       "         'confusing': 1,\n",
       "         'smell': 1,\n",
       "         'shrink': 1,\n",
       "         'picture': 7,\n",
       "         'sink': 2,\n",
       "         'phoenix': 11,\n",
       "         'faction': 1,\n",
       "         'surprise': 10,\n",
       "         'felt': 11,\n",
       "         'weirdest': 2,\n",
       "         'trailers': 1,\n",
       "         'journey': 4,\n",
       "         'died': 6,\n",
       "         'younger': 1,\n",
       "         'happening': 4,\n",
       "         'glass': 4,\n",
       "         'landed': 3,\n",
       "         'together': 1,\n",
       "         'beds': 1,\n",
       "         'carts': 1,\n",
       "         'teleported': 3,\n",
       "         'time': 51,\n",
       "         'push': 1,\n",
       "         'songs': 2,\n",
       "         'slope': 2,\n",
       "         '0': 2,\n",
       "         'switched': 1,\n",
       "         'dance': 14,\n",
       "         'skip': 1,\n",
       "         'jaylah': 38,\n",
       "         'tendrils': 2,\n",
       "         'laura': 3,\n",
       "         'theaters': 1,\n",
       "         'skate': 1,\n",
       "         'grateful': 1,\n",
       "         'skin': 9,\n",
       "         'battle': 2,\n",
       "         'chair': 2,\n",
       "         'displaying': 1,\n",
       "         'milk': 1,\n",
       "         'layers': 1,\n",
       "         'hurry': 1,\n",
       "         'x92ll': 6,\n",
       "         'zone': 3,\n",
       "         'filming': 1,\n",
       "         'flash': 1,\n",
       "         'environment': 1,\n",
       "         'answered': 4,\n",
       "         'charge': 1,\n",
       "         'fog': 1,\n",
       "         'glad': 5,\n",
       "         'discovering': 1,\n",
       "         'leaann': 2,\n",
       "         'suffered': 1,\n",
       "         'brown': 5,\n",
       "         'charles': 1,\n",
       "         'string': 1,\n",
       "         'hannah': 4,\n",
       "         'crate': 1,\n",
       "         'keeping': 3,\n",
       "         'seemingly': 2,\n",
       "         'choice': 3,\n",
       "         'x91hey': 1,\n",
       "         'defying': 1,\n",
       "         'kitten': 2,\n",
       "         'locked': 7,\n",
       "         'trouble': 3,\n",
       "         'exact': 3,\n",
       "         'cool': 2,\n",
       "         'sighed': 1,\n",
       "         'impressive': 1,\n",
       "         'level': 3,\n",
       "         'turns': 2,\n",
       "         'dig': 1,\n",
       "         'accidentally': 8,\n",
       "         'leave': 11,\n",
       "         'p': 3,\n",
       "         'subway': 2,\n",
       "         'teal': 2,\n",
       "         'x93s': 1,\n",
       "         'lever': 1,\n",
       "         'guy': 32,\n",
       "         'round': 1,\n",
       "         'talked': 5,\n",
       "         'attic': 2,\n",
       "         'discover': 1,\n",
       "         'sign': 1,\n",
       "         'widened': 1,\n",
       "         'slowed': 2,\n",
       "         'gummies': 1,\n",
       "         'eating': 11,\n",
       "         'fear': 3,\n",
       "         'church': 2,\n",
       "         'business': 1,\n",
       "         'goes': 1,\n",
       "         'shared': 1,\n",
       "         'falling': 4,\n",
       "         'ground': 9,\n",
       "         'filled': 1,\n",
       "         'creepiest': 1,\n",
       "         'gawa': 1,\n",
       "         'grandparents': 3,\n",
       "         'muslim': 1,\n",
       "         'french': 1,\n",
       "         'water': 46,\n",
       "         'downloaded': 1,\n",
       "         'nemesis': 2,\n",
       "         'witch': 1,\n",
       "         'decipher': 3,\n",
       "         'based': 1,\n",
       "         'along': 4,\n",
       "         'implanted': 1,\n",
       "         'teacher': 10,\n",
       "         'change': 7,\n",
       "         'box': 5,\n",
       "         'boy': 36,\n",
       "         'beanbag': 3,\n",
       "         'hbk': 2,\n",
       "         'battling': 2,\n",
       "         'studied': 2,\n",
       "         'guilty': 1,\n",
       "         'bow': 2,\n",
       "         'cinnamon': 1,\n",
       "         'raccoon': 1,\n",
       "         'massage': 1,\n",
       "         'bolt': 1,\n",
       "         'beautiful': 8,\n",
       "         'teenage': 1,\n",
       "         'love': 6,\n",
       "         'extra': 2,\n",
       "         'tuxedo': 1,\n",
       "         'phantomhive': 2,\n",
       "         'marked': 1,\n",
       "         'bloody': 3,\n",
       "         'customised': 1,\n",
       "         'alleyway': 1,\n",
       "         'anamorph': 2,\n",
       "         'paralyzed': 1,\n",
       "         'x93not': 1,\n",
       "         'fake': 2,\n",
       "         'marker': 1,\n",
       "         'swallowed': 1,\n",
       "         'sky': 9,\n",
       "         'everybody': 1,\n",
       "         'russell': 3,\n",
       "         'working': 7,\n",
       "         'surroundings': 4,\n",
       "         'angry': 2,\n",
       "         'visit': 3,\n",
       "         'angela': 3,\n",
       "         'insides': 1,\n",
       "         ...})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_flat = list(df[df['W266ID'] == 666]['Dream'])\n",
    "test_token = [nltk.tokenize.word_tokenize(dream) for dream in test_flat]\n",
    "test_final = [item for items in test_token for item in items]\n",
    "test_counts = Counter(test_final)\n",
    "test_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress Update: Finished splitting documents\n",
      "Progress Update: removed numbers\n",
      "Progress Update: Removed short words \n"
     ]
    }
   ],
   "source": [
    "#==================================================================\n",
    "# Cell Purpose:  Use all 26000 documents to create Corpus\n",
    "#                Train a LDA model on full corpus\n",
    "#==================================================================\n",
    "dreams = list(df['Dream'])\n",
    "\n",
    "# Split the documents into tokens.\n",
    "for idx in range(len(dreams)):\n",
    "    dreams[idx] = dreams[idx].lower()  # Convert to lowercase.\n",
    "    dreams[idx] = nltk.tokenize.word_tokenize(dreams[idx])  # Split into words.\n",
    "    \n",
    "print \"Progress Update: Finished splitting documents\"\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "dreams = [[token for token in dream if not token.isdigit()] for dream in dreams]\n",
    "\n",
    "print \"Progress Update: removed numbers\"\n",
    "# Remove words that are only one or two characters.\n",
    "dreams = [[token for token in dream if len(token) > 2] for dream in dreams]\n",
    "\n",
    "# Remove other words that are artifacts of data\n",
    "# remove /d, cellpadding, =, etc.\n",
    "\n",
    "print \"Progress Update: Removed short words \"\n",
    "# Lemmatize the dreams.\n",
    "lmtzr = WordNetLemmatizer()\n",
    "dreams = [[lmtzr.lemmatize(token) for token in dream] for dream in dreams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEBUG_VERBOSITY_HIGH = False\n",
    "if DEBUG_VERBOSITY_HIGH:\n",
    "    print len(dreams)\n",
    "    for i in range(0,100):\n",
    "        print len(dreams[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 13944\n",
      "Number of dreams: 26859\n"
     ]
    }
   ],
   "source": [
    "#==================================================================\n",
    "# Cell Purpose: Create a dictionary representation of the dreams\n",
    "#               Corpus is a Bag-of-words representation\n",
    "#==================================================================\n",
    "\n",
    "## Remove rare and common tokens.\n",
    "dictionary = Dictionary(dreams)\n",
    "\n",
    "# Filter out words that occur less than 5 dreams, \n",
    "# Experimented with removing more than 60% of the dreams.\n",
    "# But this was shown to have no effect (screen out more than 60% of dreams)\n",
    "dictionary.filter_extremes(no_below=5)\n",
    "\n",
    "#----------------\n",
    "# Vectorize data.\n",
    "#----------------\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(dream) for dream in dreams]\n",
    "\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of dreams: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#==================================================================\n",
    "# Cell Purpse:  Create a dictionary representation of the dreams.\n",
    "#==============================================================\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 49min 7s\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: './model/topics100/DreamLda100.model.state'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mIOError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-133-22c8a6c1c32e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu\"time dreamerLdaModel = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize,                            alpha='auto', eta='auto', iterations=iterations, num_topics=num_topics,                            passes=passes, eval_every=eval_every)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mMAX_TOPICS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mdreamerLdaModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLDA_MODEL_200_TOPIC_PATH_FILE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mdreamerLdaModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLDA_MODEL_50_TOPIC_PATH_FILE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Jay\\Anaconda2\\lib\\site-packages\\gensim\\models\\ldamodel.pyc\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, fname, ignore, separately, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1133\u001b[0m         \"\"\"\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msmart_extension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'.state'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;31m# Save the dictionary separately if not in 'ignore'.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'id2word'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mignore\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Jay\\Anaconda2\\lib\\site-packages\\gensim\\utils.pyc\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[0;32m    496\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"saved %s object\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# `fname_or_handle` does not have write attribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_smart_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseparately\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep_limit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Jay\\Anaconda2\\lib\\site-packages\\gensim\\utils.pyc\u001b[0m in \u001b[0;36m_smart_save\u001b[1;34m(self, fname, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[0;32m    373\u001b[0m                                        compress, subname)\n\u001b[0;32m    374\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m             \u001b[0mpickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m             \u001b[1;31m# restore attribs handled specially\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Jay\\Anaconda2\\lib\\site-packages\\gensim\\utils.pyc\u001b[0m in \u001b[0;36mpickle\u001b[1;34m(obj, fname, protocol)\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m     \"\"\"\n\u001b[1;32m--> 924\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# 'b' for binary, needed on Windows\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m         \u001b[0m_pickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Jay\\Anaconda2\\lib\\site-packages\\smart_open\\smart_open_lib.pyc\u001b[0m in \u001b[0;36msmart_open\u001b[1;34m(uri, mode, **kw)\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[1;31m# local files -- both read & write supported\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[1;31m# compression, if any, is determined by the filename extension (.gz, .bz2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfile_smart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"s3\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"s3n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"s3u\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Jay\\Anaconda2\\lib\\site-packages\\smart_open\\smart_open_lib.pyc\u001b[0m in \u001b[0;36mfile_smart_open\u001b[1;34m(fname, mode)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m     \"\"\"\n\u001b[1;32m--> 644\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcompression_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    645\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: './model/topics100/DreamLda100.model.state'"
     ]
    }
   ],
   "source": [
    "#==================================================================\n",
    "# Cell Purpse: Load  previously trained LDA model \n",
    "#              or else train LDA Model\n",
    "#================================================================== \n",
    "CREATE_FULL_CORPUS_LDA_MODEL = True\n",
    "MAX_TOPICS = True\n",
    "LDA_MODEL_200_TOPIC_PATH_FILE = './model/topics200/DreamLda200.model'\n",
    "LDA_MODEL_200_TOPIC_PATH_FILE = './model/topics100/DreamLda100.model'\n",
    "LDA_MODEL_50_TOPIC_PATH_FILE  = './model/topics50/DreamLda.model'\n",
    "\n",
    "if CREATE_FULL_CORPUS_LDA_MODEL:\n",
    "    # Set training parameters.\n",
    "    num_topics = 200\n",
    "    chunksize = 26000/10\n",
    "    passes = 10\n",
    "    iterations = 400\n",
    "    eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "    %time dreamerLdaModel = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \\\n",
    "                           alpha='auto', eta='auto', iterations=iterations, num_topics=num_topics, \\\n",
    "                           passes=passes, eval_every=eval_every)\n",
    "    if MAX_TOPICS:\n",
    "        dreamerLdaModel.save(LDA_MODEL_200_TOPIC_PATH_FILE)\n",
    "    else: \n",
    "        dreamerLdaModel.save(LDA_MODEL_50_TOPIC_PATH_FILE)\n",
    "else: \n",
    "    if MAX_TOPICS:\n",
    "        # Model with 200 Topics\n",
    "        dreamerLdaModel = LdaModel.load(LDA_MODEL_200_TOPIC_PATH_FILE)\n",
    "    else:\n",
    "        # Model with 50 Topics\n",
    "        dreamerLdaModel = LdaModel.load(LDA_MODEL_50_TOPIC_PATH_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_topics = dreamerLdaModel.top_topics(corpus, topn=50)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#==================================================================\n",
    "# Cell Purpse: 1)Take a dreamer from the Collection\n",
    "#              2) Create a BOW for each dream\n",
    "#              3) Create the LDA_Vector \n",
    "#              4) Compute the top topics per dreaml\n",
    "#================================================================== \n",
    "\n",
    "# Take Dreamer i and transform the dreams into a bow\n",
    "# cell_start = time.time()\n",
    "DEBUG_VERBOSITY_HIGH = False\n",
    "\n",
    "\n",
    "text = \"The one at the Meads's house, where it's bigger inside than out; there's a European village just inside, with a cobblestone street and a Pied-Piper sort of man with curly hair, he can do things like juggle - I go up the back stairs [there aren't any in the real house] and then down the other side [since there's a second set, immediately] then down a short empty hallway that turns a corner, where I find a tiny room...a young woman with shoulder-length blonde hair in a pageboy is there, cooking at a stove that almost fills the room...she's nice to me. Now outside, I'm waiting for my aunt to pick me up - she arrives in a little round convertible and we go for a drive, not very far - we cross a little bridge over a creek, then double back and she drops me off at the house again. Inside (?) I sit with a couple of people, playing with a string of blue balloons.\"\n",
    "#bow_vector = dictionary.doc2bow(dream)\n",
    "bow_vector = dictionary.doc2bow(nltk.tokenize.word_tokenize(text))\n",
    "bow_vector2 = dictionary.doc2bow(dreams[1])\n",
    "lda_vector = dreamerLdaModel[bow_vector2]\n",
    "print(lda_vector)\n",
    "print(dreamerLdaModel.print_topic(max(lda_vector, key=lambda item: item[1])[0]))\n",
    "\n",
    "\n",
    "if DEBUG_VERBOSITY_HIGH:\n",
    "    print len(bow_vector2)\n",
    "    #print bow_vector2\n",
    "    print([(dictionary[id], count) for id, count in bow_vector2])\n",
    "    print \"\\n\"\n",
    "    print len(bow_vector)\n",
    "    print([(dictionary[id], count) for id, count in bow_vector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_vector = model[bow_vector]\n",
    "print(lda_vector)\n",
    "print(model.print_topic(max(lda_vector, key=lambda item: item[1])[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
